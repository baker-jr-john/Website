[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Penn GSE",
    "section": "",
    "text": "MIT License\nCopyright © 2024 John Richard Baker Jr.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”) to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright and permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "",
    "text": "In the field of educational data mining, detecting off-task behavior is crucial for understanding student engagement and improving learning outcomes. Off-task behavior refers to any student actions unrelated to the learning objectives, which can hinder the educational process. Traditional methods of identifying off-task behavior are often subjective and resource-intensive. Therefore, developing automated, accurate detection methods using machine learning can significantly benefit educators and learners.\nThis study presents an in-depth analysis of machine learning models designed to classify off-task behavior in educational settings. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through experiments and analyses, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#introduction",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "",
    "text": "In the field of educational data mining, detecting off-task behavior is crucial for understanding student engagement and improving learning outcomes. Off-task behavior refers to any student actions unrelated to the learning objectives, which can hinder the educational process. Traditional methods of identifying off-task behavior are often subjective and resource-intensive. Therefore, developing automated, accurate detection methods using machine learning can significantly benefit educators and learners.\nThis study presents an in-depth analysis of machine learning models designed to classify off-task behavior in educational settings. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through experiments and analyses, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#literature-review",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#literature-review",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Literature Review",
    "text": "Literature Review\n\nBackground on Off-Task Behavior Detection\nOff-task behavior detection in educational settings has been an area of active research for several years, with studies employing various approaches to identify and analyze student disengagement, particularly in the context of algebra tutoring systems.\n\nTraditional Methods\nEarly research relied heavily on human observation and self-reporting techniques. While these methods provided rich qualitative data, they were often subjective, time-consuming, and not scalable for large-scale implementation (Baker 2007).\n\n\nMachine Learning Approaches\nThe advent of intelligent tutoring systems and educational software has enabled more sophisticated detection methods using machine learning:\n\nLog File Analysis: Researchers have developed models that analyze student interaction logs to identify patterns indicative of off-task behavior. These models often utilize features such as time spent on tasks, response correctness, and help-seeking behavior (Cocea and Weibelzahl 2009; Pardos et al. 2014).\nMultimodal Detection: Some studies have incorporated multiple data sources to create more comprehensive off-task behavior detection systems (Bosch et al. 2015).\nTemporal Models: Researchers have explored the use of sequential models to capture the temporal aspects of student behavior and improve detection accuracy (Liu and Koedinger 2017).\n\n\n\nChallenges in Off-Task Behavior Detection\nSeveral challenges have been identified in the field:\n\nClass Imbalance: Off-task behavior is typically less frequent than on-task behavior, leading to imbalanced datasets that can skew model performance (Pardos et al. 2014).\nContext Sensitivity: The definition of off-task behavior can vary depending on the learning environment and taskmaking it difficult to create universally applicable models (Baker 2007).\nPrivacy Concerns: As detection methods become more sophisticated, they often require more invasive data collection, raising ethical and privacy issues (Bosch et al. 2015). This is particularly relevant in educational settings where student data protection is paramount.\nReal-time Detection: Developing models that can detect off-task behavior in real-time to enable immediate intervention remains a significant challenge (Liu and Koedinger 2017), especially in resource-constrained educational environments.\n\n\n\nRecent Trends\nRecent research has focused on:\n\nPersonalized Models: Developing detection systems that adapt to individual student behaviors and learning patterns (Pardos et al. 2014).\nInterpretable AI: Creating models that not only detect off-task behavior but also provide insights into the reasons behind it (Cocea and Weibelzahl 2009). This trend aligns with this study’s focus on model comparison and evaluation metrics, as interpretable models can offer valuable insights for educators.\nIntegration with Intervention Strategies: Combining detection models with automated intervention systems to re-engage students in real-time (Liu and Koedinger 2017).\n\n\n\nEducational Context in Algebra Tutoring Systems\nIn the context of algebra tutoring systems, off-task behavior can significantly impact learning outcomes. Cocea and Weibelzahl found that students who frequently engage in off-task behavior in mathematics tutoring systems show lower learning gains and decreased problem-solving skills (Cocea and Weibelzahl 2009). The abstract nature of algebraic concepts makes sustained engagement crucial for skill development, highlighting the importance of accurate off-task behavior detection in these environments.\nThis study builds upon existing work by addressing the persistent challenge of class imbalance and exploring advanced machine learning techniques to improve off-task behavior detection accuracy. A focus on threshold optimization and model comparison provides valuable insights into the practical implementation of these detection systems in educational settings, particularly for algebra tutoring systems where maintaining student engagement is critical for learning success.\nBy comparing multiple classifiers and employing techniques like SMOTE, this research contributes to the ongoing effort to develop more robust and accurate off-task behavior detection models. Furthermore, an emphasis on performance metrics such as Cohen’s Kappa and F1-score addresses the need for comprehensive evaluation in imbalanced datasets, a critical aspect often overlooked in previous studies."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#methodology",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#methodology",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Methodology",
    "text": "Methodology\nMy study employed a multi-step approach to develop and evaluate machine learning models:\n\nData Preparation: I utilized a dataset containing features related to student behavior, with a binary target variable indicating off-task status (OffTask: Y/N).\nData Overview: The dataset contains 616 student interactions with a close-loop algebra tutoring system. Each entry includes 29 features capturing various aspects of student performance, such as correctness of responses, help-seeking behavior, and time spent on tasks. Key features include:\n\nBinary indicator of off-task behavior\nPerformance metrics (e.g., average correct responses, errors)\nTime-related features\nError and help-seeking metrics\nRecent performance indicators\n\n\nThis data allows for analysis of learning patterns and the effectiveness of the tutoring system in teaching algebraic concepts.\n\nModel Selection: I implemented three classifiers: Random Forest, XGBoost, and Gradient Boosting.\nHandling Class Imbalance: To address the imbalanced nature of the dataset, I applied the Synthetic Minority Over-sampling Technique (SMOTE).\nHyperparameter Tuning: I used GridSearchCV to optimize model parameters, focusing on maximizing the F1-score.\nThreshold Optimization: I explored various decision thresholds to balance precision and recall, particularly for the minority class (off-task behavior).\nPerformance Evaluation: I assessed model performance using metrics such as Cohen’s Kappa score, precision, recall, F1-score, and confusion matrices.\nCross-Validation: I employed k-fold cross-validation to ensure robust performance estimates across different data subsets.\n\n\nData Preparation\nI began by importing the necessary libraries and loading the dataset:\n\n# Import libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, cohen_kappa_score, confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\ndata = pd.read_csv('data/ca1-dataset.csv')\n\nI then prepared the data by encoding the target variable and selecting relevant features:\n\n# Prepare the data\ndata['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable\nX = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features\ny = data['OffTask']  # Target variable\n\n\n\nHandling Class Imbalance with SMOTE\nThe dataset exhibited class imbalance, with significantly more instances of “Not OffTask” than “OffTask.” To address this issue, I applied SMOTE to the training data:\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Calculate the ratio of classes\nclass_0_count = sum(y_train_resampled == 0)\nclass_1_count = sum(y_train_resampled == 1)\nratio_of_classes = class_0_count / class_1_count\n\n\n\nModel Selection and Hyperparameter Tuning\n\nRandom Forest Classifier\nI defined the Random Forest model and set up a parameter grid for hyperparameter tuning:\n\n# Define the model\nmodel_rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n# Define the parameter grid\nparam_grid_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Set up GridSearchCV with corrected parameter names and variables\ngrid_search_rf = GridSearchCV(estimator=model_rf, param_grid=param_grid_rf,\n                              scoring='f1', cv=5, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV\ngrid_search_rf.fit(X_train_resampled, y_train_resampled)\n\n# Best parameters\nprint(\"Best parameters found for Random Forest: \", grid_search_rf.best_params_)\n\nFitting 5 folds for each of 108 candidates, totalling 540 fits\nBest parameters found for Random Forest:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n\n\n\n\nXGBoost Classifier\nI initialized the XGBoost model, adjusting for class imbalance using scale_pos_weight:\n\n# Define the XGBoost model\nxgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)\n\n# Fit the model\nxgb_model.fit(X_train_resampled, y_train_resampled)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) \n\n\n\n\nGradient Boosting Classifier\nI defined the Gradient Boosting model with specific hyperparameters:\n\n# Define the Gradient Boosting model\ngb_model = GradientBoostingClassifier(\n    learning_rate=0.2,\n    max_depth=5,\n    min_samples_split=10,\n    n_estimators=200,\n    random_state=42\n)\n\n# Fit the model on the resampled training data\ngb_model.fit(X_train_resampled, y_train_resampled)\n\nGradientBoostingClassifier(learning_rate=0.2, max_depth=5, min_samples_split=10,\n                           n_estimators=200, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.2, max_depth=5, min_samples_split=10,\n                           n_estimators=200, random_state=42) \n\n\n\n\n\nPerformance Evaluation\nI evaluated each model using the test set and calculated the Cohen’s Kappa score and classification report.\n\nRandom Forest Evaluation\n\n# Make predictions on the test set\ny_pred_rf = grid_search_rf.predict(X_test)\n\n# Evaluate the model\nkappa_rf = cohen_kappa_score(y_test, y_pred_rf)\nprint(\"Kappa Score (Random Forest):\", kappa_rf)\nprint(classification_report(y_test, y_pred_rf))\n\nKappa Score (Random Forest): 0.40175953079178883\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       147\n           1       0.38      0.50      0.43         6\n\n    accuracy                           0.95       153\n   macro avg       0.68      0.73      0.70       153\nweighted avg       0.96      0.95      0.95       153\n\n\n\n\n\nXGBoost Evaluation\n\n# Make predictions\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluate the model\nkappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)\nprint(\"Kappa Score (XGBoost):\", kappa_xgb)\nprint(classification_report(y_test, y_pred_xgb))\n\nKappa Score (XGBoost): 0.29655172413793107\n              precision    recall  f1-score   support\n\n           0       0.98      0.94      0.96       147\n           1       0.25      0.50      0.33         6\n\n    accuracy                           0.92       153\n   macro avg       0.61      0.72      0.65       153\nweighted avg       0.95      0.92      0.93       153\n\n\n\n\n\nGradient Boosting Evaluation\n\n# Make predictions on the test set\ny_pred_gb = gb_model.predict(X_test)\n\n# Evaluate the model\nkappa_gb = cohen_kappa_score(y_test, y_pred_gb)\nprint(\"Kappa Score (Gradient Boosting):\", kappa_gb)\nprint(classification_report(y_test, y_pred_gb))\n\nKappa Score (Gradient Boosting): 0.4137931034482758\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       147\n           1       0.33      0.67      0.44         6\n\n    accuracy                           0.93       153\n   macro avg       0.66      0.81      0.70       153\nweighted avg       0.96      0.93      0.94       153\n\n\n\n\n\n\nThreshold Optimization\nTo improve the detection of off-task behavior, I experimented with adjusting the decision threshold:\n\n# Get predicted probabilities\ny_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n\n# Experiment with different thresholds\nthresholds = np.arange(0.0, 1.0, 0.05)\nprecisions = []\nrecalls = []\nkappa_scores = []\n\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb &gt;= threshold).astype(int)\n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) &gt; 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) &gt; 0 else 0\n    kappa = cohen_kappa_score(y_test, y_pred_adjusted)\n    precisions.append(precision)\n    recalls.append(recall)\n    kappa_scores.append(kappa)\n\n# Plot Precision, Recall, and Kappa Score vs. Threshold\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')\nplt.title('Precision, Recall, and Kappa Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nI determined that a threshold of 0.90 maximized the F1-score for the off-task class.\n\n# Apply the optimal threshold\nbest_threshold = 0.90\ny_pred_final = (gb_model.predict_proba(X_test)[:, 1] &gt;= best_threshold).astype(int)\n\n# Evaluate the model with the new predictions\nkappa_final = cohen_kappa_score(y_test, y_pred_final)\nprint(\"Final Kappa Score with Threshold 0.90:\", kappa_final)\nprint(classification_report(y_test, y_pred_final))\n\nFinal Kappa Score with Threshold 0.90: 0.5513196480938416\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       147\n           1       0.50      0.67      0.57         6\n\n    accuracy                           0.96       153\n   macro avg       0.74      0.82      0.78       153\nweighted avg       0.97      0.96      0.96       153\n\n\n\n\n\nConfusion Matrix and Cross-Validation\nI computed the confusion matrix and performed k-fold cross-validation to assess model stability:\n\n# Calculate and print confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred_final)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Visualize the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Not OffTask (0)', 'OffTask (1)'],\n            yticklabels=['Not OffTask (0)', 'OffTask (1)'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Perform k-fold cross-validation\ncv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')\n\n# Print the cross-validation scores\nprint(\"Cross-Validation F1 Scores:\", cv_scores)\nprint(\"Mean F1 Score:\", np.mean(cv_scores))\nprint(\"Standard Deviation of F1 Scores:\", np.std(cv_scores))\n\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\n\n\n\n\n\n\n\n\nCross-Validation F1 Scores: [0.25       0.54545455 0.5        0.2        0.        ]\nMean F1 Score: 0.2990909090909091\nStandard Deviation of F1 Scores: 0.20136722754852265"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#results",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Results",
    "text": "Results\n\nModel Performance Comparison\nThe best hyperparameters found for the Random Forest Classifier were:\n\nmax_depth: 20\nmin_samples_leaf: 1\nmin_samples_split: 2\nn_estimators: 50\n\nThe Cohen’s Kappa Scores for the models were:\n\nRandom Forest: 0.4018\nXGBoost: 0.2966\nGradient Boosting: 0.5513 (after threshold optimization)\n\n\n\nThreshold Optimization Insights\nAdjusting the decision threshold significantly impacted the model’s performance:\n\nAt Threshold 0.90:\n\nPrecision (OffTask): 0.50\nRecall (OffTask): 0.67\nF1-score (OffTask): 0.57\nCohen’s Kappa Score: 0.5513\n\n\n\n\nConfusion Matrix Analysis\nThe confusion matrix at the optimal threshold was:\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\nTrue Positives: 4\nFalse Positives: 4\nTrue Negatives: 143\nFalse Negatives: 2\n\n\n\nCross-Validation Results\n\nCross-Validation F1 Scores: [0.4, 0.0, 0.2857, 0.5, 0.5455]\nMean F1 Score: 0.299\nStandard Deviation: 0.201"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#discussion",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Discussion",
    "text": "Discussion\n\nChallenges with Class Imbalance\nThe imbalanced dataset posed significant challenges:\n\nDifficulty in Learning Minority Class Patterns: The scarcity of OffTask instances made it hard for models to generalize.\nOverfitting Risk: Without proper handling, models could overfit to the majority class.\n\n\n\nEffectiveness of SMOTE\nApplying SMOTE helped in:\n\nBalancing the Dataset: Synthetic samples improved the representation of the minority class.\nImproving Recall: The model improved at identifying OffTask instances.\n\nHowever, reliance on synthetic data might not capture the complexity of actual off-task behavior.\n\n\nThreshold Optimization Trade-offs\n\nImproved Detection: A higher threshold increased the precision for the OffTask class.\nFalse Positives and Negatives: Adjusting the threshold affected the balance between missing actual OffTask instances and incorrectly flagging Not OffTask instances.\n\n\n\nModel Selection Insights\n\nGradient Boosting Superiority: Its ability to focus on misclassified instances led to better performance.\nRandom Forest and XGBoost Limitations: These models were less effective, possibly due to their parameter sensitivity and handling of imbalanced data.\n\n\n\nCross-Validation Variability\nThe significant standard deviation in cross-validation scores suggests:\n\nModel Instability: Performance varied across different data splits.\nNeed for Robustness: Further techniques are required to ensure consistent performance."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#limitations",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#limitations",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Limitations",
    "text": "Limitations\nWhile this study provides valuable insights into off-task behavior detection in algebra tutoring systems, it’s important to acknowledge several limitations:\n\nDataset Constraints\n\nSize: The dataset, while substantial, is limited to 616 student interactions. A larger dataset might reveal additional patterns or improve model generalizability.\nContext: The data is specific to algebra tutoring systems and may not generalize well to other subjects or learning environments.\nTemporal aspects: The data represents a snapshot in time and doesn’t capture long-term changes in student behavior or learning patterns.\n\n\n\nFeature Selection\n\nLimited feature set: I relied on 29 pre-defined features. There may be other relevant features not captured in the dataset that could improve detection accuracy.\nFeature interpretability: Some features, particularly those related to recent performance indicators, are challenging to interpret in an educational context.\n\n\n\nModel Limitations\n\nModel selection: While I compared several classifiers, there are other advanced models (e.g., deep learning architectures) that I didn’t explore due to computational constraints.\nHyperparameter tuning: Despite using GridSearchCV, I may not have exhaustively explored all possible hyperparameter combinations.\n\n\n\nClass Imbalance Handling\n\nSMOTE limitations: While SMOTE helped address class imbalance, it generates synthetic examples which may not perfectly represent real-world off-task behavior.\nAlternative techniques: Other class imbalance handling techniques (e.g., adaptive boosting, cost-sensitive learning) were not explored and could potentially yield different results.\n\n\n\nPerformance Metrics\n\nMetric selection: I focused on Cohen’s Kappa and F1-score. Other metrics might provide additional insights into model performance.\nThreshold sensitivity: The results are sensitive to the chosen decision threshold, which may not be optimal for all use cases.\n\n\n\nGeneralizability\n\nStudent population: The dataset may not represent the full diversity of student populations, potentially limiting the model’s applicability across different demographics.\nEducational system specificity: The patterns of off-task behavior detected may be specific to the particular algebra tutoring system used and might not generalize to other educational software.\n\n\n\nReal-world Application\n\nReal-time detection: This study doesn’t address the challenges of implementing these models for real-time off-task behavior detection in live classroom settings.\nComputational resources: The computational requirements for running these models may be a limiting factor for widespread adoption in resource-constrained educational environments.\n\n\n\nLack of Qualitative Insights\n\nStudent perspective: My quantitative approach does not capture students’ own perceptions of their engagement or reasons for off-task behavior.\nContextual factors: Environmental or personal factors that might influence off-task behavior are not accounted for in the model.\n\n\n\nValidation in Live Settings\n\nControlled environment: The models were developed and tested on historical data. Their performance in live, dynamic classroom environments remains to be validated.\n\nThese limitations provide opportunities for future research to build upon and refine my approach to off-task behavior detection in educational settings."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#ethical-considerations-in-off-task-behavior-detection",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#ethical-considerations-in-off-task-behavior-detection",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Ethical Considerations in Off-Task Behavior Detection",
    "text": "Ethical Considerations in Off-Task Behavior Detection\nThe implementation of off-task behavior detection systems in educational settings raises several ethical concerns that researchers and practitioners must carefully consider:\n\nPrivacy and Data Protection\n\nCollection of sensitive data: Off-task behavior detection often involves collecting detailed data about student activities, potentially including keystroke patterns, eye movements, or even facial expressions. This level of monitoring raises significant privacy concerns.\nData storage and security: Ensuring the secure storage and transmission of student data is crucial to prevent unauthorized access or breaches.\nCompliance with regulations: Researchers must adhere to data protection regulations such as GDPR in Europe or FERPA and COPPA in the United States, which have strict guidelines on handling student data.\n\n\n\nInformed Consent\n\nStudent awareness: Students (and their parents/guardians for minors) should be fully informed about what data is being collected, how it will be used, and who will have access to it.\nOpt-out options: Providing students with the ability to opt-out of monitoring without academic penalty is an important ethical consideration.\n\n\n\nBias and Fairness\n\nAlgorithmic bias: Machine learning models may inadvertently perpetuate or amplify existing biases related to race, gender, or socioeconomic status. Ensuring fairness in off-task behavior detection across diverse student populations is crucial.\nCultural sensitivity: What constitutes “off-task” behavior may vary across cultures, and detection systems should be designed with cultural differences in mind.\n\n\n\nTransparency and Explainability\n\nInterpretable models: Using interpretable AI models allows for better understanding of how off-task behavior is being detected, which is important for both educators and students.\nClear communication: The criteria for determining off-task behavior should be clearly communicated to students and educators.\n\n\n\nPotential for Misuse\n\nOver-reliance on technology: There’s a risk that educators might rely too heavily on automated systems, potentially overlooking important contextual factors in student behavior.\nPunitive use: Safeguards should be in place to prevent the use of off-task behavior data for punitive measures rather than supportive interventions.\n\n\n\nPsychological Impact\n\nStress and anxiety: Constant monitoring could lead to increased stress and anxiety among students, potentially impacting their learning and well-being.\nSelf-fulfilling prophecies: Labeling students as frequently “off-task” could negatively impact their self-perception and motivation.\n\n\n\nData Retention and Right to be Forgotten\n\nLimited data retention: Implementing policies for how long data is kept and when it should be deleted.\nStudent rights: Allowing students to request the deletion of their data, especially after they’ve left the educational institution.\n\n\n\nContextual Considerations\n\nFlexibility in detection: Recognizing that brief off-task moments can be part of the learning process and not always detrimental.\nAdaptive systems: Developing systems that can adapt to individual student learning styles and needs.\n\n\n\nStakeholder Involvement\n\nInclusive design: Involving educators, students, and parents in the design and implementation of off-task behavior detection systems.\nOngoing evaluation: Regularly assessing the impact and effectiveness of these systems with input from all stakeholders.\n\nBy addressing these ethical considerations, researchers and educators can work towards developing off-task behavior detection systems that are not only effective but also respect student rights, promote fairness, and contribute positively to the learning environment. As you conduct your study, it’s important to explicitly address how your methodology takes these ethical considerations into account, demonstrating a commitment to responsible research practices in educational technology."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#conclusion",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Conclusion",
    "text": "Conclusion\nThis study highlights the complexities involved in detecting off-task behavior using machine learning. Key findings include:\n\nGradient Boosting Effectiveness: With proper tuning and threshold adjustment, it outperformed other models.\nImportance of Handling Class Imbalance: Techniques like SMOTE are crucial but have limitations.\nThreshold Optimization: Essential for improving minority class detection but requires careful trade-off consideration.\n\n\nFuture Work\n\nAdvanced Imbalance Handling: Explore cost-sensitive learning and ensemble methods.\nFeature Engineering: Incorporate more behavioral indicators to improve model accuracy.\nReal-world Implementation: Test models in live educational settings for practical validation."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#submission-guidelines",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#submission-guidelines",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Baker - Learning Analytics",
    "section": "",
    "text": "Core Methods in Educational Data Mining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/index.html",
    "href": "educ_6191_001/index.html",
    "title": "Core Methods in Educational Data Mining",
    "section": "",
    "text": "Creative Assignments\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/index.html",
    "href": "educ_6191_001/creative_assignments/index.html",
    "title": "Creative Assignments",
    "section": "",
    "text": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection\n\n\n \n\nA machine learning model to detect off-task behavior\n\n\n\n`September 17, 2024`{=html}\nJohn Baker\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]