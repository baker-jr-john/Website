[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Penn GSE",
    "section": "",
    "text": "MIT License\nCopyright © 2024 John Richard Baker Jr.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”) to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright and permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Baker - Learning Analytics",
    "section": "",
    "text": "Core Methods in Educational Data Mining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/index.html",
    "href": "educ_6191_001/index.html",
    "title": "Core Methods in Educational Data Mining",
    "section": "",
    "text": "Creative Assignments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/index.html",
    "href": "educ_6191_001/creative_assignments/index.html",
    "title": "Creative Assignments",
    "section": "",
    "text": "No matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "title": "Behavior Detection",
    "section": "",
    "text": "Freeze computations after work is complete.\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites!\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, cohen_kappa_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\n\n# Load the dataset\ndata = pd.read_csv('data/ca1-dataset.csv')\n\n# Prepare the data\ndata['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable\nX = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features\ny = data['OffTask']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Calculate the ratio of classes\nclass_0_count = sum(y_train_resampled == 0)\nclass_1_count = sum(y_train_resampled == 1)\nratio_of_classes = class_0_count / class_1_count\n\n# Define the model\nmodel = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n                           scoring='f1', cv=5, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV\ngrid_search.fit(X_train_resampled, y_train_resampled)\n\n# Best parameters\nprint(\"Best parameters found: \", grid_search.best_params_)\n\nFitting 5 folds for each of 108 candidates, totalling 540 fits\nBest parameters found:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n\n\n\n# Train the model on the resampled data\nmodel.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nkappa = cohen_kappa_score(y_test, y_pred)\nprint(\"Kappa Score:\", kappa)\nprint(classification_report(y_test, y_pred))\n\nKappa Score: 0.40175953079178883\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       147\n           1       0.38      0.50      0.43         6\n\n    accuracy                           0.95       153\n   macro avg       0.68      0.73      0.70       153\nweighted avg       0.96      0.95      0.95       153\n\n\n\n\n# Train the XGBoost model without the use_label_encoder parameter\nxgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)\nxgb_model.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluate the model\nkappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)\nprint(\"Kappa Score (XGBoost):\", kappa_xgb)\nprint(classification_report(y_test, y_pred_xgb))\n\nKappa Score (XGBoost): 0.29655172413793107\n              precision    recall  f1-score   support\n\n           0       0.98      0.94      0.96       147\n           1       0.25      0.50      0.33         6\n\n    accuracy                           0.92       153\n   macro avg       0.61      0.72      0.65       153\nweighted avg       0.95      0.92      0.93       153\n\n\n\n\n# Define the Gradient Boosting model\ngb_model = GradientBoostingClassifier(\n    learning_rate=0.2,\n    max_depth=5,\n    min_samples_split=10,\n    n_estimators=200,\n    random_state=42\n)\n\n# Fit the model on the resampled training data\ngb_model.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions on the test set\ny_pred_gb = gb_model.predict(X_test)\n\n# Evaluate the model\nkappa_gb = cohen_kappa_score(y_test, y_pred_gb)\nprint(\"Kappa Score (Gradient Boosting):\", kappa_gb)\nprint(classification_report(y_test, y_pred_gb))\n\nKappa Score (Gradient Boosting): 0.4137931034482758\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       147\n           1       0.33      0.67      0.44         6\n\n    accuracy                           0.93       153\n   macro avg       0.66      0.81      0.70       153\nweighted avg       0.96      0.93      0.94       153\n\n\n\n\n# Get predicted probabilities\ny_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n\n# Set a new threshold\nthreshold = 0.3  # Example threshold\ny_pred_adjusted_gb = (y_pred_proba_gb &gt;= threshold).astype(int)\n\n# Evaluate the model with the adjusted predictions\nkappa_adjusted_gb = cohen_kappa_score(y_test, y_pred_adjusted_gb)\nprint(\"Adjusted Kappa Score (Gradient Boosting):\", kappa_adjusted_gb)\nprint(classification_report(y_test, y_pred_adjusted_gb))\n\nAdjusted Kappa Score (Gradient Boosting): 0.36514522821576767\n              precision    recall  f1-score   support\n\n           0       0.99      0.93      0.96       147\n           1       0.29      0.67      0.40         6\n\n    accuracy                           0.92       153\n   macro avg       0.64      0.80      0.68       153\nweighted avg       0.96      0.92      0.94       153\n\n\n\n\n# Experiment with different thresholds\nthresholds = np.arange(0.0, 1.0, 0.05)\nprecisions = []\nrecalls = []\nkappa_scores = []\n\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb &gt;= threshold).astype(int)\n    \n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) &gt; 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) &gt; 0 else 0\n    \n    kappa = cohen_kappa_score(y_test, y_pred_adjusted)\n    \n    precisions.append(precision)\n    recalls.append(recall)\n    kappa_scores.append(kappa)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')\nplt.title('Precision, Recall, and Kappa Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\nbest_threshold_index = np.argmax(recalls)\nbest_threshold = thresholds[best_threshold_index]\nprint(f\"Best Threshold for Maximum Recall: {best_threshold:.2f}\")\nprint(f\"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}\")\nprint(f\"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}\")\nprint(f\"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}\")\n\n\n\n\n\n\n\n\nBest Threshold for Maximum Recall: 0.00\nPrecision at Best Threshold: 0.04\nRecall at Best Threshold: 1.00\nKappa Score at Best Threshold: 0.00\n\n\n\n# Initialize lists to store precision, recall, and F1-score values\nf1_scores = []\n\n# Calculate precision, recall, and F1-score for each threshold\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb &gt;= threshold).astype(int)\n    \n    # Calculate precision and recall\n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) &gt; 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) &gt; 0 else 0\n    \n    # Calculate F1-score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n    \n    # Append F1-score to the list\n    f1_scores.append(f1_score)\n\n# Plot Precision, Recall, and F1-Score curve\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, f1_scores, label='F1 Score', marker='o')\nplt.title('Precision, Recall, and F1 Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\n# Print the best threshold based on maximum F1-score\nbest_threshold_index = np.argmax(f1_scores)\nbest_threshold = thresholds[best_threshold_index]\nprint(f\"Best Threshold for Maximum F1-Score: {best_threshold:.2f}\")\nprint(f\"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}\")\nprint(f\"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}\")\nprint(f\"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}\")\n\n\n\n\n\n\n\n\nBest Threshold for Maximum F1-Score: 0.90\nPrecision at Best Threshold: 0.50\nRecall at Best Threshold: 0.67\nKappa Score at Best Threshold: 0.55\n\n\n\n# Make predictions using the new threshold\ny_pred_final = (gb_model.predict_proba(X_test)[:, 1] &gt;= 0.90).astype(int)\n\n# Evaluate the model with the new predictions\nkappa_final = cohen_kappa_score(y_test, y_pred_final)\nprint(\"Final Kappa Score with Threshold 0.90:\", kappa_final)\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_final))\n\nFinal Kappa Score with Threshold 0.90: 0.5513196480938416\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       147\n           1       0.50      0.67      0.57         6\n\n    accuracy                           0.96       153\n   macro avg       0.74      0.82      0.78       153\nweighted avg       0.97      0.96      0.96       153\n\n\n\n\n# Optionally, you can also calculate and print confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_test, y_pred_final)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Visualize the confusion matrix (optional)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Not OffTask (0)', 'OffTask (1)'], \n            yticklabels=['Not OffTask (0)', 'OffTask (1)'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Perform k-fold cross-validation\ncv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')  # You can change scoring to 'accuracy', 'precision', etc.\n\n# Print the cross-validation scores\nprint(\"Cross-Validation F1 Scores:\", cv_scores)\nprint(\"Mean F1 Score:\", np.mean(cv_scores))\nprint(\"Standard Deviation of F1 Scores:\", np.std(cv_scores))\n\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\n\n\n\n\n\n\n\n\nCross-Validation F1 Scores: [0.25       0.54545455 0.5        0.2        0.        ]\nMean F1 Score: 0.2990909090909091\nStandard Deviation of F1 Scores: 0.20136722754852265\n\n\n\n\n\n Back to top"
  }
]