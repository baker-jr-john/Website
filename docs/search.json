[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Penn GSE",
    "section": "",
    "text": "MIT License\nCopyright © 2024 John Richard Baker Jr.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”) to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright and permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Baker - Learning Analytics",
    "section": "",
    "text": "Core Methods in Educational Data Mining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/index.html",
    "href": "educ_6191_001/index.html",
    "title": "Core Methods in Educational Data Mining",
    "section": "",
    "text": "Creative Assignments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/index.html",
    "href": "educ_6191_001/creative_assignments/index.html",
    "title": "Creative Assignments",
    "section": "",
    "text": "No matching items\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "title": "Behavior Detection",
    "section": "",
    "text": "# Import libraries\n1import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, cohen_kappa_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\n\n\n1\n\nImporting Libraries\n\n\n\n\n\npandas: Used for data manipulation.\ntrain_test_split: Used to split the dataset into training and testing sets.\nGridSearchCV: Helps in finding the optimal hyperparameters for the model.\nRandomForestClassifier: A decision-tree-based classifier that combines many trees to improve performance.\nSMOTE: Synthetic Minority Over-sampling Technique, used to handle class imbalance in the training data.\nclassification_report: Used to generate a detailed report on model performance metrics.\n\n\n# Load the dataset\n2data = pd.read_csv('data/ca1-dataset.csv')\n\n\n2\n\nLoading the Dataset\n\n\n\n\n\nLoads the dataset from the CSV file.\n\n\n# Prepare the data\n3data['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable\nX = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features\ny = data['OffTask']  # Target variable\n\n\n3\n\nPreparing the Data\n\n\n\n\n\nThe target variable OffTask is encoded, converting ‘N’ (No) to 0 and ‘Y’ (Yes) to 1.\nX is set as the feature set by dropping irrelevant columns (‘Unique-id’, ‘namea’, ‘OffTask’).\ny is the target variable, which is the encoded OffTask column.\n\n\n# Split the data into training and testing sets\n4X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n4\n\nSplitting Data\n\n\n\n\n\nThe dataset is split into training (80%) and testing sets (20%).\n\n\n# Apply SMOTE to the training data\n5smote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n\n5\n\nHandling Imbalanced Data with SMOTE\n\n\n\n\n\nSMOTE is applied to balance the classes in the training data. It creates synthetic samples for the minority class (OffTask == 1).\nX_train_resampled and y_train_resampled contain the balanced training data.\n\n\n# Calculate the ratio of classes\n6class_0_count = sum(y_train_resampled == 0)\nclass_1_count = sum(y_train_resampled == 1)\nratio_of_classes = class_0_count / class_1_count\n\n\n6\n\nClass Distribution Check\n\n\n\n\n\nCounts the number of instances in each class after resampling, to calculate the class ratio (0 = not off-task, 1 = off-task).\n\n\n# Define the model\n7model = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n\n7\n\nDefining the Model\n\n\n\n\n\nA RandomForest model is instantiated. The class_weight=‘balanced’ argument is used to adjust weights inversely proportional to class frequencies.\n\n\n# Define the parameter grid\n8param_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n\n8\n\nSetting Up Hyperparameter Tuning (GridSearchCV)\n\n\n\n\n\nDefines the parameter grid for GridSearchCV to find the optimal combination of hyperparameters:\n\nn_estimators: Number of trees.\nmax_depth: Maximum depth of trees.\nmin_samples_split: Minimum samples required to split a node.\nmin_samples_leaf: Minimum number of samples required in a leaf node.\n\n\n\n# Set up GridSearchCV\n9grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n                           scoring='f1', cv=5, n_jobs=-1, verbose=2)\n\n\n9\n\nGridSearchCV for Hyperparameter Tuning\n\n\n\n\n\nGridSearchCV is initialized with:\n\nmodel: The RandomForest classifier.\nparam_grid: The defined hyperparameter grid.\nscoring=‘f1’: Uses the F1 score as the evaluation metric.\ncv=5: Performs 5-fold cross-validation.\nn_jobs=-1: Utilizes all available CPU cores.\nverbose=2: Provides detailed output.\n\n\n\n# Fit GridSearchCV\n10grid_search.fit(X_train_resampled, y_train_resampled)\n\n\n10\n\nFitting the Model\n\n\n\n\nFitting 5 folds for each of 108 candidates, totalling 540 fits\n\n\nGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(class_weight='balanced',\n                                              random_state=42),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 10, 20, 30],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10],\n                         'n_estimators': [50, 100, 200]},\n             scoring='f1', verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(class_weight='balanced',\n                                              random_state=42),\n             n_jobs=-1,\n             param_grid={'max_depth': [None, 10, 20, 30],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5, 10],\n                         'n_estimators': [50, 100, 200]},\n             scoring='f1', verbose=2) best_estimator_: RandomForestClassifierRandomForestClassifier(class_weight='balanced', max_depth=20, n_estimators=50,\n                       random_state=42)  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(class_weight='balanced', max_depth=20, n_estimators=50,\n                       random_state=42) \n\n\n\nFits the GridSearchCV on the resampled training data to find the best hyperparameters.\n\n\n# Best parameters\nprint(\"Best parameters found: \", grid_search.best_params_)\n\nBest parameters found:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n\n\n\n# Train the model on the resampled data\nmodel.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nkappa = cohen_kappa_score(y_test, y_pred)\nprint(\"Kappa Score:\", kappa)\nprint(classification_report(y_test, y_pred))\n\nKappa Score: 0.40175953079178883\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       147\n           1       0.38      0.50      0.43         6\n\n    accuracy                           0.95       153\n   macro avg       0.68      0.73      0.70       153\nweighted avg       0.96      0.95      0.95       153\n\n\n\n\n# Train the XGBoost model without the use_label_encoder parameter\nxgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)\nxgb_model.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluate the model\nkappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)\nprint(\"Kappa Score (XGBoost):\", kappa_xgb)\nprint(classification_report(y_test, y_pred_xgb))\n\nKappa Score (XGBoost): 0.29655172413793107\n              precision    recall  f1-score   support\n\n           0       0.98      0.94      0.96       147\n           1       0.25      0.50      0.33         6\n\n    accuracy                           0.92       153\n   macro avg       0.61      0.72      0.65       153\nweighted avg       0.95      0.92      0.93       153\n\n\n\n\n# Define the Gradient Boosting model\ngb_model = GradientBoostingClassifier(\n    learning_rate=0.2,\n    max_depth=5,\n    min_samples_split=10,\n    n_estimators=200,\n    random_state=42\n)\n\n# Fit the model on the resampled training data\ngb_model.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions on the test set\ny_pred_gb = gb_model.predict(X_test)\n\n# Evaluate the model\nkappa_gb = cohen_kappa_score(y_test, y_pred_gb)\nprint(\"Kappa Score (Gradient Boosting):\", kappa_gb)\nprint(classification_report(y_test, y_pred_gb))\n\nKappa Score (Gradient Boosting): 0.4137931034482758\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       147\n           1       0.33      0.67      0.44         6\n\n    accuracy                           0.93       153\n   macro avg       0.66      0.81      0.70       153\nweighted avg       0.96      0.93      0.94       153\n\n\n\n\n# Get predicted probabilities\ny_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n\n# Set a new threshold\nthreshold = 0.3  # Example threshold\ny_pred_adjusted_gb = (y_pred_proba_gb &gt;= threshold).astype(int)\n\n# Evaluate the model with the adjusted predictions\nkappa_adjusted_gb = cohen_kappa_score(y_test, y_pred_adjusted_gb)\nprint(\"Adjusted Kappa Score (Gradient Boosting):\", kappa_adjusted_gb)\nprint(classification_report(y_test, y_pred_adjusted_gb))\n\nAdjusted Kappa Score (Gradient Boosting): 0.36514522821576767\n              precision    recall  f1-score   support\n\n           0       0.99      0.93      0.96       147\n           1       0.29      0.67      0.40         6\n\n    accuracy                           0.92       153\n   macro avg       0.64      0.80      0.68       153\nweighted avg       0.96      0.92      0.94       153\n\n\n\n\n# Experiment with different thresholds\nthresholds = np.arange(0.0, 1.0, 0.05)\nprecisions = []\nrecalls = []\nkappa_scores = []\n\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb &gt;= threshold).astype(int)\n    \n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) &gt; 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) &gt; 0 else 0\n    \n    kappa = cohen_kappa_score(y_test, y_pred_adjusted)\n    \n    precisions.append(precision)\n    recalls.append(recall)\n    kappa_scores.append(kappa)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')\nplt.title('Precision, Recall, and Kappa Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\nbest_threshold_index = np.argmax(recalls)\nbest_threshold = thresholds[best_threshold_index]\nprint(f\"Best Threshold for Maximum Recall: {best_threshold:.2f}\")\nprint(f\"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}\")\nprint(f\"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}\")\nprint(f\"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}\")\n\n\n\n\n\n\n\n\nBest Threshold for Maximum Recall: 0.00\nPrecision at Best Threshold: 0.04\nRecall at Best Threshold: 1.00\nKappa Score at Best Threshold: 0.00\n\n\n\n# Initialize lists to store precision, recall, and F1-score values\nf1_scores = []\n\n# Calculate precision, recall, and F1-score for each threshold\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb &gt;= threshold).astype(int)\n    \n    # Calculate precision and recall\n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) &gt; 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) &gt; 0 else 0\n    \n    # Calculate F1-score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n    \n    # Append F1-score to the list\n    f1_scores.append(f1_score)\n\n# Plot Precision, Recall, and F1-Score curve\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, f1_scores, label='F1 Score', marker='o')\nplt.title('Precision, Recall, and F1 Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\n# Print the best threshold based on maximum F1-score\nbest_threshold_index = np.argmax(f1_scores)\nbest_threshold = thresholds[best_threshold_index]\nprint(f\"Best Threshold for Maximum F1-Score: {best_threshold:.2f}\")\nprint(f\"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}\")\nprint(f\"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}\")\nprint(f\"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}\")\n\n\n\n\n\n\n\n\nBest Threshold for Maximum F1-Score: 0.90\nPrecision at Best Threshold: 0.50\nRecall at Best Threshold: 0.67\nKappa Score at Best Threshold: 0.55\n\n\n\n# Make predictions using the new threshold\ny_pred_final = (gb_model.predict_proba(X_test)[:, 1] &gt;= 0.90).astype(int)\n\n# Evaluate the model with the new predictions\nkappa_final = cohen_kappa_score(y_test, y_pred_final)\nprint(\"Final Kappa Score with Threshold 0.90:\", kappa_final)\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_final))\n\nFinal Kappa Score with Threshold 0.90: 0.5513196480938416\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       147\n           1       0.50      0.67      0.57         6\n\n    accuracy                           0.96       153\n   macro avg       0.74      0.82      0.78       153\nweighted avg       0.97      0.96      0.96       153\n\n\n\n\n# Optionally, you can also calculate and print confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_test, y_pred_final)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Visualize the confusion matrix (optional)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Not OffTask (0)', 'OffTask (1)'], \n            yticklabels=['Not OffTask (0)', 'OffTask (1)'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Perform k-fold cross-validation\ncv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')  # You can change scoring to 'accuracy', 'precision', etc.\n\n# Print the cross-validation scores\nprint(\"Cross-Validation F1 Scores:\", cv_scores)\nprint(\"Mean F1 Score:\", np.mean(cv_scores))\nprint(\"Standard Deviation of F1 Scores:\", np.std(cv_scores))\n\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\n\n\n\n\n\n\n\n\nCross-Validation F1 Scores: [0.25       0.54545455 0.5        0.2        0.        ]\nMean F1 Score: 0.2990909090909091\nStandard Deviation of F1 Scores: 0.20136722754852265\n\n\n\n\n\n Back to top"
  }
]