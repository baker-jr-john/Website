[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Penn GSE",
    "section": "",
    "text": "MIT License\nCopyright © 2024 John Richard Baker Jr.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”) to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright and permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/index.html",
    "href": "educ_6191_001/creative_assignments/index.html",
    "title": "Creative Assignments",
    "section": "",
    "text": "Knowledge Structure Mapping: a Comprehensive Report\n\n\n\n\n\nAn in-depth exploration of knowledge structure mapping using Factor Analysis, K-Means clustering, and PCA to uncover latent skills in an eight-item test dataset\n\n\n\n\n\nNovember 20, 2024\n\n\nJohn Baker\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Enhanced Behavior Detector: A Machine Learning Approach\n\n\n\n\n\nDeveloping an improved behavior classifier using feature engineering and ensemble methods.\n\n\n\n\n\nOctober 2, 2024\n\n\nJohn Baker\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection\n\n\n\n\n\nA machine learning model to detect off-task behavior\n\n\n\n\n\nSeptember 18, 2024\n\n\nJohn Baker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "educ_6191_001/index.html",
    "href": "educ_6191_001/index.html",
    "title": "Core Methods in Educational Data Mining",
    "section": "",
    "text": "Creative Assignments\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Baker – Learning Analytics",
    "section": "",
    "text": "Core Methods in Educational Data Mining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "",
    "text": "In the field of educational data mining, detecting off-task behavior is crucial for understanding student engagement and improving learning outcomes. Off-task behavior refers to any student actions unrelated to the learning objectives, which can hinder the educational process. Traditional methods of identifying off-task behavior are often subjective and resource-intensive. Therefore, developing automated, accurate detection methods using machine learning can significantly benefit educators and learners.\nThis study presents an in-depth analysis of machine learning models designed to classify off-task behavior in educational settings. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through experiments and analyses, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#introduction",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "",
    "text": "In the field of educational data mining, detecting off-task behavior is crucial for understanding student engagement and improving learning outcomes. Off-task behavior refers to any student actions unrelated to the learning objectives, which can hinder the educational process. Traditional methods of identifying off-task behavior are often subjective and resource-intensive. Therefore, developing automated, accurate detection methods using machine learning can significantly benefit educators and learners.\nThis study presents an in-depth analysis of machine learning models designed to classify off-task behavior in educational settings. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through experiments and analyses, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#literature-review",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#literature-review",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Literature Review",
    "text": "Literature Review\n\nBackground on Off-Task Behavior Detection\nOff-task behavior detection in educational settings has been an area of active research for several years, with studies employing various approaches to identify and analyze student disengagement.\n\nTraditional Methods\nEarly research relied heavily on human observation and self-reporting techniques. While these methods provided rich qualitative data, they were often subjective, time-consuming, and not scalable for large-scale implementation (Baker 2007).\n\n\nMachine Learning Approaches\nThe advent of intelligent tutoring systems and educational software has enabled more sophisticated detection methods using machine learning:\n\nLog File Analysis: Researchers have developed models that analyze student interaction logs to identify patterns indicative of off-task behavior. These models often utilize features such as time spent on tasks, response correctness, and help-seeking behavior (Cocea and Weibelzahl 2009; Pardos et al. 2014).\nMultimodal Detection: Some studies have incorporated multiple data sources to create more comprehensive off-task behavior detection systems (Bosch et al. 2015).\nTemporal Models: Researchers have explored the use of sequential models to capture the temporal aspects of student behavior and improve detection accuracy (Liu and Koedinger 2017).\n\n\n\nChallenges in Off-Task Behavior Detection\nSeveral challenges have been identified in the field:\n\nClass Imbalance: Off-task behavior is typically less frequent than on-task behavior, leading to imbalanced datasets that can skew model performance (Pardos et al. 2014).\nContext Sensitivity: The definition of off-task behavior can vary depending on the learning environment and task making it difficult to create universally applicable models (Baker 2007).\nPrivacy Concerns: As detection methods become more sophisticated, they often require more invasive data collection, raising ethical and privacy issues (Bosch et al. 2015). This is particularly relevant in educational settings where student data protection is paramount.\nReal-time Detection: Developing models that can detect off-task behavior in real-time to enable immediate intervention remains a significant challenge (Liu and Koedinger 2017), especially in resource-constrained educational environments.\n\n\n\nRecent Trends\nRecent research has focused on:\n\nPersonalized Models: Developing detection systems that adapt to individual student behaviors and learning patterns (Pardos et al. 2014).\nInterpretable AI: Creating models that not only detect off-task behavior but also provide insights into the reasons behind it (Cocea and Weibelzahl 2009). This trend aligns with this study’s focus on model comparison and evaluation metrics, as interpretable models can offer valuable insights for educators.\nIntegration with Intervention Strategies: Combining detection models with automated intervention systems to re-engage students in real-time (Liu and Koedinger 2017).\n\n\n\nEducational Context in e-Learning Environments\nIn the context of e-learning environments, off-task behavior can significantly impact learning outcomes. Cocea and Weibelzahl found that students who frequently engage in off-task behavior in e-learning environments show lower learning gains and decreased problem-solving skills (Cocea and Weibelzahl 2009). The abstract nature of some concepts makes sustained engagement crucial for skill development, highlighting the importance of accurate off-task behavior detection in these environments.\nMy study builds upon existing work by addressing the persistent challenge of class imbalance and exploring advanced machine learning techniques to improve off-task behavior detection accuracy. A focus on threshold optimization and model comparison provides valuable insights into the practical implementation of these detection systems in educational settings, particularly for tutoring systems where maintaining student engagement is critical for learning success.\nBy comparing multiple classifiers and employing techniques like SMOTE, this research contributes to the ongoing effort to develop more robust and accurate off-task behavior detection models. Furthermore, an emphasis on performance metrics such as Cohen’s Kappa and F1-score addresses the need for comprehensive evaluation in imbalanced datasets, a critical aspect often overlooked in previous studies."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#methodology",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#methodology",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Methodology",
    "text": "Methodology\nThis study employed a multi-step approach to develop and evaluate machine learning models:\n\nData Preparation: I utilized a dataset containing features related to student behavior, with a binary target variable indicating off-task status (OffTask: Y/N).\nData Overview: The dataset contains 616 student interactions with a close-loop tutoring system. Each entry includes 29 features capturing various aspects of student performance, such as correctness of responses, help-seeking behavior, and time spent on tasks. Key features include:\n\nBinary indicator of off-task behavior\nPerformance metrics (e.g., average correct responses, errors)\nTime-related features\nError and help-seeking metrics\nRecent performance indicators\n\n\nThis data allows for analysis of learning patterns and the effectiveness of the tutoring system in teaching.\n\nModel Selection: I implemented three classifiers: Random Forest, XGBoost, and Gradient Boosting.\nHandling Class Imbalance: To address the imbalanced nature of the dataset, I applied the Synthetic Minority Over-sampling Technique (SMOTE).\nHyperparameter Tuning: I used GridSearchCV to optimize model parameters, focusing on maximizing the F1-score.\nThreshold Optimization: I explored various decision thresholds to balance precision and recall, particularly for the minority class (off-task behavior).\nPerformance Evaluation: I assessed model performance using metrics such as Cohen’s Kappa score, precision, recall, F1-score, and confusion matrices.\nCross-Validation: I employed k-fold cross-validation to ensure robust performance estimates across different data subsets.\n\n\nData Preparation\nI began by importing the necessary libraries and loading the dataset:\n\n# Import libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, cohen_kappa_score, confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\ndata = pd.read_csv('data/ca1-dataset.csv')\n\nI then prepared the data by encoding the target variable and selecting relevant features:\n\n# Prepare the data\ndata['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable\nX = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features\ny = data['OffTask']  # Target variable\n\n\n\nHandling Class Imbalance with SMOTE\nThe dataset exhibited class imbalance, with significantly more instances of “Not OffTask” than “OffTask.” To address this issue, I applied SMOTE to the training data:\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Calculate the ratio of classes\nclass_0_count = sum(y_train_resampled == 0)\nclass_1_count = sum(y_train_resampled == 1)\nratio_of_classes = class_0_count / class_1_count\n\n\n\nModel Selection and Hyperparameter Tuning\n\nRandom Forest Classifier\nI defined the Random Forest model and set up a parameter grid for hyperparameter tuning:\n\n# Define the model\nmodel_rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n# Define the parameter grid\nparam_grid_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Set up GridSearchCV with corrected parameter names and variables\ngrid_search_rf = GridSearchCV(estimator=model_rf, param_grid=param_grid_rf,\n                              scoring='f1', cv=5, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV\ngrid_search_rf.fit(X_train_resampled, y_train_resampled)\n\n# Best parameters\nprint(\"Best parameters found for Random Forest: \", grid_search_rf.best_params_)\n\nFitting 5 folds for each of 108 candidates, totalling 540 fits\nBest parameters found for Random Forest:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n\n\n\n\nXGBoost Classifier\nI initialized the XGBoost model, adjusting for class imbalance using scale_pos_weight:\n\n# Define the XGBoost model\nxgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)\n\n# Fit the model\nxgb_model.fit(X_train_resampled, y_train_resampled)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) \n\n\n\n\nGradient Boosting Classifier\nI defined the Gradient Boosting model with specific hyperparameters:\n\n# Define the Gradient Boosting model\ngb_model = GradientBoostingClassifier(\n    learning_rate=0.2,\n    max_depth=5,\n    min_samples_split=10,\n    n_estimators=200,\n    random_state=42\n)\n\n# Fit the model on the resampled training data\ngb_model.fit(X_train_resampled, y_train_resampled)\n\nGradientBoostingClassifier(learning_rate=0.2, max_depth=5, min_samples_split=10,\n                           n_estimators=200, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.2, max_depth=5, min_samples_split=10,\n                           n_estimators=200, random_state=42) \n\n\n\n\n\nPerformance Evaluation\nI evaluated each model using the test set and calculated the Cohen’s Kappa score and classification report.\n\nRandom Forest Evaluation\n\n# Make predictions on the test set\ny_pred_rf = grid_search_rf.predict(X_test)\n\n# Evaluate the model\nkappa_rf = cohen_kappa_score(y_test, y_pred_rf)\nprint(\"Kappa Score (Random Forest):\", kappa_rf)\nprint(classification_report(y_test, y_pred_rf))\n\nKappa Score (Random Forest): 0.40175953079178883\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       147\n           1       0.38      0.50      0.43         6\n\n    accuracy                           0.95       153\n   macro avg       0.68      0.73      0.70       153\nweighted avg       0.96      0.95      0.95       153\n\n\n\n\n\nXGBoost Evaluation\n\n# Make predictions\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluate the model\nkappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)\nprint(\"Kappa Score (XGBoost):\", kappa_xgb)\nprint(classification_report(y_test, y_pred_xgb))\n\nKappa Score (XGBoost): 0.29655172413793107\n              precision    recall  f1-score   support\n\n           0       0.98      0.94      0.96       147\n           1       0.25      0.50      0.33         6\n\n    accuracy                           0.92       153\n   macro avg       0.61      0.72      0.65       153\nweighted avg       0.95      0.92      0.93       153\n\n\n\n\n\nGradient Boosting Evaluation\n\n# Make predictions on the test set\ny_pred_gb = gb_model.predict(X_test)\n\n# Evaluate the model\nkappa_gb = cohen_kappa_score(y_test, y_pred_gb)\nprint(\"Kappa Score (Gradient Boosting):\", kappa_gb)\nprint(classification_report(y_test, y_pred_gb))\n\nKappa Score (Gradient Boosting): 0.4137931034482758\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       147\n           1       0.33      0.67      0.44         6\n\n    accuracy                           0.93       153\n   macro avg       0.66      0.81      0.70       153\nweighted avg       0.96      0.93      0.94       153\n\n\n\n\n\n\nThreshold Optimization\nTo improve the detection of off-task behavior, I experimented with adjusting the decision threshold:\n\n# Get predicted probabilities\ny_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n\n# Experiment with different thresholds\nthresholds = np.arange(0.0, 1.0, 0.05)\nprecisions = []\nrecalls = []\nkappa_scores = []\n\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb &gt;= threshold).astype(int)\n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) &gt; 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) &gt; 0 else 0\n    kappa = cohen_kappa_score(y_test, y_pred_adjusted)\n    precisions.append(precision)\n    recalls.append(recall)\n    kappa_scores.append(kappa)\n\n# Plot Precision, Recall, and Kappa Score vs. Threshold\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')\nplt.title('Precision, Recall, and Kappa Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nI determined that a threshold of 0.90 maximized the F1-score for the off-task class.\n\n# Apply the optimal threshold\nbest_threshold = 0.90\ny_pred_final = (gb_model.predict_proba(X_test)[:, 1] &gt;= best_threshold).astype(int)\n\n# Evaluate the model with the new predictions\nkappa_final = cohen_kappa_score(y_test, y_pred_final)\nprint(\"Final Kappa Score with Threshold 0.90:\", kappa_final)\nprint(classification_report(y_test, y_pred_final))\n\nFinal Kappa Score with Threshold 0.90: 0.5513196480938416\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       147\n           1       0.50      0.67      0.57         6\n\n    accuracy                           0.96       153\n   macro avg       0.74      0.82      0.78       153\nweighted avg       0.97      0.96      0.96       153\n\n\n\n\n\nConfusion Matrix and Cross-Validation\nI computed the confusion matrix and performed k-fold cross-validation to assess model stability:\n\n# Calculate and print confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred_final)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Visualize the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Not OffTask (0)', 'OffTask (1)'],\n            yticklabels=['Not OffTask (0)', 'OffTask (1)'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Perform k-fold cross-validation\ncv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')\n\n# Print the cross-validation scores\nprint(\"Cross-Validation F1 Scores:\", cv_scores)\nprint(\"Mean F1 Score:\", np.mean(cv_scores))\nprint(\"Standard Deviation of F1 Scores:\", np.std(cv_scores))\n\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\n\n\n\n\n\n\n\n\nCross-Validation F1 Scores: [0.25       0.54545455 0.5        0.2        0.        ]\nMean F1 Score: 0.2990909090909091\nStandard Deviation of F1 Scores: 0.20136722754852265"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#results",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Results",
    "text": "Results\n\nModel Performance Comparison\nThe best hyperparameters found for the Random Forest Classifier were:\n\nmax_depth: 20\nmin_samples_leaf: 1\nmin_samples_split: 2\nn_estimators: 50\n\nThe Cohen’s Kappa Scores for the models were:\n\nRandom Forest: 0.4018\nXGBoost: 0.2966\nGradient Boosting: 0.5513 (after threshold optimization)\n\n\n\nThreshold Optimization Insights\nAdjusting the decision threshold significantly impacted the model’s performance:\n\nAt Threshold 0.90:\n\nPrecision (OffTask): 0.50\nRecall (OffTask): 0.67\nF1-score (OffTask): 0.57\nCohen’s Kappa Score: 0.5513\n\n\n\n\nConfusion Matrix Analysis\nThe confusion matrix at the optimal threshold was:\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\nTrue Positives: 4\nFalse Positives: 4\nTrue Negatives: 143\nFalse Negatives: 2\n\n\n\nCross-Validation Results\n\nCross-Validation F1 Scores: [0.25, 0.5455, 0.5, 0.2, 0.0]\nMean F1 Score: 0.299\nStandard Deviation: 0.201"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#discussion",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Discussion",
    "text": "Discussion\n\nChallenges with Class Imbalance\nThe imbalanced dataset posed significant challenges:\n\nDifficulty in Learning Minority Class Patterns: The scarcity of OffTask instances made it hard for models to generalize.\nOverfitting Risk: Without proper handling, models could overfit to the majority class.\n\n\n\nEffectiveness of SMOTE\nApplying SMOTE helped in:\n\nBalancing the Dataset: Synthetic samples improved the representation of the minority class.\nImproving Recall: The model improved at identifying OffTask instances.\n\nHowever, reliance on synthetic data might not capture the complexity of actual off-task behavior.\n\n\nThreshold Optimization Trade-offs\n\nImproved Detection: A higher threshold increased the precision for the OffTask class.\nFalse Positives and Negatives: Adjusting the threshold affected the balance between missing actual OffTask instances and incorrectly flagging Not OffTask instances.\n\n\n\nModel Selection Insights\n\nGradient Boosting Superiority: Its ability to focus on misclassified instances led to better performance.\nRandom Forest and XGBoost Limitations: These models were less effective, possibly due to their parameter sensitivity and handling of imbalanced data.\n\n\n\nCross-Validation Variability\nThe significant standard deviation in cross-validation scores suggests:\n\nModel Instability: Performance varied across different data splits.\nNeed for Robustness: Further techniques are required to ensure consistent performance."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#limitations",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#limitations",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Limitations",
    "text": "Limitations\nWhile this study provides valuable insights into off-task behavior detection in tutoring systems, it’s important to acknowledge several limitations:\n\nDataset Constraints\n\nSize: The dataset, while substantial, is limited to 616 student interactions. A larger dataset might reveal additional patterns or improve model generalizability.\nContext: The data may not generalize well to cerain subjects or learning environments.\nTemporal aspects: The data represents a snapshot in time and doesn’t capture long-term changes in student behavior or learning patterns.\n\n\n\nFeature Selection\n\nLimited feature set: I relied on 29 pre-defined features. There may be other relevant features not captured in the dataset that could improve detection accuracy.\nFeature interpretability: Some features, particularly those related to recent performance indicators, are challenging to interpret in an educational context.\n\n\n\nModel Limitations\n\nModel selection: While I compared several classifiers, there are other advanced models (e.g., deep learning architectures) that I didn’t explore due to computational constraints.\nHyperparameter tuning: Despite using GridSearchCV, I may not have exhaustively explored all possible hyperparameter combinations.\n\n\n\nClass Imbalance Handling\n\nSMOTE limitations: While SMOTE helped address class imbalance, it generates synthetic examples which may not perfectly represent real-world off-task behavior.\nAlternative techniques: Other class imbalance handling techniques (e.g., adaptive boosting, cost-sensitive learning) were not explored and could potentially yield different results.\n\n\n\nPerformance Metrics\n\nMetric selection: I focused on Cohen’s Kappa and F1-score. Other metrics might provide additional insights into model performance.\nThreshold sensitivity: The results are sensitive to the chosen decision threshold, which may not be optimal for all use cases.\n\n\n\nGeneralizability\n\nStudent population: The dataset may not represent the full diversity of student populations, potentially limiting the model’s applicability across different demographics.\nEducational system specificity: The patterns of off-task behavior detected may be specific to the particular tutoring systems used and might not generalize to other educational software.\n\n\n\nReal-world Application\n\nReal-time detection: This study doesn’t address the challenges of implementing these models for real-time off-task behavior detection in live classroom settings.\nComputational resources: The computational requirements for running these models may be a limiting factor for widespread adoption in resource-constrained educational environments.\n\n\n\nLack of Qualitative Insights\n\nStudent perspective: My quantitative approach does not capture students’ own perceptions of their engagement or reasons for off-task behavior.\nContextual factors: Environmental or personal factors that might influence off-task behavior are not accounted for in the model.\n\n\n\nValidation in Live Settings\n\nControlled environment: The models were developed and tested on historical data. Their performance in live, dynamic classroom environments remains to be validated.\n\nThese limitations provide opportunities for future research to build upon and refine my approach to off-task behavior detection in educational settings."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#ethical-considerations-in-off-task-behavior-detection",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#ethical-considerations-in-off-task-behavior-detection",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Ethical Considerations in Off-Task Behavior Detection",
    "text": "Ethical Considerations in Off-Task Behavior Detection\nThe implementation of off-task behavior detection systems in educational settings raises several ethical concerns that researchers and practitioners must carefully consider:\n\nPrivacy and Data Protection\n\nCollection of sensitive data: Off-task behavior detection often involves collecting detailed data about student activities, potentially including keystroke patterns, eye movements, or even facial expressions. This level of monitoring raises significant privacy concerns.\nData storage and security: Ensuring the secure storage and transmission of student data is crucial to prevent unauthorized access or breaches.\nCompliance with regulations: Researchers must adhere to data protection regulations such as GDPR in Europe or FERPA and COPPA in the United States, which have strict guidelines on handling student data.\n\n\n\nInformed Consent\n\nStudent awareness: Students (and their parents/guardians for minors) should be fully informed about what data is being collected, how it will be used, and who will have access to it.\nOpt-out options: Providing students with the ability to opt-out of monitoring without academic penalty is an important ethical consideration.\n\n\n\nBias and Fairness\n\nAlgorithmic bias: Machine learning models may inadvertently perpetuate or amplify existing biases related to race, gender, or socioeconomic status. Ensuring fairness in off-task behavior detection across diverse student populations is crucial.\nCultural sensitivity: What constitutes “off-task” behavior may vary across cultures, and detection systems should be designed with cultural differences in mind.\n\n\n\nTransparency and Explainability\n\nInterpretable models: Using interpretable AI models allows for better understanding of how off-task behavior is being detected, which is important for both educators and students.\nClear communication: The criteria for determining off-task behavior should be clearly communicated to students and educators.\n\n\n\nPotential for Misuse\n\nOver-reliance on technology: There’s a risk that educators might rely too heavily on automated systems, potentially overlooking important contextual factors in student behavior.\nPunitive use: Safeguards should be in place to prevent the use of off-task behavior data for punitive measures rather than supportive interventions.\n\n\n\nPsychological Impact\n\nStress and anxiety: Constant monitoring could lead to increased stress and anxiety among students, potentially impacting their learning and well-being.\nSelf-fulfilling prophecies: Labeling students as frequently “off-task” could negatively impact their self-perception and motivation.\n\n\n\nData Retention and Right to be Forgotten\n\nLimited data retention: Implementing policies for how long data is kept and when it should be deleted.\nStudent rights: Allowing students to request the deletion of their data, especially after they’ve left the educational institution.\n\n\n\nContextual Considerations\n\nFlexibility in detection: Recognizing that brief off-task moments can be part of the learning process and not always detrimental.\nAdaptive systems: Developing systems that can adapt to individual student learning styles and needs.\n\n\n\nStakeholder Involvement\n\nInclusive design: Involving educators, students, and parents in the design and implementation of off-task behavior detection systems.\nOngoing evaluation: Regularly assessing the impact and effectiveness of these systems with input from all stakeholders.\n\nBy addressing these ethical considerations, researchers and educators can work towards developing off-task behavior detection systems that are not only effective but also respect student rights, promote fairness, and contribute positively to the learning environment."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#conclusion",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Conclusion",
    "text": "Conclusion\nThis study highlights the complexities involved in detecting off-task behavior using machine learning. Key findings include:\n\nGradient Boosting Effectiveness: With proper tuning and threshold adjustment, it outperformed other models.\nImportance of Handling Class Imbalance: Techniques like SMOTE are crucial but have limitations.\nThreshold Optimization: Essential for improving minority class detection but requires careful trade-off consideration.\n\n\nFuture Work\n\nAdvanced Imbalance Handling: Explore cost-sensitive learning and ensemble methods.\nFeature Engineering: Incorporate more behavioral indicators to improve model accuracy.\nReal-world Implementation: Test models in live educational settings for practical validation."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#submission-guidelines",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#submission-guidelines",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "",
    "text": "Understanding student behavior within educational software environments is crucial for providing timely interventions and enhancing learning outcomes. Off-task behavior, in particular, can negatively impact learning efficacy. Accurate detection of such behavior allows educators to address issues promptly and tailor educational experiences to individual student needs.\nThis project builds upon previous work by engineering new features derived from detailed logs of student interactions. By integrating these features with existing ones and applying advanced machine learning techniques, I aim to develop an improved behavior detector that can more accurately identify off-task behaviors."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#introduction",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "",
    "text": "Understanding student behavior within educational software environments is crucial for providing timely interventions and enhancing learning outcomes. Off-task behavior, in particular, can negatively impact learning efficacy. Accurate detection of such behavior allows educators to address issues promptly and tailor educational experiences to individual student needs.\nThis project builds upon previous work by engineering new features derived from detailed logs of student interactions. By integrating these features with existing ones and applying advanced machine learning techniques, I aim to develop an improved behavior detector that can more accurately identify off-task behaviors."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#literature-review",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#literature-review",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "Literature Review",
    "text": "Literature Review\n\nDetecting Off-Task Behavior and Addressing Algorithmic Bias in Learning Systems\nEducational Data Mining (EDM) has emerged as a significant field, leveraging student data to enhance learning outcomes. Recent research has focused on developing algorithms and metrics to address algorithmic bias in education and other related fields (Cohausz, Kappenberger, and Stuckenschmidt 2024). Analyzing student data can provide valuable insights into factors influencing academic performance, including social connections (Siemens and Baker 2012).\nA particularly relevant area within EDM for this study is detecting student misuse of educational systems. Baker and Siemens (Siemens and Baker 2012) explored how data mining techniques can identify instances where students “game the system” in constraint-based tutors. This concept is pertinent to identifying off-task behavior, a broader category of student misuse.\nOff-task behavior encompasses actions where students deviate from their intended engagement with educational software, including disengagement, inappropriate tool use, or attempts to circumvent learning activities. “Gaming the system” (Ryan SJD Baker, Yacef, et al. 2009) can be understood as a specific manifestation of off-task behavior in which students exploit system mechanics to achieve desired outcomes without genuine engagement.\nOther relevant methodologies and ethical considerations include:\n\nThe use of “text replays” to gain a deeper understanding of student behavior (Sao Pedro, Baker, and Gobert 2012; Slater, Baker, and Wang 2020), which could potentially be adapted for analyzing off-task behavior patterns.\nAddressing fairness and bias in machine learning models used in educational contexts (Cohausz, Kappenberger, and Stuckenschmidt 2024; Ryan S. Baker and Hawn 2022), ensuring that models for detecting off-task behavior are equitable and do not unfairly disadvantage certain student groups."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#methodology",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#methodology",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "Methodology",
    "text": "Methodology\n\nData Preparation\nI began by importing essential libraries for data manipulation and machine learning, loading the datasets (ca1 and ca2) from CSV files.\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, cohen_kappa_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\n\n# Load datasets\nca1 = pd.read_csv('data/ca1-dataset.csv')\nca2 = pd.read_csv('data/ca2-dataset.csv')\n\n\nca1-dataset.csv: Contains existing features related to student interactions.\nca2-dataset.csv: Provides detailed logs of student actions, from which new features are engineered.\n\nBoth datasets were imported into Pandas dataframes for manipulation and analysis.\n\nDataset Overview\nca1-dataset.csv:\n\nEntries: 763 rows\nColumns: 27\nData Types: Numeric and categorical\nMissing Values: None\n\nca2-dataset.csv:\n\nEntries: 1,745 rows\nColumns: 34\nData Types: Numeric and categorical\nMissing Values: None\n\nKey insights:\n\nThe ca1-dataset.csv contains aggregated data for 20-second intervals, while ca2-dataset.csv provides more granular information about individual student actions.\nBoth datasets share a common Unique-id field, allowing for integration of the new features.\n\nPreprocessing steps:\n\nConverted categorical variables to numerical using one-hot encoding.\nNormalized numerical features to ensure consistent scale across all variables.\n\n\n\n\nFeature Engineering\nFrom ca2-dataset.csv, I engineered several user-specific features to capture behavioral patterns:\n\nAction Frequency: Total number of actions per user within the 20-second interval.\n\nCalculation: Count of actions for each Unique-id.\nRationale: Higher frequency may indicate engagement or potentially off-task rapid clicking.\n\nAverage Time Between Actions: Mean time interval between consecutive actions.\n\nCalculation: Mean of time differences between consecutive actions for each Unique-id.\nRationale: Longer intervals might suggest disengagement or thoughtful consideration.\n\nMaximum Action Duration: Longest time interval between actions.\n\nCalculation: Maximum time difference between consecutive actions for each Unique-id.\nRationale: Extremely long durations could indicate off-task behavior or system issues.\n\nAction Diversity: Number of unique actions performed.\n\nCalculation: Count of distinct action types for each Unique-id.\nRationale: Higher diversity might indicate more engaged, on-task behavior.\n\nIdle Time Ratio: Proportion of time spent idle (no actions recorded).\n\nCalculation: Sum of time intervals exceeding 5 seconds divided by total interval time.\nRationale: Higher idle time may suggest off-task behavior or disengagement.\n\n\n\n# Action Frequency, Avg Time Between Actions, Max Action Duration, Action Diversity, Idle/Active Ratio\naction_freq = ca2.groupby('Unique-id')['Row'].count().reset_index()\naction_freq.columns = ['Unique-id', 'Action_Frequency']\n\nca2['time'] = pd.to_datetime(ca2['time'], errors='coerce')\nca2 = ca2.sort_values(by=['Unique-id', 'time'])\nca2['Time_Diff'] = ca2.groupby('Unique-id')['time'].diff().dt.total_seconds()\navg_time_diff = ca2.groupby('Unique-id')['Time_Diff'].mean().reset_index()\navg_time_diff.columns = ['Unique-id', 'Avg_Time_Between_Actions']\n\nmax_time_diff = ca2.groupby('Unique-id')['Time_Diff'].max().reset_index()\nmax_time_diff.columns = ['Unique-id', 'Max_Action_Duration']\n\naction_diversity = ca2.groupby('Unique-id')['prod'].nunique().reset_index()\naction_diversity.columns = ['Unique-id', 'Action_Diversity']\n\nca2['Idle_Time'] = ca2['Time_Diff'].apply(lambda x: x if x &gt; 60 else 0)\ntotal_idle_time = ca2.groupby('Unique-id')['Idle_Time'].sum().reset_index()\ntotal_active_time = ca2.groupby('Unique-id')['Time_Diff'].sum().reset_index()\ntotal_active_time.columns = ['Unique-id', 'Total_Active_Time']\nidle_active_ratio = total_idle_time.merge(total_active_time, on='Unique-id')\nidle_active_ratio['Idle_Active_Ratio'] = idle_active_ratio['Idle_Time'] / idle_active_ratio['Total_Active_Time']\n\nThese features aim to quantify user engagement and detect patterns indicative of off-task behavior.\n\n\nData Merging and Cleaning\nThe new features were merged with ca1-dataset.csv based on the Unique-id key. Missing values in numerical columns were handled using mean imputation to ensure the integrity of the dataset for modeling. Categorical variables were encoded using one-hot encoding to prepare them for machine learning algorithms.\n\n# Merging the new features into the original ca1-dataset.csv\nca1_enhanced = ca1.merge(action_freq, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(avg_time_diff, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(max_time_diff, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(action_diversity, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(idle_active_ratio[['Unique-id', 'Idle_Active_Ratio']], on='Unique-id', how='left')\n\n# Handling missing values using mean imputation\nnumeric_cols = ca1_enhanced.select_dtypes(include=['number']).columns\nca1_enhanced[numeric_cols] = ca1_enhanced[numeric_cols].fillna(ca1_enhanced[numeric_cols].mean())\n\n\n\nModel Development\nI developed two primary models to compare the effectiveness of the newly engineered features:\n\nModel 1: Original Features\nA Random Forest Classifier was trained using only the original features from ca1-dataset.csv. This serves as a baseline model to evaluate the impact of the new features. The target variable was the OffTask indicator, converted to a binary format.\n\n# Model Development using RandomForestClassifier\noriginal_features = ['Avgright', 'Avgbug', 'Avghelp', 'Avgchoice', 'Avgstring', 'Avgnumber', 'Avgpoint', 'Avgpchange', 'Avgtime', 'AvgtimeSDnormed', 'Avgtimelast3SDnormed', 'Avgtimelast5SDnormed', 'Avgnotright', 'Avghowmanywrong-up', 'Avghelppct-up', 'Avgwrongpct-up', 'Avgtimeperact-up', 'AvgPrev3Count-up', 'AvgPrev5Count-up', 'Avgrecent8help', 'Avg recent5wrong', 'Avgmanywrong-up', 'AvgasymptoteA-up', 'AvgasymptoteB-up']\n\n# Separate features and target variable ('OffTask')\nX_original = ca1_enhanced[original_features]\ny = ca1_enhanced['OffTask'].apply(lambda x: 1 if x == 'Y' else 0)\n\n# Split the dataset into train and test sets\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original, y, test_size=0.2, random_state=42)\n\n# Build the RandomForest model\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_orig, y_train_orig)\n\n# Predict and evaluate the model\ny_pred_orig = rf.predict(X_test_orig)\nauc_orig = roc_auc_score(y_test_orig, y_pred_orig)\nkappa_orig = cohen_kappa_score(y_test_orig, y_pred_orig)\nprint(f\"AUC: {auc_orig}, Kappa: {kappa_orig}\")\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=42) \n\n\nAUC: 0.5833333333333334, Kappa: 0.2776203966005666\n\n\n\n\nModel 2: Combined Features\nThe second model incorporated both original and new features. I performed hyperparameter tuning using GridSearchCV to optimize the Random Forest Classifier.\n\n# Combined Features\nnew_features = ['Action_Frequency', 'Avg_Time_Between_Actions', 'Max_Action_Duration', 'Action_Diversity', 'Idle_Active_Ratio']\nX_combined = ca1_enhanced[original_features + new_features]\nX_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}\n\n# Initialize the RandomForestClassifier\nrf = RandomForestClassifier(random_state=42)\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='roc_auc')\n\n# Fit GridSearchCV\ngrid_search.fit(X_train_comb, y_train_comb)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n\nprint(f'Best parameters found: {best_params}')\n\nBest parameters found: {'bootstrap': False, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n\n\nBased on the grid search, the best model configuration is a RandomForestClassifier with:\n\nNo bootstrapping (bootstrap=False)\nMaximum depth of 10 (max_depth=10)\nMinimum of 2 samples per leaf node (min_samples_leaf=2)\nMinimum of 2 samples required to split an internal node (min_samples_split=2)\n100 decision trees (n_estimators=100)\n\n\n# Train the model with the best parameters\nbest_rf = RandomForestClassifier(**best_params, random_state=42)\nbest_rf.fit(X_train_comb, y_train_comb)\ny_pred_comb = best_rf.predict(X_test_comb)\n\n# Evaluate the model\nauc_comb = roc_auc_score(y_test_comb, y_pred_comb)\nkappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)\nprint(f'AUC for Combined Features with Best Params: {auc_comb}')\nprint(f'Kappa for Combined Features with Best Params: {kappa_comb}')\n\nRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) \n\n\nAUC for Combined Features with Best Params: 0.75\nKappa for Combined Features with Best Params: 0.6577181208053692\n\n\n\n\n\nAddressing Class Imbalance with SMOTE and Ensemble Modeling\nTo address potential class imbalance in the dataset, Synthetic Minority Over-sampling Technique (SMOTE) was applied only to the training set during each fold of cross-validation. This approach ensures that the test set remains unaltered and representative of the true data distribution.\nAn ensemble model comprising a Random Forest, Logistic Regression, and Support Vector Classifier was built using a soft voting strategy. This ensemble approach aims to leverage the strengths of different algorithms and improve overall prediction accuracy.\n\n# Separate features and target variable\nX = ca1_enhanced[original_features + new_features]\ny = ca1_enhanced['OffTask'].apply(lambda x: 1 if x == 'Y' else 0)\n\n# Split the dataset into train and test sets before SMOTE\nX_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Check original class distribution\nprint(f'Original training set class distribution: {Counter(y_train_comb)}')\n\n# Apply SMOTE to balance the training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)\n\n# Check class distribution after SMOTE\nprint(f'Resampled training set class distribution: {Counter(y_train_resampled)}')\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit the scaler on the training data and transform both training and test data\nX_train_scaled = scaler.fit_transform(X_train_resampled)\nX_test_scaled = scaler.transform(X_test_comb)\n\n# Initialize individual models with appropriate parameters\nrf = RandomForestClassifier(**best_params, random_state=42)\nlr = LogisticRegression(random_state=42, max_iter=1000)\nsvc = SVC(probability=True, random_state=42)\n\n# Create an ensemble model\nensemble = VotingClassifier(\n    estimators=[('rf', rf), ('lr', lr), ('svc', svc)],\n    voting='soft'\n)\n\n# Train the ensemble model\nensemble.fit(X_train_scaled, y_train_resampled)\n\n# Predict on the scaled test data\ny_pred_comb = ensemble.predict(X_test_scaled)\n\n# Evaluate the ensemble model\nauc_comb = roc_auc_score(y_test_comb, y_pred_comb)\nkappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)\nprint(f'AUC for Combined Features with Ensemble: {auc_comb}')\nprint(f'Kappa for Combined Features with Ensemble: {kappa_comb}')\n\nOriginal training set class distribution: Counter({0: 582, 1: 28})\nResampled training set class distribution: Counter({0: 582, 1: 582})\n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAUC for Combined Features with Ensemble: 0.8027210884353742\nKappa for Combined Features with Ensemble: 0.3882224645583424\n\n\n\n\nCross-Validation Strategy\nTo ensure the model’s generalizability and prevent overfitting, I initially employed GroupKFold cross-validation based on Unique-id. However, based on feedback from Jiayi Zhang, I updated my strategy to employ GroupKFold cross-validation based on namea. This approach groups data by students, ensuring all data from one student is used for training or testing, not split between both. It prevents data leakage across folds, as logs from the same student will not appear in both sets—crucial in educational data mining due to highly individual student behavior. Cross-validating based on namea allows the model to learn from some students’ behavior patterns and generalize to others, mirroring real-world usage where the detector should work for new students. This method better indicates the model’s ability to generalize and ensures a more robust, fair evaluation.\nImplementation:\n\n# Cross-validation using GroupKFold with the ensemble model\ngkf = GroupKFold(n_splits=5)\ngroups = ca1_enhanced['namea']  # Change from 'Unique-id' to 'namea'\n\nauc_scores_comb = []\nkappa_scores_comb = []\n\nfor train_idx, test_idx in gkf.split(X_combined, y, groups=groups):\n    X_train_comb, X_test_comb = X_combined.iloc[train_idx], X_combined.iloc[test_idx]\n    y_train_comb, y_test_comb = y.iloc[train_idx], y.iloc[test_idx]\n    \n    # Apply SMOTE to each fold\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)\n    \n    ensemble.fit(X_train_resampled, y_train_resampled)\n    y_pred_comb = ensemble.predict(X_test_comb)\n    auc_scores_comb.append(roc_auc_score(y_test_comb, y_pred_comb))\n    kappa_scores_comb.append(cohen_kappa_score(y_test_comb, y_pred_comb))\n\n# Averaged cross-validation results for combined features with ensemble\navg_auc_comb = sum(auc_scores_comb) / len(auc_scores_comb)\navg_kappa_comb = sum(kappa_scores_comb) / len(kappa_scores_comb)\nprint(f'Average AUC for Combined Features with Ensemble: {avg_auc_comb}')\nprint(f'Average Kappa for Combined Features with Ensemble: {avg_kappa_comb}')\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAverage AUC for Combined Features with Ensemble: 0.7754631036811888\nAverage Kappa for Combined Features with Ensemble: 0.2765342137632087\n\n\nThis approach ensures that our model evaluation reflects its ability to generalize to new students, which is crucial for real-world application in educational settings.\n\n\nFeature Selection\nRecursive Feature Elimination (RFE) was utilized to identify the top ten most significant features. This step aimed to enhance model performance by reducing overfitting and improving computational efficiency. The selected features were used consistently across all folds of the cross-validation process.\n\n# Perform Recursive Feature Elimination (RFE)\nrfe = RFE(estimator=rf, n_features_to_select=10, step=1)\nrfe.fit(X_combined, y)\n\n# Get the selected features\nselected_features = X_combined.columns[rfe.support_]\n\n# Use only the selected features for training and testing\nX_combined_selected = X_combined[selected_features]\n\n# Apply SMOTE to balance the dataset\nX_resampled, y_resampled = smote.fit_resample(X_combined_selected, y)\n\n# Split the resampled data\nX_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Train the ensemble model with selected features\nensemble.fit(X_train_comb, y_train_comb)\ny_pred_comb = ensemble.predict(X_test_comb)\n\n# Evaluate the ensemble model with selected features\nauc_comb = roc_auc_score(y_test_comb, y_pred_comb)\nkappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)\nprint(f'AUC for Combined Features with Ensemble and RFE: {auc_comb}')\nprint(f'Kappa for Combined Features with Ensemble and RFE: {kappa_comb}')\n\nRFE(estimator=RandomForestClassifier(bootstrap=False, max_depth=10,\n                                     min_samples_leaf=2, random_state=42),\n    n_features_to_select=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RFE?Documentation for RFEiFittedRFE(estimator=RandomForestClassifier(bootstrap=False, max_depth=10,\n                                     min_samples_leaf=2, random_state=42),\n    n_features_to_select=10) estimator: RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42)  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAUC for Combined Features with Ensemble and RFE: 0.9417808219178081\nKappa for Combined Features with Ensemble and RFE: 0.8835616438356164\n\n\n\nCross-Validation with Selected Features\n\n# Cross-validation using GroupKFold with the ensemble model and selected features\nauc_scores_comb = []\nkappa_scores_comb = []\n\nfor train_idx, test_idx in gkf.split(X_combined_selected, y, groups=groups):\n    X_train_comb, X_test_comb = X_combined_selected.iloc[train_idx], X_combined_selected.iloc[test_idx]\n    y_train_comb, y_test_comb = y.iloc[train_idx], y.iloc[test_idx]\n    \n    # Apply SMOTE to each fold\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)\n    \n    ensemble.fit(X_train_resampled, y_train_resampled)\n    y_pred_comb = ensemble.predict(X_test_comb)\n    auc_scores_comb.append(roc_auc_score(y_test_comb, y_pred_comb))\n    kappa_scores_comb.append(cohen_kappa_score(y_test_comb, y_pred_comb))\n\n# Averaged cross-validation results for combined features with ensemble and RFE\navg_auc_comb = sum(auc_scores_comb) / len(auc_scores_comb)\navg_kappa_comb = sum(kappa_scores_comb) / len(kappa_scores_comb)\nprint(f'Average AUC for Combined Features with Ensemble and RFE: {avg_auc_comb}')\nprint(f'Average Kappa for Combined Features with Ensemble and RFE: {avg_kappa_comb}')\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAverage AUC for Combined Features with Ensemble and RFE: 0.7600872171563662\nAverage Kappa for Combined Features with Ensemble and RFE: 0.26520511012132253"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#results",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "Results",
    "text": "Results\n\nModel Performance Comparison\nModel 1 (Original Features):\n\nAUC Score: 0.583\nCohen’s Kappa: 0.278\n\nModel 2 (Combined Features with Best Parameters):\n\nAUC Score: 0.75\nCohen’s Kappa: 0.658\n\nModel 2 with Ensemble and SMOTE:\n\nAUC Score: 0.803\nCohen’s Kappa: 0.388\n\nModel 2 with Ensemble, SMOTE, and RFE:\n\nAUC Score: 0.942\nCohen’s Kappa: 0.884\n\nCross-Validation Results (With RFE):\n\nAverage AUC: 0.760\nAverage Cohen’s Kappa: 0.265\n\n\n\nInterpretation of Results\n\nBaseline Model: The original features provided modest predictive power, performing slightly better than random guessing.\nFeature Engineering Impact: Incorporating new features significantly improved model performance, with AUC increasing from 0.583 to 0.75 and Cohen’s Kappa from 0.278 to 0.658.\nEnsemble and SMOTE Effect: Addressing class imbalance and using ensemble methods further improved AUC to 0.803, though Cohen’s Kappa decreased slightly.\nFeature Selection Benefit: RFE led to a substantial performance boost, achieving an AUC of 0.942 and Cohen’s Kappa of 0.884 on the test set.\nCross-Validation Insights: The cross-validation results (AUC: 0.760, Kappa: 0.265) suggest potential overfitting, highlighting the importance of robust validation techniques."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#discussion",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "Discussion",
    "text": "Discussion\nThe progressive enhancements in model performance demonstrate the effectiveness of our feature engineering and model optimization techniques:\n\nFeature Engineering: The introduction of new features derived from ca2-dataset.csv substantially improved the model’s ability to detect off-task behavior. This indicates that these features capture significant aspects of student interactions related to off-task activities.\nHyperparameter Tuning: Optimizing the Random Forest parameters led to better model performance, highlighting the importance of tailoring the model to the data characteristics.\nAddressing Class Imbalance: Applying SMOTE balanced the training data, which is crucial when dealing with imbalanced classes. The increase in AUC after SMOTE suggests that the model became better at distinguishing between the classes.\nEnsemble Modeling: Combining different algorithms (Random Forest, Logistic Regression, and SVC) in an ensemble improved the robustness of the predictions. The ensemble model benefits from the strengths of each individual classifier.\nFeature Selection with RFE: Reducing the feature set to the most significant 10 features using RFE not only simplified the model but also enhanced performance. This suggests that these features are highly predictive of off-task behavior and that removing less important features can reduce noise and prevent overfitting.\nCross-Validation Insights: The cross-validation results, while lower than the test set scores, are critical for assessing how the model might perform on new, unseen data. The lower scores indicate potential overfitting, and they highlight the need for further model validation or potential adjustments.\n\n\nImplications for Educational Interventions\n\nThe top features identified can help educators understand which behaviors are most indicative of off-task activities.\nReal-time monitoring systems can be developed using these key features to alert educators when a student may need intervention.\nThe improved accuracy of off-task behavior detection can lead to more timely and targeted support for students, potentially improving learning outcomes."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#limitations-and-future-work",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#limitations-and-future-work",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "Limitations and Future Work",
    "text": "Limitations and Future Work\nDespite the promising results, several limitations must be acknowledged:\n\nDataset Size: The relatively small dataset may limit the model’s generalizability to broader student populations.\nPotential Overfitting: The discrepancy between test set and cross-validation performance suggests potential overfitting, which needs to be addressed in future iterations.\nFeature Availability: Some engineered features may not be immediately available in real-time scenarios, potentially limiting the model’s applicability in live educational settings.\nExternal Validation: The model has not been tested on external datasets or in real-world educational environments, which is crucial for assessing its true effectiveness.\n\nFuture work should focus on:\n\nExpanding the Dataset: Collecting more diverse data from a larger student population to improve model generalizability.\nReal-time Feature Engineering: Developing methods to calculate and update features in real-time for live intervention systems.\nAdvanced Model Architectures: Exploring deep learning approaches or more sophisticated ensemble methods that might capture complex patterns in student behavior.\nLongitudinal Studies: Conducting long-term studies to assess the model’s effectiveness in improving student engagement and learning outcomes over time.\nInterpretability: Developing tools to explain model predictions to educators and students, ensuring transparency and trust in the system."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#conclusion",
    "title": "Building an Enhanced Behavior Detector: A Machine Learning Approach",
    "section": "Conclusion",
    "text": "Conclusion\nThis study successfully developed an enhanced behavior detector by engineering new features from detailed student interaction data and applying advanced machine learning techniques. The final model demonstrates a high ability to detect off-task behavior, which is crucial for timely educational interventions.\nKey achievements include:\n\nSignificant improvement in AUC (from 0.583 to 0.942) and Cohen’s Kappa (from 0.278 to 0.884) compared to the baseline model.\nDevelopment of 10 novel features that capture nuanced aspects of student behavior.\nImplementation of a robust cross-validation strategy that accounts for student-level grouping.\n\nWhile the results are promising, the identified limitations provide clear directions for future research to further enhance the model’s reliability and applicability in real-world educational settings.\n\nSubmission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "",
    "text": "Knowledge structure mapping is a powerful tool that allows educators to uncover hidden connections between what students know and what they are tested on. By revealing the relationships between test items and the underlying skills they measure, knowledge structure mapping provides crucial insights for developing targeted educational interventions and improving student outcomes. This understanding is essential for creating effective assessments, personalizing instruction, and ensuring that all students have the opportunity to succeed.\nHowever, identifying the optimal representation of latent skills within educational data is a complex challenge. Traditional methods often rely on assumptions that may not generalize across diverse contexts or assessment types. To address this issue, researchers have developed a range of data-driven approaches that aim to uncover skill structures in a more flexible and robust manner.\nThis study presents a comprehensive methodology for identifying the latent skill structure underlying an eight-item test dataset. By leveraging the complementary strengths of Factor Analysis, K-Means Clustering, and Principal Component Analysis (PCA), I aim to derive a robust and interpretable model of the key skills assessed by the test items. My approach involves iteratively testing models with varying numbers of components to determine the optimal balance between model complexity and explanatory power.\nThe resulting three-skill model offers a clear and actionable framework for understanding student performance on the test items. The model’s interpretability and strong empirical foundation make it a valuable tool for informing assessment design, instructional planning, and student support initiatives. By aligning educational practices with the identified skill structure, educators can more effectively foster student learning and achievement.\nMoreover, this study contributes to the broader field of educational data mining and learning analytics by demonstrating the value of a multi-method, data-driven approach to knowledge structure mapping. The methodology presented here can serve as a template for future research aimed at uncovering the hidden skills and competencies that underlie student performance across a wide range of educational contexts and assessment types.\nIn the following sections, I provide an overview of relevant background literature, describe my methodological approach in detail, present the key findings of my analysis, and discuss the implications of my work for educational practice and future research."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#introduction",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "",
    "text": "Knowledge structure mapping is a powerful tool that allows educators to uncover hidden connections between what students know and what they are tested on. By revealing the relationships between test items and the underlying skills they measure, knowledge structure mapping provides crucial insights for developing targeted educational interventions and improving student outcomes. This understanding is essential for creating effective assessments, personalizing instruction, and ensuring that all students have the opportunity to succeed.\nHowever, identifying the optimal representation of latent skills within educational data is a complex challenge. Traditional methods often rely on assumptions that may not generalize across diverse contexts or assessment types. To address this issue, researchers have developed a range of data-driven approaches that aim to uncover skill structures in a more flexible and robust manner.\nThis study presents a comprehensive methodology for identifying the latent skill structure underlying an eight-item test dataset. By leveraging the complementary strengths of Factor Analysis, K-Means Clustering, and Principal Component Analysis (PCA), I aim to derive a robust and interpretable model of the key skills assessed by the test items. My approach involves iteratively testing models with varying numbers of components to determine the optimal balance between model complexity and explanatory power.\nThe resulting three-skill model offers a clear and actionable framework for understanding student performance on the test items. The model’s interpretability and strong empirical foundation make it a valuable tool for informing assessment design, instructional planning, and student support initiatives. By aligning educational practices with the identified skill structure, educators can more effectively foster student learning and achievement.\nMoreover, this study contributes to the broader field of educational data mining and learning analytics by demonstrating the value of a multi-method, data-driven approach to knowledge structure mapping. The methodology presented here can serve as a template for future research aimed at uncovering the hidden skills and competencies that underlie student performance across a wide range of educational contexts and assessment types.\nIn the following sections, I provide an overview of relevant background literature, describe my methodological approach in detail, present the key findings of my analysis, and discuss the implications of my work for educational practice and future research."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#background-and-related-work",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#background-and-related-work",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Background and Related Work",
    "text": "Background and Related Work\nKnowledge structure mapping is a fundamental area of research in educational data mining and learning analytics, focusing on uncovering the latent skills and relationships that underlie student performance on educational assessments (Baker, Barnes, and Beck 2008). By providing insights into the hidden structure of educational data, knowledge structure mapping enables researchers and educators to develop more effective assessments, instructional interventions, and student support systems.\n\nMethods for Knowledge Structure Mapping\nResearchers have developed a range of methods to map knowledge structures, each offering unique advantages and limitations. Factor Analysis, a widely used statistical technique, identifies latent skills by analyzing patterns of correlations among test items (Beavers et al. 2019). This data-driven approach uncovers hidden skill structures without requiring prior knowledge of the relationships.\nIn contrast, Barnes’s Q-matrix method (Barnes 2005) takes a different approach by using a binary matrix to represent item-skill associations. The Q-matrix provides a visual tool for understanding which skills are assessed by each item, making it valuable for cognitive modeling and educational data mining. By explicitly encoding the relationships between items and skills, the Q-matrix enables researchers to develop more interpretable and actionable models of student knowledge.\nK-Means Clustering offers another perspective by grouping items based on response patterns, allowing researchers to infer underlying skills from the emergent clusters (Kargupta et al. 2001). This unsupervised learning technique enables exploratory analysis of skill structures when predefined skill mappings are unavailable. By identifying groups of items that elicit similar student responses, K-Means Clustering can reveal hidden commonalities that may correspond to latent skills.\nPrincipal Component Analysis (PCA is another powerful tool for uncovering latent structures in educational data (Chen et al. 2018). By identifying the principal components that explain the maximum variance in the data, PCA can help researchers identify the key dimensions or skills that underlie student performance. While PCA is not specifically designed for knowledge structure mapping, it can provide valuable insights into the overall structure of the data and inform the interpretation of other methods.\n\n\nApplications in Educational Data Mining\nThe methods described above have been widely applied in educational data mining and learning analytics to support a range of tasks and objectives. One key application area is the development of intelligent tutoring systems and adaptive learning environments (Cukurova et al. 2022). By incorporating knowledge structure mapping techniques, these systems can dynamically assess student skills and provide personalized feedback and recommendations based on individual needs.\nKnowledge structure mapping also plays a crucial role in assessment design and evaluation. By uncovering the latent skills assessed by test items, researchers can develop more valid and reliable assessments that effectively measure student knowledge. This information can also be used to identify areas where assessments may be over- or under-emphasizing certain skills, enabling educators to make informed decisions about assessment design and revision.\n\n\nLimitations and Challenges\nDespite the significant advances in knowledge structure mapping, there are still important limitations and challenges to address. One key issue is the need for more flexible and robust methods that can handle the complexity and diversity of educational data (Gordon and Jorgensen 2003). Many existing methods rely on strong assumptions about the structure of the data or the nature of the skills being assessed, which may not hold across different contexts or domains.\nAnother important challenge is the need for more interpretable and actionable models that can inform educational practice (Chen et al. 2018). While knowledge structure mapping can provide valuable insights into the hidden structure of educational data, translating these insights into concrete recommendations for educators and learners remains a significant challenge.\nTo address these limitations, researchers are exploring new approaches that combine multiple methods and data sources to develop more comprehensive and robust models of student knowledge (Cukurova et al. 2022). There is also growing interest in developing more transparent and explainable models that can provide clear guidance to educators and learners (Gordon and Jorgensen 2003).\nIn the present study, I aim to contribute to this ongoing research effort by presenting a comprehensive methodology for knowledge structure mapping that leverages the strengths of multiple methods to uncover the latent skills underlying an eight-item test dataset. By comparing models with varying levels of complexity and interpretability, I seek to identify the optimal balance between model fit and practical utility. My approach demonstrates the value of a multi-method, data-driven approach to knowledge structure mapping and provides a template for future research in this area."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#methods-used",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#methods-used",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Methods Used",
    "text": "Methods Used\n\nOverview\nTo uncover the latent skills underlying the eight-item test dataset, I employed a multi-method approach that combines Factor Analysis, K-Means Clustering, and PCA. Each method offers unique strengths and limitations, and by leveraging their complementary perspectives, I aimed to develop a more robust and comprehensive understanding of the knowledge structure underlying the data.\nFactor Analysis served as the primary method for identifying latent skills, as it is specifically designed to uncover hidden constructs that explain the patterns of correlations among observed variables (Beavers et al. 2019). K-Means Clustering provided a complementary perspective by grouping items based on their response patterns, allowing me to explore potential skill clusters without imposing strong assumptions about the number or nature of the underlying skills (Kargupta et al. 2001). Finally, PCA was used as a validation technique to assess the stability and robustness of the latent skill structure identified by the other methods (Chen et al. 2018).\nBy comparing the results of these three methods and exploring models with varying levels of complexity, I sought to identify the optimal balance between model fit and interpretability. My goal was to develop a parsimonious and actionable representation of the latent skills that could inform assessment design, instructional planning, and student support initiatives.\n\n\nFactor Analysis\nFactor Analysis was selected as the primary method for identifying latent skills due to its ability to uncover hidden constructs that explain the patterns of correlations among test items (Beavers et al. 2019).\nTo determine the optimal number of factors to retain, I used a combination of statistical criteria and substantive considerations. Specifically, I examined the scree plot of eigenvalues, the percentage of variance explained by each factor, and the interpretability of the resulting factor solutions (Beavers et al. 2019). I also compared models with varying numbers of factors (ranging from two to four) to assess their relative fit and interpretability.\nWhile Factor Analysis is a powerful tool for uncovering latent constructs, it is important to acknowledge its assumptions and limitations. Factor Analysis assumes that the observed variables are continuous and normally distributed, which may not hold for binary or ordinal data (such as the correct or incorrect responses in the present dataset). However, research has shown that Factor Analysis can still provide useful insights when applied to binary data, particularly when the sample size is large and the factor loadings are strong (Watkins 2018).\n\n\nK-Means Clustering\nK-Means Clustering was used as a complementary method to explore potential skill clusters based on item response patterns (Kargupta et al. 2001). Unlike Factor Analysis, K-Means Clustering does not impose strong assumptions about the structure of the data or the nature of the underlying constructs. Instead, it aims to partition the data into a specified number of clusters based on the similarity of their response patterns.\nTo apply K-Means Clustering, I first transformed the data to represent each item as a vector of binary responses across all students. I then used the elbow method to determine the optimal number of clusters, which involves plotting the within-cluster sum of squares (WCSS) against the number of clusters and identifying the “elbow” point where the rate of decrease in WCSS begins to level off (Kargupta et al. 2001). Based on this analysis, I selected a three-cluster solution as the most parsimonious and interpretable representation of the data.\nWhile K-Means Clustering can provide valuable insights into the structure of the data, it, too, has limitations. K-Means Clustering assumes that the clusters are spherical and of equal size, which may not hold in practice (Gordon and Jorgensen 2003). Additionally, the resulting clusters are sensitive to the initial placement of the cluster centroids, which can lead to different solutions across multiple runs of the algorithm (Kargupta et al. 2001). To mitigate these issues, I used multiple random initializations and selected the solution with the lowest WCSS.\n\n\nPrincipal Component Analysis (PCA)\nPCA was employed as a validation technique to assess the stability and robustness of the latent skill structure identified by Factor Analysis and K-Means Clustering (Chen et al. 2018). PCA is a dimensionality reduction technique that aims to identify the principal components that explain the maximum amount of variance in the data.\nTo apply PCA, the data was initially standardized to ensure all items were on a consistent scale. The scree plot of eigenvalues was then examined to identify the optimal number of components for the analysis. PCA was subsequently conducted on the standardized item response data to evaluate the stability and robustness of the underlying skill structure (Chen et al. 2018).\nPCA provides a useful complement to Factor Analysis and K-Means Clustering, as it does not impose strong assumptions about the structure of the data or the nature of the underlying constructs. Instead, it identifies the key dimensions of variation in the data, which can be used to validate the stability and robustness of the latent skill structure identified by the other methods (Chen et al. 2018).\nHowever, it is important to recognize that PCA is a purely data-driven technique and does not necessarily identify constructs that are substantively meaningful or interpretable (Gordon and Jorgensen 2003). Additionally, PCA assumes that the relationships among the observed variables are linear, which may not hold in practice (Chen et al. 2018). Despite these limitations, PCA can still provide valuable insights into the overall structure of the data and inform the interpretation of the latent skill structure."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#implementation-details",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#implementation-details",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nData Preparation\n\nLoading the Data\nThe first step in my analysis was to load and preprocess the eight-item test dataset. The dataset consisted of binary responses (correct or incorrect) from 1,920 students on eight test items. I used the pandas library in Python to load the data into a data frame and perform initial data exploration\n```{python}\n# Import necessary libraries\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('data/8items.csv')\n\n# Display the first few rows of the dataset\ndata.head()\n```\n\n\n\n\nTable 1: First few rows of the dataset\n\n\n\n\n\n\n\n\n\n\nstudent\nitem1\nitem2\nitem3\nitem4\nitem5\nitem6\nitem7\nitem8\n\n\n\n\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n0\n1\n1\n0\n0\n0\n1\n1\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n4\n5\n1\n1\n0\n0\n0\n1\n1\n0\n\n\n\n\n\n\n\n\n\n\nTo prepare the data for analysis, I examined the structure of the data frame and checked for missing values.\n\n```{python}\n# Check the dimensions of the dataset\nprint(f\"Dataset dimensions: {data.shape}\")\n\n# Check for missing values\nprint(\"Missing values in each column:\")\nprint(data.isnull().sum())\n```\n\nDataset dimensions: (1920, 9)\nMissing values in each column:\nstudent    0\nitem1      0\nitem2      0\nitem3      0\nitem4      0\nitem5      0\nitem6      0\nitem7      0\nitem8      0\ndtype: int64\n\n\n\n\n\nFactor Analysis\n\nPreparing Data for Factor Analysis\nI extracted the item response data, excluding any non-item columns such as student identifiers. This step ensured that my analyses focused solely on the patterns of student responses across the eight test items.\n\n```{python}\n# Extract item data (excluding the 'student' column if present)\nitem_data = data.drop(columns=['student'], errors='ignore')\n```\n\n\n\nDetermining the Number of Factors Using Scree Plot\nTo determine the optimal number of factors to retain, I examined a scree plot of eigenvalues.\n\n```{python}\n# Import necessary modules\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom factor_analyzer import FactorAnalyzer\n\n# Standardize the data\nscaler = StandardScaler()\nitem_data_scaled = scaler.fit_transform(item_data)\n\n# Perform factor analysis with maximum factors\nfa_model = FactorAnalyzer(rotation=None)\nfa_model.fit(item_data_scaled)\n```\n\nFactorAnalyzer(rotation=None, rotation_kwargs={})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FactorAnalyzeriFittedFactorAnalyzer(rotation=None, rotation_kwargs={}) \n\n\n\n```{python}\n# Get eigenvalues and variance explained\nev, v = fa_model.get_eigenvalues()\nvariance = fa_model.get_factor_variance()\n\n# Extract variance explained and cumulative variance\nvariance_explained = variance[1]\ncumulative_variance_explained = variance[2]\n\n# Total variance explained by the factors\ntotal_variance_explained = cumulative_variance_explained[-1]\nprint(f\"Total Variance Explained by Factors: {total_variance_explained}\")\n```\n\nTotal Variance Explained by Factors: 0.5554629091890351\n\n\n```{python}\n# Plot the scree plot\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(ev) + 1), ev, 'o-', color='blue')\n_ = plt.title('Scree Plot for Factor Analysis')\n_ = plt.xlabel('Factor Number')\n_ = plt.ylabel('Eigenvalue')\nplt.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 1: Scree Plot for Factor Analysis\n\n\n\n\n\nThe scree plot shows a clear “elbow” after the second factor, where the eigenvalues drop sharply initially and then level off. This “elbow” suggests that the first two factors capture most of the meaningful variance, with subsequent factors contributing relatively little.\n\n\nPerforming Factor Analysis\nI applied Factor Analysis with three components to identify latent skills in the dataset.\n```{python}\n# Retrieve the factor loadings\nfactor_loadings = fa_model.loadings_\n\n# Dynamically determine the number of factors extracted\nn_factors_extracted = factor_loadings.shape[1]\n\n# Create a data frame for the factor loadings\nfactor_loadings_df = pd.DataFrame(\n    factor_loadings,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(n_factors_extracted)]\n)\n\n# Display the factor loadings\nfactor_loadings_df\n```\n\n\n\n\nTable 2: Factor Loadings\n\n\n\n\n\n\n\n\n\n\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\nitem1\n0.039341\n-0.063724\n0.986007\n\n\nitem2\n-0.099743\n0.299886\n-0.010651\n\n\nitem3\n0.806481\n0.044630\n-0.084003\n\n\nitem4\n-0.017268\n0.327678\n0.044305\n\n\nitem5\n0.483557\n-0.001616\n0.331988\n\n\nitem6\n-0.069649\n1.004075\n0.063035\n\n\nitem7\n0.778050\n0.028228\n-0.084929\n\n\nitem8\n0.782307\n0.064981\n-0.078495\n\n\n\n\n\n\n\n\n\n\nThe Factor Analysis revealed three distinct latent skills underlying the eight test items. The first skill was characterized by high loadings on Items 3, 5, 7, and 8. The second skill was defined by high loadings on Items 2, 4, and 6. The third skill was primarily associated with Item 1.\n\n\n\nK-Means Clustering\n\nTransposing Item Data\nI transposed the item data to cluster items based on their response patterns.\n\n```{python}\n# Import K-Means module\nfrom sklearn.cluster import KMeans\n\n# Transpose the item data to have items as rows and students as columns\nitem_data_transposed = item_data_scaled.T\n\n# Specify the number of clusters (skills)\nn_clusters = 3\n\n# Initialize the K-Means model\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\n\n# Fit the model to the transposed item data\nkmeans.fit(item_data_transposed)\n```\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42) \n\n\n\n```{python}\n# Retrieve the cluster labels for each item\ncluster_labels = kmeans.labels_\n\n# Create a data frame to display the item-cluster mapping\nkmeans_q_matrix_df = pd.DataFrame({\n    'Item': item_data.columns,\n    'Mapped_Skill': [f'Skill_{label+1}' for label in cluster_labels]\n})\n```\n\n\n\nDetermining the Number of Clusters Using Elbow Method\nTo determine the optimal number of clusters, I used the elbow method, which involved plotting the within-cluster sum of squares (WCSS) against the number of clusters and identifying the “elbow” point where the rate of decrease in WCSS began to level off.\n\n```{python}\n# Import necessary module\nimport numpy as np\n\n# Calculate WCSS for different number of clusters\nwcss = []\nfor i in range(1, 7):\n    kmeans_elbow = KMeans(n_clusters=i, random_state=42)\n    kmeans_elbow.fit(item_data_transposed)\n    wcss.append(kmeans_elbow.inertia_)\n```\n\nKMeans(n_clusters=1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=1, random_state=42) \n\n\nKMeans(n_clusters=2, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2, random_state=42) \n\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42) \n\n\nKMeans(n_clusters=4, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=4, random_state=42) \n\n\nKMeans(n_clusters=5, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=5, random_state=42) \n\n\nKMeans(n_clusters=6, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=6, random_state=42) \n\n\n```{python}\n# Plot the elbow graph\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, 7), wcss, 'o-', color='red')\n_ = plt.title('Elbow Method for K-Means Clustering')\n_ = plt.xlabel('Number of Clusters')\n_ = plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 2: Elbow Method for K-Means Clustering\n\n\n\n\n\nBased on the elbow plot, I decided a three-cluster solution is the most parsimonious and interpretable representation of the data.\n\n\nApplying K-Means Clustering\nI applied K-Means Clustering to the transformed item response data to explore potential skill clusters based on the similarity of item response patterns (Kargupta et al. 2001).\n```{python}\n# Get unique clusters (skills) from the kmeans_q_matrix_df\nunique_skills = kmeans_q_matrix_df['Mapped_Skill'].unique()\nn_clusters = len(unique_skills)\n\n# Create a binary Q-matrix based on the kmeans clustering results\n# Create an empty matrix of zeros\nbinary_matrix = np.zeros((len(kmeans_q_matrix_df), n_clusters), dtype=int)\n\n# Iterate through the rows of kmeans_q_matrix_df and fill in the appropriate cluster assignment\nfor index, row in kmeans_q_matrix_df.iterrows():\n    skill_index = int(row['Mapped_Skill'].split('_')[1]) - 1  # Extract the skill number and convert to zero-indexed\n    binary_matrix[index, skill_index] = 1\n\n# Create a DataFrame for the binary Q-matrix\nq_matrix_kmeans_binary_df = pd.DataFrame(\n    binary_matrix,\n    index=kmeans_q_matrix_df['Item'],\n    columns=[f'Skill_{i+1}' for i in range(n_clusters)]\n)\n\n# Reset index\nq_matrix_kmeans_binary_df.reset_index(inplace=True)\nq_matrix_kmeans_binary_df.rename(columns={'index': 'Item'}, inplace=True)\n\n# Display the Q-matrix\nq_matrix_kmeans_binary_df\n```\n\n\n\n\nTable 3: Item-Cluster Mapping\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\n0\nitem1\n0\n0\n1\n\n\n1\nitem2\n0\n1\n0\n\n\n2\nitem3\n1\n0\n0\n\n\n3\nitem4\n0\n1\n0\n\n\n4\nitem5\n1\n0\n0\n\n\n5\nitem6\n0\n1\n0\n\n\n6\nitem7\n1\n0\n0\n\n\n7\nitem8\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\nThe resulting clusters closely aligned with the latent skills identified by the Factor Analysis, providing convergent evidence for the three-skill structure underlying the test items.\n\n\n\nPrincipal Component Analysis (PCA)\n\nDetermining the Number of Components Using Scree Plot\nI generated another scree plot to help determine the optimal number of components.\n\n```{python}\n# Import necessary module\nfrom sklearn.decomposition import PCA\n\n# Initialize PCA to get all components\npca = PCA()\npca.fit(item_data_scaled)\n```\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PCA?Documentation for PCAiFittedPCA() \n\n\n```{python}\n# Calculate explained variance\nexplained_variance = pca.explained_variance_\n\n# Plot the scree plot\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(explained_variance) + 1), explained_variance, 'o-', color='green')\n_ = plt.title('Scree Plot for PCA')\n_ = plt.xlabel('Principal Component Number')\n_ = plt.ylabel('Eigenvalue')\nplt.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 3: Scree Plot for PCA\n\n\n\n\n\nSimilar to the scree plot used in Factor Analysis, this plot indicates that the majority of significant variance is explained by the first two factors, while the remaining factors contribute comparatively little additional information.\n\n\nPerforming PCA\nI conducted PCA on the standardized item response data to assess the stability and robustness of the latent skill structure identified by Factor Analysis and K-Means Clustering (Chen et al. 2018).\n\n```{python}\n# Initialize the PCA model with three components\npca_model = PCA(n_components=3)\n\n# Fit the PCA model to the item data\npca_model.fit(item_data_scaled)\n```\n\nPCA(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PCA?Documentation for PCAiFittedPCA(n_components=3) \n\n\n```{python}\n# Retrieve the PCA loadings\npca_loadings = pca_model.components_.T\n\n# Create a data frame for the PCA loadings\npca_loadings_df = pd.DataFrame(\n    pca_loadings,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(3)]\n)\n\n# Display the PCA loadings\npca_loadings_df\n```\n\n\n\n\nTable 4: PCA Loadings\n\n\n\n\n\n\n\n\n\n\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\nitem1\n0.032759\n-0.017652\n0.809766\n\n\nitem2\n-0.089362\n0.467578\n-0.076462\n\n\nitem3\n0.535527\n0.044483\n-0.140383\n\n\nitem4\n-0.013661\n0.517393\n0.094851\n\n\nitem5\n0.380393\n0.015493\n0.517176\n\n\nitem6\n-0.040435\n0.711481\n0.017382\n\n\nitem7\n0.527706\n0.026847\n-0.148140\n\n\nitem8\n0.528353\n0.064950\n-0.141454\n\n\n\n\n\n\n\n\n\n\nThe PCA results largely confirmed the three-skill structure identified by the other methods."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#results",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Results",
    "text": "Results\n\nMapping Items to Skills Using PCA\nTo understand the item-skill relationships further, I created a Q-matrix based on the PCA loadings. Each item is assigned to the skill (principal component) with which it has the highest loading.\n```{python}\n# Convert the PCA loadings to a Q-matrix format (binary)\n# Set a threshold to determine if the item is associated with a skill\nthreshold = 0.2\n\n# Create a binary Q-matrix based on the loadings and the threshold\nq_matrix_binary = (np.abs(pca_loadings_df) &gt; threshold).astype(int)\n\n# Display the Q-matrix\nq_matrix_binary.index.name = 'Item'\nq_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n\n# Reset the index to display it like a table\nq_matrix_binary_df = q_matrix_binary.reset_index()\n\n# Display the Q-matrix\nq_matrix_binary_df\n```\n\n\n\n\nTable 5: PCA Q-Matrix\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\n0\nitem1\n0\n0\n1\n\n\n1\nitem2\n0\n1\n0\n\n\n2\nitem3\n1\n0\n0\n\n\n3\nitem4\n0\n1\n0\n\n\n4\nitem5\n1\n0\n1\n\n\n5\nitem6\n0\n1\n0\n\n\n6\nitem7\n1\n0\n0\n\n\n7\nitem8\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\nComparison Across Methods\nThe mappings obtained from Factor Analysis, K-Means Clustering, and PCA show considerable agreement, suggesting the presence of three distinct latent skills assessed by the test items.\n\n\nTesting Alternative Factor Analysis Models\nI tested Factor Analysis models with two-, three-, and four-factor models to determine the optimal number of latent skills.\n\nFactor Analysis with Four Components\n\n```{python}\n# Performing Factor Analysis with four components to explore the potential presence of additional latent skills\nn_factors_extended = 4\nfa_model_extended = FactorAnalyzer(n_factors=n_factors_extended, rotation=None)\nfa_model_extended.fit(item_data_scaled)\n```\n\nFactorAnalyzer(n_factors=4, rotation=None, rotation_kwargs={})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FactorAnalyzeriFittedFactorAnalyzer(n_factors=4, rotation=None, rotation_kwargs={}) \n\n\n```{python}\n# Get the factor loadings for the 4-component model\nfactor_loadings_extended = fa_model_extended.loadings_\n\n# Create a data frame to visualize the factor loadings for the four-component model\nfactor_loadings_extended_df = pd.DataFrame(\n    factor_loadings_extended,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(n_factors_extended)]\n)\n\n# Reset the index to properly align the \"Item\" column with the factor loadings\nfactor_loadings_extended_df.reset_index(inplace=True)\nfactor_loadings_extended_df.rename(columns={'index': 'Item'}, inplace=True)\n\n# Display the extended factor loadings\nfactor_loadings_extended_df\n```\n\n\n\n\nTable 6: Factor Analysis with Four Components\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\nSkill_4\n\n\n\n\n0\nitem1\n0.058296\n-0.001032\n0.425385\n-0.033357\n\n\n1\nitem2\n-0.113564\n0.444694\n-0.018753\n0.494151\n\n\n2\nitem3\n0.776272\n0.032031\n-0.228715\n0.011729\n\n\n3\nitem4\n-0.015773\n0.481321\n0.018040\n-0.462656\n\n\n4\nitem5\n0.681866\n0.033105\n0.725587\n0.050257\n\n\n5\nitem6\n-0.051020\n0.760484\n-0.002363\n-0.000564\n\n\n6\nitem7\n0.750738\n0.011458\n-0.233651\n-0.018004\n\n\n7\nitem8\n0.754160\n0.054250\n-0.223509\n0.027685\n\n\n\n\n\n\n\n\n\n\nObservations from the Four-Component Model:\n\nComplexity and Overfitting: The four-component model introduces additional complexity without significant gains in explained variance. Some items load significantly on multiple factors, making interpretation challenging.\nItem Loadings:\n\nItem2 and Item4 have substantial loadings on both Skill_2 and Skill_4, indicating overlapping skills.\nItem5 loads highly on both Skill_1 and Skill_3, suggesting it may be measuring a combination of skills.\n\nInterpretability: The overlapping loadings reduce the model’s interpretability, making it less practical for educational applications.\n\n\n\nFactor Analysis with Two Components\n\n```{python}\n# Performing Factor Analysis with two components to explore if a simpler model might explain the relationships\nn_factors_simpler = 2\nfa_model_simpler = FactorAnalyzer(n_factors=n_factors_simpler, rotation=None)\nfa_model_simpler.fit(item_data_scaled)\n```\n\nFactorAnalyzer(n_factors=2, rotation=None, rotation_kwargs={})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FactorAnalyzeriFittedFactorAnalyzer(n_factors=2, rotation=None, rotation_kwargs={}) \n\n\n```{python}\n# Get the factor loadings for the two-component model\nfactor_loadings_simpler = fa_model_simpler.loadings_\n\n# Create a data frame to visualize the factor loadings for the two-component model\nfactor_loadings_simpler_df = pd.DataFrame(\n    factor_loadings_simpler,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(n_factors_simpler)]\n)\n\n# Reset the index and rename it to align with the desired table format\nfactor_loadings_simpler_df.reset_index(inplace=True)\nfactor_loadings_simpler_df.rename(columns={'index': 'Item'}, inplace=True)\n\nfactor_loadings_simpler_df\n```\n\n\n\n\nTable 7: Factor Analysis with Two Components\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\n\n\n\n\n0\nitem1\n0.015398\n-0.007536\n\n\n1\nitem2\n-0.099736\n0.298751\n\n\n2\nitem3\n0.809990\n0.040807\n\n\n3\nitem4\n-0.017753\n0.329312\n\n\n4\nitem5\n0.449164\n0.013942\n\n\n5\nitem6\n-0.070497\n1.006000\n\n\n6\nitem7\n0.781571\n0.024412\n\n\n7\nitem8\n0.786056\n0.061425\n\n\n\n\n\n\n\n\n\n\n\nSimplicity vs. Variance Explained: The two-component model is simpler but explains less variance compared to the three-component model.\nItem Loadings:\n\nItem3, Item5, Item7, and Item8 load highly on Skill_1.\nItem2, Item4, and Item6 load on Skill_2.\nItem1 has very low loadings on both factors, suggesting it may not be well-represented in this model.\n\nLoss of Detail: The two-component model may be too simplistic, failing to capture nuances in the data, particularly the unique contribution of Item1.\n\n\n\n\nVisualizations\n\nDiagrams\nI created a couple Mermaid diagrams to gain further insight.\n```{mermaid}\ngraph TB\n    subgraph PCA\n        PCA_S1[Skill_1] --- PCA_I3[Item 3]\n        PCA_S1 --- PCA_I7[Item 7]\n        PCA_S1 --- PCA_I8[Item 8]\n        \n        PCA_S2[Skill_2] --- PCA_I2[Item 2]\n        PCA_S2 --- PCA_I4[Item 4]\n        PCA_S2 --- PCA_I6[Item 6]\n        \n        PCA_S3[Skill_3] --- PCA_I1[Item 1]\n        PCA_S3 --- PCA_I5[Item 5]\n    end\n    \n    subgraph KMeans\n        KM_S1[Skill_1] --- KM_I3[Item 3]\n        KM_S1 --- KM_I5[Item 5]\n        KM_S1 --- KM_I7[Item 7]\n        KM_S1 --- KM_I8[Item 8]\n        \n        KM_S2[Skill_2] --- KM_I2[Item 2]\n        KM_S2 --- KM_I4[Item 4]\n        KM_S2 --- KM_I6[Item 6]\n        \n        KM_S3[Skill_3] --- KM_I1[Item 1]\n    end\n    \n    subgraph Factor_Analysis\n        FA_S1[Skill_1] --- FA_I3[Item 3]\n        FA_S1 --- FA_I5[Item 5]\n        FA_S1 --- FA_I7[Item 7]\n        FA_S1 --- FA_I8[Item 8]\n        \n        FA_S2[Skill_2] --- FA_I2[Item 2]\n        FA_S2 --- FA_I4[Item 4]\n        FA_S2 --- FA_I6[Item 6]\n        \n        FA_S3[Skill_3] --- FA_I1[Item 1]\n    end\n\n    style PCA fill:#f9f,stroke:#333,stroke-width:2px\n    style KMeans fill:#bbf,stroke:#333,stroke-width:2px\n    style Factor_Analysis fill:#bfb,stroke:#333,stroke-width:2px\n```\n\n\n\n\n\n\ngraph TB\n    subgraph PCA\n        PCA_S1[Skill_1] --- PCA_I3[Item 3]\n        PCA_S1 --- PCA_I7[Item 7]\n        PCA_S1 --- PCA_I8[Item 8]\n        \n        PCA_S2[Skill_2] --- PCA_I2[Item 2]\n        PCA_S2 --- PCA_I4[Item 4]\n        PCA_S2 --- PCA_I6[Item 6]\n        \n        PCA_S3[Skill_3] --- PCA_I1[Item 1]\n        PCA_S3 --- PCA_I5[Item 5]\n    end\n    \n    subgraph KMeans\n        KM_S1[Skill_1] --- KM_I3[Item 3]\n        KM_S1 --- KM_I5[Item 5]\n        KM_S1 --- KM_I7[Item 7]\n        KM_S1 --- KM_I8[Item 8]\n        \n        KM_S2[Skill_2] --- KM_I2[Item 2]\n        KM_S2 --- KM_I4[Item 4]\n        KM_S2 --- KM_I6[Item 6]\n        \n        KM_S3[Skill_3] --- KM_I1[Item 1]\n    end\n    \n    subgraph Factor_Analysis\n        FA_S1[Skill_1] --- FA_I3[Item 3]\n        FA_S1 --- FA_I5[Item 5]\n        FA_S1 --- FA_I7[Item 7]\n        FA_S1 --- FA_I8[Item 8]\n        \n        FA_S2[Skill_2] --- FA_I2[Item 2]\n        FA_S2 --- FA_I4[Item 4]\n        FA_S2 --- FA_I6[Item 6]\n        \n        FA_S3[Skill_3] --- FA_I1[Item 1]\n    end\n\n    style PCA fill:#f9f,stroke:#333,stroke-width:2px\n    style KMeans fill:#bbf,stroke:#333,stroke-width:2px\n    style Factor_Analysis fill:#bfb,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 4: Comparison Across the Three Methods\n\n\n\n\n\n\nMethod Comparison (Factor Analysis, K-Means, PCA)\n\nKey Observations:\n\nConsistency Across Methods: Many items (e.g., Item 3 and Item 7) align similarly across Factor Analysis, K-Means, and PCA, reinforcing the robustness of these mappings.\nItem Overlap: The clustering of items (e.g., Items 3, 7, and 8 under Skill_1) consistently suggests a strong latent skill grouping.\nDiscrepancies: While most items map consistently, some differences (e.g., Item 5 under Factor Analysis vs. PCA) suggest subtle differences in how these methods interpret data structures.\nSkill 3 Representation: This skill emerges consistently across methods but captures fewer items, which might indicate a niche or less represented skill.\n\n\nThe visual comparison highlights overlaps and outliers more effectively than numerical tables, making it easier to identify items that contribute ambiguously to multiple skills or are method-dependent.\n```{mermaid}\ngraph TB\n    subgraph Four_Component_Model\n        FC_S1[Skill_1] --- FC_I3[Item 3]\n        FC_S1 --- FC_I7[Item 7]\n        FC_S1 --- FC_I8[Item 8]\n        FC_S1 -.-&gt; FC_I5[Item 5]\n        \n        FC_S2[Skill_2] --- FC_I6[Item 6]\n        FC_S2 -.-&gt; FC_I2[Item 2]\n        FC_S2 -.-&gt; FC_I4[Item 4]\n        \n        FC_S3[Skill_3] --- FC_I1[Item 1]\n        FC_S3 --- FC_I5\n        \n        FC_S4[Skill_4] -.-&gt; FC_I2\n        FC_S4 -.-&gt; FC_I4\n    end\n    \n    subgraph Three_Component_Model\n        TH_S1[Skill_1] --- TH_I3[Item 3]\n        TH_S1 --- TH_I7[Item 7]\n        TH_S1 --- TH_I8[Item 8]\n        TH_S1 --- TH_I5[Item 5]\n        \n        TH_S2[Skill_2] --- TH_I2[Item 2]\n        TH_S2 --- TH_I4[Item 4]\n        TH_S2 --- TH_I6[Item 6]\n        \n        TH_S3[Skill_3] --- TH_I1[Item 1]\n        TH_S3 -.-&gt; TH_I5\n    end\n    \n    subgraph Two_Component_Model\n        TC_S1[Skill_1] --- TC_I3[Item 3]\n        TC_S1 --- TC_I5[Item 5]\n        TC_S1 --- TC_I7[Item 7]\n        TC_S1 --- TC_I8[Item 8]\n        \n        TC_S2[Skill_2] --- TC_I2[Item 2]\n        TC_S2 --- TC_I4[Item 4]\n        TC_S2 --- TC_I6[Item 6]\n        \n        TC_I1[Item 1&lt;br/&gt;Weak Loadings] -..- TC_S1\n        TC_I1 -..- TC_S2\n    end\n\n    style Two_Component_Model fill:#bfb,stroke:#333,stroke-width:2px\n    style Three_Component_Model fill:#bbf,stroke:#333,stroke-width:2px\n    style Four_Component_Model fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n\n\n\n\n\ngraph TB\n    subgraph Four_Component_Model\n        FC_S1[Skill_1] --- FC_I3[Item 3]\n        FC_S1 --- FC_I7[Item 7]\n        FC_S1 --- FC_I8[Item 8]\n        FC_S1 -.-&gt; FC_I5[Item 5]\n        \n        FC_S2[Skill_2] --- FC_I6[Item 6]\n        FC_S2 -.-&gt; FC_I2[Item 2]\n        FC_S2 -.-&gt; FC_I4[Item 4]\n        \n        FC_S3[Skill_3] --- FC_I1[Item 1]\n        FC_S3 --- FC_I5\n        \n        FC_S4[Skill_4] -.-&gt; FC_I2\n        FC_S4 -.-&gt; FC_I4\n    end\n    \n    subgraph Three_Component_Model\n        TH_S1[Skill_1] --- TH_I3[Item 3]\n        TH_S1 --- TH_I7[Item 7]\n        TH_S1 --- TH_I8[Item 8]\n        TH_S1 --- TH_I5[Item 5]\n        \n        TH_S2[Skill_2] --- TH_I2[Item 2]\n        TH_S2 --- TH_I4[Item 4]\n        TH_S2 --- TH_I6[Item 6]\n        \n        TH_S3[Skill_3] --- TH_I1[Item 1]\n        TH_S3 -.-&gt; TH_I5\n    end\n    \n    subgraph Two_Component_Model\n        TC_S1[Skill_1] --- TC_I3[Item 3]\n        TC_S1 --- TC_I5[Item 5]\n        TC_S1 --- TC_I7[Item 7]\n        TC_S1 --- TC_I8[Item 8]\n        \n        TC_S2[Skill_2] --- TC_I2[Item 2]\n        TC_S2 --- TC_I4[Item 4]\n        TC_S2 --- TC_I6[Item 6]\n        \n        TC_I1[Item 1&lt;br/&gt;Weak Loadings] -..- TC_S1\n        TC_I1 -..- TC_S2\n    end\n\n    style Two_Component_Model fill:#bfb,stroke:#333,stroke-width:2px\n    style Three_Component_Model fill:#bbf,stroke:#333,stroke-width:2px\n    style Four_Component_Model fill:#f9f,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 5: Comparison Across the Three Models\n\n\n\n\n\n\n\nModel Comparison (Two-, Three-, and Four-Component Models)\n\nKey Observations:\n\nTwo-Component Model: Simpler but lacks granularity, as evident in fewer distinct mappings and the merging of certain skills.\nThree-Component Model: Balanced in complexity and interpretability, with clear item-skill relationships (e.g., Items 3, 7, and 8 consistently linked to Skill 1).\nFour-Component Model: Overcomplicates relationships with multiple cross-loadings (e.g., Item 5 linked to both Skill 1 and Skill 3), making the model harder to interpret.\nWeak Loadings (Item 1): Visualizing weak loadings in the two-component model underscores its limited ability to represent all test items adequately.\n\n\nThe diagrams provide a clear visual distinction between the interpretability trade-offs of different models. For instance, they highlight how additional components in the four-component model lead to more overlap, supporting the conclusion that the three-component model is optimal.\n\n\nBroader Insights:\n\nSupport for Prior Work: The diagrams reinforce the findings that a three-component model is the most interpretable and aligns well across methods.\nNew Learnings:\n\nItem-Specific Trends: Items like Item 5 show variability across methods and models, suggesting they may assess complex or multiple skills.\nSkill Coverage: Skills identified in PCA seem broader, potentially capturing more nuanced relationships, while K-Means provides a stricter clustering.\nCross-Method Validation: The diagrams visually validate the multi-method approach, showing where methods agree or diverge.\n\n\n\n\n\nHeatmap of Factor Loadings (Three Components)\nUsing a heatmap, I visualized the factor loadings from the three-component Factor Analysis model.\n```{python}\nimport seaborn as sns\n\n# Create a heatmap to visualize item-skill relationships from Factor Analysis\nplt.figure(figsize=(10, 6))\nsns.heatmap(factor_loadings_df, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='black', cbar=True)\n_ = plt.title('Item-Skill Relationships (Factor Analysis with Three Components)')\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 6: Factor Analysis with Three Components\n\n\n\n\n\n\nKey Observations:\n\nDominant Item-Skill Relationships:\n\nItem 1 strongly loads on Skill 3 (0.99), indicating that it is almost exclusively associated with this latent skill.\nItem 3, Item 7, and Item 8 have high loadings on Skill 1 (0.81, 0.78, and 0.78, respectively), showing that they are closely related to this skill.\nItem 6 is strongly associated with Skill 2 (1.00), suggesting it is a clear indicator of this skill.\n\nCross-Skill Contributions:\n\nItem 5 has moderate loadings on both Skill 1 (0.48) and Skill 3 (0.33), indicating that it measures a mix of these skills.\nItem 2 has a moderate loading on Skill 2 (0.30), with negligible contributions to other skills, suggesting it is moderately representative of this skill but not a strong indicator.\n\nWeak Loadings:\n\nItem 4 shows relatively weak loadings across all skills, with the highest on Skill 2 (0.33). This suggests that it may not align well with any single skill or may be ambiguously measuring multiple skills.\nSimilarly, Item 2 and Item 5 exhibit weak or mixed relationships across skills, warranting further investigation.\n\nDistinct Skills:\n\nSkill 1: Clearly defined by Item 3, Item 7, and Item 8.\nSkill 2: Dominated by Item 6, with some contributions from Item 2 and Item 4.\nSkill 3: Clearly represented by Item 1, with partial contributions from Item 5.\n\n\n\n\nInsights:\n\nItem-Skill Assignment: The heatmap visually confirms the appropriateness of assigning items to the skills based on their dominant factor loadings.\nComplex or Ambiguous Items: Items like Item 5 and Item 4 exhibit weaker or mixed relationships, suggesting potential challenges in their interpretation or measurement of a specific skill.\nSkill Coverage: Each skill appears to have at least one strongly associated item, ensuring that all skills are represented in the model.\n\n\n\n\nBar Charts for Individual Items\nI generated bar charts to illustrate the factor loadings of each item across the three skills.\n```{python}\n# Create bar charts for each item to show its relationship across skills\nnum_items = len(factor_loadings_df.index)\nfig, axes = plt.subplots(num_items, 1, figsize=(9, num_items * 2))\n\nfor i, item in enumerate(factor_loadings_df.index):\n    axes[i].bar(factor_loadings_df.columns, factor_loadings_df.loc[item], color='skyblue')\n    _ = axes[i].set_title(f'Relationship of {item} with Skills')\n    _ = axes[i].set_ylabel('Loading Value')\n    _ = axes[i].set_ylim(-1, 1)\n\nplt.tight_layout()\n\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 7: Bar Charts for Individual Items\n\n\n\n\n\n\nKey Insights:\n\nDominant Item-Skill Relationships:\n\nItem 1: Almost exclusively associated with Skill 3, with a very high loading value (~0.99). It does not meaningfully load on Skill 1 or Skill 2.\nItem 3, Item 7, and Item 8: Strongly associated with Skill 1, with high positive loadings (~0.81 and ~0.78). These items clearly represent this latent skill.\nItem 6: Solely aligned with Skill 2 (loading ~1.00), making it the clearest representative of this skill.\n\nMixed and Moderate Relationships:\n\nItem 5: Shows moderate loadings on both Skill 1 (~0.48) and Skill 3 (~0.33), indicating that it may measure a combination of these skills.\nItem 2: Moderately aligned with Skill 2 (~0.30) but has negligible loadings on the other skills, making it a less prominent representative of any single skill.\n\nAmbiguous or Weak Relationships:\n\nItem 4: Has low to moderate loadings across the board, with the highest (~0.33) on Skill 2. This indicates that the item may be ambiguous or weakly related to the latent skills in this model.\nItem 2: Although moderately associated with Skill 2, its low loadings suggest it does not strongly differentiate itself in measuring this skill.\n\nDistinct Skills:\n\nSkill 1: Clearly defined by Item 3, Item 7, and Item 8.\nSkill 2: Primarily represented by Item 6, with minor contributions from Item 2 and Item 4.\nSkill 3: Dominated by Item 1, with partial contributions from Item 5.\n\n\n\n\nFurther Insight:\n\nSupport for Factor Analysis Findings:\n\nThe charts confirm that the three-component model successfully captures distinct latent skills, with most items showing strong associations with a single skill.\nThe visualization highlights items that load cleanly on one skill (e.g., Item 6 for Skill 2, Item 1 for Skill 3).\n\nAmbiguous Items:\n\nItems like Item 4 and Item 5 demonstrate weaker or mixed relationships, indicating potential issues with their design or alignment with specific skills.\nThese items may require revision or could indicate the need for further exploration of an additional component.\n\nStrength of Representation:\n\nCertain skills (e.g., Skill 1 and Skill 3) have multiple items with high loadings, providing strong representation.\nSkill 2 is highly dependent on a single dominant item (Item 6), which could make it more vulnerable to measurement error.\n\n\n\n\n\n\nCreating the Final Q-Matrix\nBased on the consistency of results across methods, I developed a final Q-matrix that maps each item to its primary associated skill based on the three-factor model. Table 8 presents the final Q-matrix, which shows a clear and interpretable mapping of items to skills.\n```{python}\n# Creating the final Q-matrix based on the visualization and analysis findings\n# Assigning each item to the skill with the highest loading from the Factor Analysis with three components\nfinal_q_matrix = factor_loadings_df.idxmax(axis=1)\n\n# Create a data frame to visualize the final Q-matrix, showing the mapping between items and skills\nfinal_q_matrix_df = pd.DataFrame({'Item': item_data.columns, 'Mapped_Skill': final_q_matrix.values})\n\n# Set a threshold to determine the significant loading\nthreshold = 0.2\n\n# Create a binary Q-matrix based on the factor loadings and the threshold\nq_matrix_binary = (np.abs(factor_loadings_df) &gt; threshold).astype(int)\n\n# Rename index and columns for better readability in the Q-matrix\nq_matrix_binary.index.name = 'Item'\nq_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n\n# Reset the index to present it as a table\nq_matrix_binary_df = q_matrix_binary.reset_index()\n\n# Display the final Q-matrix\nq_matrix_binary_df\n```\n\n\n\n\nTable 8: Final Q-Matrix\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\n0\nitem1\n0\n0\n1\n\n\n1\nitem2\n0\n1\n0\n\n\n2\nitem3\n1\n0\n0\n\n\n3\nitem4\n0\n1\n0\n\n\n4\nitem5\n1\n0\n1\n\n\n5\nitem6\n0\n1\n0\n\n\n6\nitem7\n1\n0\n0\n\n\n7\nitem8\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\nI also developed another diagram to support the final Q-Matrix.\n```{mermaid}\ngraph LR\n    subgraph Final_Q_Matrix_Mappings\n        S1[Skill_1] --- I3[Item 3]\n        S1 --- I5[Item 5]\n        S1 --- I7[Item 7]\n        S1 --- I8[Item 8]\n        \n        S2[Skill_2] --- I2[Item 2]\n        S2 --- I4[Item 4]\n        S2 --- I6[Item 6]\n        \n        S3[Skill_3] --- I1[Item 1]\n    end\n    \n    style Final_Q_Matrix_Mappings fill:#bfb,stroke:#333,stroke-width:2px\n```\n\n\n\n\n\n\ngraph LR\n    subgraph Final_Q_Matrix_Mappings\n        S1[Skill_1] --- I3[Item 3]\n        S1 --- I5[Item 5]\n        S1 --- I7[Item 7]\n        S1 --- I8[Item 8]\n        \n        S2[Skill_2] --- I2[Item 2]\n        S2 --- I4[Item 4]\n        S2 --- I6[Item 6]\n        \n        S3[Skill_3] --- I1[Item 1]\n    end\n    \n    style Final_Q_Matrix_Mappings fill:#bfb,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 8: Final Q-Matirx\n\n\n\n\n\n\n\nKey Strengths of the Final Q-Matrix and Diagram\n\nClear Mapping:\n\nEach item is assigned to the skill with the highest loading, ensuring that the relationships are driven by the statistical analysis.\nThe diagram visually highlights these relationships, making it easy to understand and communicate the structure.\n\nSkill Representation:\n\nSkill 1: Represented by four items (Item 3, Item 5, Item 7, and Item 8), providing robust coverage and reliability for assessing this skill.\nSkill 2: Supported by three items (Item 2, Item 4, and Item 6), with Item 6 being the strongest indicator.\nSkill 3: Represented by Item 1, a highly specific item exclusively aligned with this skill.\n\nAlignment with Analyses:\n\nThe Q-matrix directly reflects the findings from the Factor Analysis heatmap and bar charts, ensuring consistency and validation of the mappings.\n\nBalanced Complexity:\n\nBy selecting three components, the Q-matrix strikes a balance between interpretability and detail, avoiding the over-complexity of a four-component model while capturing nuances missed in a two-component model.\n\n\n\n\nObservations and Recommendations\n\nStrength of Item Representation:\n\nSkill 3 relies on a single item (Item 1). While Item 1 has a strong loading, additional items (e.g., Item 5) may be needed to ensure the skill is robustly assessed.\nSkill 2 shows moderate contributions from Item 2 and Item 4, which might require review to ensure their alignment with this skill.\n\nAmbiguous Items:\n\nItem 5 has a mixed loading (moderate on Skill 1 and Skill 3), but its assignment to Skill 1 aligns well with the overall structure.\nItem 4 has weaker loadings but is still included under Skill 2, reflecting its statistical alignment while acknowledging its relative ambiguity.\n\n\n\n\nModel Evaluation Metrics\n\nCalculating Proportion of Variance Explained (\\(R^2\\))\nFor Factor Analysis:\n\n```{python}\n# Compute the communalities\ncommunalities = fa_model.get_communalities()\n\n# Total variance explained\ntotal_variance_explained = np.sum(communalities)\n\n# Total variance (number of variables)\ntotal_variance = item_data_scaled.shape[1]\n\n# Proportion of variance explained\nr_squared_fa = total_variance_explained / total_variance\n\nprint(f\"Factor Analysis R^2: {r_squared_fa:.2f}\")\n```\n\nFactor Analysis R^2: 0.56\n\n\nInterpretation:\n\nThe \\(R^2\\) value of 0.56 indicates that the three-factor model explains 56% of the total variance in the data.\nImplications:\n\nA proportion of variance explained greater than 50% is generally considered acceptable in exploratory Factor Analysis, especially with psychological or educational data where constructs are often complex.\nHowever, it also suggests that 44% of the variance is not explained by the model, which may be due to measurement error, unique variance of items, or additional latent factors not captured by the model.\n\n\nFor PCA:\n\n```{python}\n# Calculate cumulative variance explained\ncumulative_variance = np.cumsum(pca_model.explained_variance_ratio_)\nprint(f\"PCA cumulative variance explained by first 3 components: {cumulative_variance[2]:.2f}\")\n```\n\nPCA cumulative variance explained by first 3 components: 0.66\n\n\nInterpretation:\n\nThe first three principal components explain 66% of the total variance in the data.\nImplications:\n\nThis indicates a slightly better variance explanation than the Factor Analysis model.\nPCA aims to capture the maximum variance with the fewest components, so a higher cumulative variance explained is desirable.\nHowever, PCA components may not be as interpretable as factors from Factor Analysis, since PCA components are linear combinations that maximize variance without considering underlying latent constructs.\n\n\nComparison:\n\nThe PCA model explains more variance (66%) compared to the Factor Analysis model (56%).\nThis difference may be due to the methodological differences between PCA and Factor Analysis:\n\nPCA focuses on capturing variance and is sensitive to the scale of the data.\nFactor Analysis models the underlying latent constructs and accounts for measurement error.\n\n\nConsiderations:\n\nAdequacy of Variance Explained:\n\nIn social sciences, cumulative variance explained between 50% and 75% is generally acceptable.\nBoth models fall within this range, but there is room for improvement.\n\nUnexplained Variance:\n\nThe unexplained variance suggests that additional factors or components might exist, or that some items do not fit well within the identified latent skills.\n\n\n\n\nCalculating Cohen’s Kappa Coefficient\nI also examined the consistency of item assignments across methods using Cohen’s kappa coefficient (Cohen 1960).\n\n```{python}\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.optimize import linear_sum_assignment\n\n# Map skill labels to numeric codes for Factor Analysis\nfa_skill_codes = final_q_matrix_df['Mapped_Skill'].map({'Skill_1': 0, 'Skill_2': 1, 'Skill_3': 2}).values\n\n# K-Means cluster labels\nkmeans_labels = kmeans.labels_\n\n# Compute confusion matrix\nconfusion = confusion_matrix(fa_skill_codes, kmeans_labels)\nprint(\"Confusion Matrix:\")\nprint(confusion)\n\n# Align clusters with skills using the Hungarian algorithm\nrow_ind, col_ind = linear_sum_assignment(-confusion)\nmapping = dict(zip(col_ind, row_ind))\n\n# Map K-Means labels to Factor Analysis skill codes\nkmeans_labels_mapped = np.array([mapping[label] for label in kmeans_labels])\n\n# Compute Cohen's kappa\nfrom sklearn.metrics import cohen_kappa_score\n\nkappa = cohen_kappa_score(fa_skill_codes, kmeans_labels_mapped)\nprint(f\"Cohen's kappa after alignment: {kappa:.2f}\")\n```\n\nConfusion Matrix:\n[[4 0 0]\n [0 3 0]\n [0 0 1]]\nCohen's kappa after alignment: 1.00\n\n\nInterpretation:\n\nConfusion Matrix:\n\nThe confusion matrix shows perfect agreement between the methods after alignment:\n\nAll items assigned to Skill 1 in Factor Analysis are also assigned to the corresponding cluster in K-Means.\nThe same applies to Skills 2 and 3.\n\n\nCohen’s Kappa Value:\n\nA Kappa value of 1.00 indicates perfect agreement between the two methods after alignment.\n\nImplications:\n\nThis high level of agreement suggests that both methods are consistently identifying the same underlying item-skill structures.\nIt provides strong validation for the robustness of your item-skill mappings.\n\n\nConsiderations:\n\nAlignment Step:\n\nThe necessity of aligning clusters to skills underscores that cluster labels are arbitrary.\nIt’s important to perform this alignment to make meaningful comparisons.\n\nCohen’s Kappa Interpretation:\n\nKappa values range from -1 to 1, where:\n\n&lt; 0: Less than chance agreement.\n0–0.20: Slight agreement.\n0.21–0.40: Fair agreement.\n0.41–0.60: Moderate agreement.\n0.61–0.80: Substantial agreement.\n0.81–1.00: Almost perfect agreement.\n\nA value of 1.00 confirms that the two methods are in complete concordance post-alignment.\n\n\n\n\n\nOverall Evaluation\nStrengths:\n\nConverging Evidence:\n\nThe high Cohen’s Kappa value indicates that different analytical methods converge on the same item-skill mappings, enhancing confidence in the results.\n\nVariance Explained:\n\nBoth Factor Analysis and PCA explain a substantial portion of the variance, supporting the validity of the three-component model.\n\nMethodological Rigor:\n\nMy approach of using multiple methods and comparing them through quantitative metrics strengthens the robustness of the findings.\n\n\nLimitations:\n\nVariance Not Explained:\n\nApproximately 34% to 44% of the variance remains unexplained, which could be due to:\n\nMeasurement error.\nAdditional latent skills not captured by the model.\nUnique variances of items.\n\n\nAssumptions of Methods:\n\nFactor Analysis and PCA assumptions may not be fully met with binary data, which could affect the variance explained.\n\n\n\nVerifying Item-Skill Mappings\n```{python}\nfinal_q_matrix_df\n```\n\n\n\n\nTable 9: Factor Analysis Mappings\n\n\n\n\n\n\n\n\n\n\nItem\nMapped_Skill\n\n\n\n\n0\nitem1\nSkill_3\n\n\n1\nitem2\nSkill_2\n\n\n2\nitem3\nSkill_1\n\n\n3\nitem4\nSkill_2\n\n\n4\nitem5\nSkill_1\n\n\n5\nitem6\nSkill_2\n\n\n6\nitem7\nSkill_1\n\n\n7\nitem8\nSkill_1\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nItem Assignments: Each item is assigned to the skill with which it has the highest factor loading from the final Q-matrix.\nSkill Representation:\n\nSkill_1: Items 3, 5, 7, 8\nSkill_2: Items 2, 4, 6\nSkill_3: Item 1\n\n\nSignificance:\n\nConsistent Mapping: The assignments reflect the conclusions drawn from my Factor Analysis.\nFoundation for Comparison: These mappings serve as the reference point for comparing with the K-Means Clustering results.\n\n```{python}\nkmeans_q_matrix_df\n```\n\n\n\n\nTable 10: K-Means Clustering Mappings (before alignment)\n\n\n\n\n\n\n\n\n\n\nItem\nMapped_Skill\n\n\n\n\n0\nitem1\nSkill_3\n\n\n1\nitem2\nSkill_2\n\n\n2\nitem3\nSkill_1\n\n\n3\nitem4\nSkill_2\n\n\n4\nitem5\nSkill_1\n\n\n5\nitem6\nSkill_2\n\n\n6\nitem7\nSkill_1\n\n\n7\nitem8\nSkill_1\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nCluster Assignments: Items are assigned to clusters labeled as Skill_1, Skill_2, or Skill_3, based on the K-Means Clustering algorithm.\nArbitrary Labels: The cluster labels (e.g., Skill_1, Skill_2) are assigned by the algorithm and do not necessarily correspond to the skills identified in Factor Analysis.\n\nSignificance:\n\nInitial Comparison: At first glance, the mappings appear similar to the Factor Analysis mappings, but due to arbitrary labeling, a direct comparison isn’t meaningful yet.\nNeed for Alignment: To accurately compare the item-skill assignments, cluster labels must be aligned with the skills from Factor Analysis.\n\n\n```{python}\n# Map clusters to skills after alignment\nkmeans_skill_names_aligned = ['Skill_' + str(mapping[label] + 1) for label in kmeans_labels]\nkmeans_q_matrix_df_aligned = kmeans_q_matrix_df.copy()\nkmeans_q_matrix_df_aligned['Mapped_Skill'] = kmeans_skill_names_aligned\n```\n\nProcess:\n\nAlignment Using the Hungarian Algorithm:\n\nSince cluster labels are arbitrary, I used the Hungarian algorithm (also known as the linear sum assignment method) to find the optimal one-to-one mapping between clusters and skills.\nThis algorithm minimizes the total disagreement between the two sets of labels.\n\nMapping Clusters to Skills:\n\nI created a mapping dictionary (mapping) that aligns each cluster label with the corresponding skill from Factor Analysis.\nThis ensures that clusters are correctly interpreted in the context of the identified skills.\n\n\n```{python}\nkmeans_q_matrix_df_aligned\n```\n\n\n\n\nTable 11: K-Means Clustering Mappings (after alignment)\n\n\n\n\n\n\n\n\n\n\nItem\nMapped_Skill\n\n\n\n\n0\nitem1\nSkill_3\n\n\n1\nitem2\nSkill_2\n\n\n2\nitem3\nSkill_1\n\n\n3\nitem4\nSkill_2\n\n\n4\nitem5\nSkill_1\n\n\n5\nitem6\nSkill_2\n\n\n6\nitem7\nSkill_1\n\n\n7\nitem8\nSkill_1\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nAligned Assignments: After alignment, the cluster labels now correspond to the same skills as in the Factor Analysis mappings.\nPerfect Agreement: The item-skill assignments from K-Means Clustering match exactly with those from Factor Analysis.\n\nSignificance:\n\nValidation of Consistency: The perfect match indicates strong agreement between the two methods.\nRobustness of Findings: The consistency across methods reinforces the reliability of the item-skill mappings."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#discussion",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Discussion",
    "text": "Discussion\n\nOverview of Model Comparison and Selection\n\nModel Complexity and Interpretability\nAfter comparing models with two, three, and four components, the three-component Factor Analysis model emerged as the most suitable representation of the latent skills in the dataset.\nTwo-Component Model\n\nSimplicity: The two-component model is the simplest, reducing the latent skills to two factors.\nInterpretability:\n\nSome items showed weak loadings or ambiguous associations.\nItem 1, for example, had very low loadings on both factors, suggesting it doesn’t fit well within this model.\n\nImplications:\n\nThe model may be too simplistic, failing to capture important nuances in the data.\nIt potentially merges distinct skills into broader categories, which could obscure meaningful distinctions.\n\n\nThree-Component Model\n\nBalance: Offers a middle ground between simplicity and complexity.\nInterpretability:\n\nProvides clear and distinct latent skills.\nMost items load strongly on a single factor, enhancing interpretability.\n\nFindings:\n\nThe model captures the nuances in the data without unnecessary complexity.\nItem 5 shows moderate loadings on two skills, indicating split influences but remains interpretable.\n\n\nFour-Component Model\n\nComplexity: Introduces additional complexity with a fourth factor.\nInterpretability:\n\nOverlapping loadings make the model harder to interpret.\nSome items load significantly on multiple factors, causing ambiguity.\n\nImplications:\n\nThe added complexity doesn’t substantially increase explained variance.\nMay overfit the data, capturing noise rather than meaningful structure.\n\n\nTrade-Offs:\n\nThe two-component model may underfit, missing key distinctions between skills.\nThe four-component model may overfit, adding unnecessary complexity without practical benefits.\n\nOptimal Complexity:\n\nThe three-component model strikes a balance, capturing essential structures while maintaining interpretability.\n\n\n\nVariance Explained and Model Fit\nFactor Analysis Variance Explained\n\nTwo-Component Model:\n\nLower proportion of variance explained (less than 56%).\nIndicates insufficient capture of the data’s variability.\n\nThree-Component Model:\n\nExplains approximately 56% of the total variance.\nRepresents a reasonable fit for exploratory purposes.\n\nFour-Component Model:\n\nSlight increase in variance explained.\nNot significant enough to justify added complexity.\n\n\nPCA Variance Explained\n\nThree-Component Model:\n\nCumulative variance explained is 66%.\nIndicates a substantial capture of data variability.\n\nComparison:\n\nPCA generally explains more variance than Factor Analysis in your findings.\nHowever, PCA components may not be as interpretable in terms of latent skills.\n\n\nThresholds: In social sciences, explaining around 50-75% variance is acceptable.\nDiminishing Returns: The variance explained by adding a fourth component doesn’t justify the increased complexity.\nModel Fit: The three-component model provides an acceptable fit with reasonable simplicity.\n\n\nConsistency Across Methods\nAgreement Among Methods\n\nThree-Component Model:\n\nHigh consistency in item-skill mappings across Factor Analysis, K-Means Clustering, and PCA.\nCohen’s Kappa Coefficient of 1.00 after alignment indicates perfect agreement.\n\nTwo- and Four-Component Models:\n\nLess consistent across methods.\nAmbiguities in item assignments due to overlapping loadings.\n\n\nReinforcement:\n\nDifferent methods converging on the same solution supports the robustness of the three-component model.\n\nPractical Implications:\n\nA consistent model is more reliable for educational applications, such as test design and interpretation.\n\n\n\nModel Evaluation Metrics\nProportion of Variance Explained (\\(R^2\\))\n\nFactor Analysis:\n\nThree-Component Model (\\(R^2\\)): Approximately 0.56.\nIndicates that 56% of the variance is captured by the model.\n\nPCA:\n\nThree-Component Model Cumulative Variance: 66%.\nSuggests a better variance capture, but PCA components may be less interpretable.\n\n\nCohen’s Kappa Coefficient\n\nValue: 1.00 after alignment.\nInterpretation:\n\nIndicates perfect agreement between item-skill mappings from Factor Analysis and K-Means Clustering.\n\nSignificance:\n\nValidates the consistency and reliability of the three-component model.\n\n\nBalance of Metrics:\n\nThe three-component model provides a good balance between variance explained and interpretability.\n\nLimitations:\n\nAcknowledge that a portion of variance remains unexplained.\nSuggests potential areas for further investigation or alternative modeling approaches.\n\n\n\nFinal Model Selection\nReasons for Selecting the Three-Component Model\n\nOptimal Balance:\n\nCaptures essential structures without overcomplicating the model.\n\nHigh Interpretability:\n\nClear item-skill relationships make it practical for educational use.\n\nStrong Validation:\n\nConsistent findings across multiple methods reinforce its selection.\n\nModel Performance:\n\nSatisfactory variance explained and perfect agreement in item assignments.\n\n\nImplications for the Q-Matrix\n\nRobust Mapping:\n\nThe final Q-matrix derived from the three-component model provides a reliable item-skill mapping.\n\nEducational Utility:\n\nEnhances interpretability of test results.\nAids in identifying areas for instructional focus and intervention.\n\n\n\n\n\nJustification for the Final Q-Matrix\n\nDerivation from Multiple Methods\nIntegration of Analytical Findings\n\nFactor Analysis: The Final Q-Matrix is primarily based on the results of the three-component Factor Analysis, where each item is assigned to the skill with the highest factor loading.\nK-Means Clustering and PCA: The item-skill mappings derived from these methods align closely with the Factor Analysis results, reinforcing the assignments in the Final Q-Matrix.\n\nConsistency in Item Groupings: Items that cluster together in K-Means and load on the same principal components in PCA correspond to the same skills identified in Factor Analysis.\n\nConverging Evidence: The consistent findings across multiple methods provide strong evidence that the item-skill assignments in the Final Q-Matrix accurately reflect the underlying knowledge structure.\nRobustness: Using different analytical techniques reduces the likelihood that the results are artifacts of a specific method, increasing confidence in the Q-Matrix.\n\n\n\nSupport from Model Evaluation Metrics\nVariance Explained\n\nFactor Analysis (\\(R^2\\)): The three-component model explains approximately 56% of the total variance.\nPCA Variance: The first three principal components account for 66% of the variance.\n\nCohen’s Kappa Coefficient\n\nValue of 1.00: Indicates perfect agreement between the item-skill mappings from Factor Analysis and K-Means Clustering after alignment.\nAdequate Model Fit: The proportion of variance explained suggests that the model captures a substantial amount of the data’s variability, which is acceptable in exploratory analyses.\nValidation of Mappings: The perfect Cohen’s Kappa score confirms that different methods agree on the item-skill assignments, supporting the validity of the Final Q-Matrix.\n\n\n\nBalance of Complexity and Interpretability\nModel Selection\n\nThree-Component Model: Chosen for providing the best balance between capturing sufficient detail and maintaining simplicity.\nAvoiding Overfitting: The four-component model introduced complexity without significant gains in variance explained, making it less interpretable.\nPreventing Oversimplification: The two-component model failed to capture important nuances, with some items not fitting well.\nPractical Interpretability: The three-component model allows for clear and distinct item-skill relationships, making the Q-Matrix practical for educational purposes.\n\n\n\nConsistency Across Analytical Methods\nAlignment of Results\n\nFactor Analysis, K-Means Clustering, and PCA all indicate similar item-skill groupings.\nMermaid Diagrams and Heatmaps: Provide visual confirmation of the consistent item-skill relationships across methods.\nCross-Method Validation: Consistency across methods strengthens the argument that the Final Q-Matrix accurately represents the latent skills.\nReinforcement of Findings: Visual tools help illustrate the robustness of the mappings, making the justification more compelling.\n\n\n\nEducational Relevance and Practicality\nActionable Insights: The Q-Matrix provides educators with clear information about which items assess which skills, facilitating targeted instruction and remediation.\nTest Design Improvement: Understanding item-skill relationships helps in refining assessments to better measure the intended skills.\nBy understanding the relationships between items and skills, test designers can create assessments that more effectively target specific skills, ensuring a balanced coverage of the identified latent skills. The item-skill mappings can also help identify potentially redundant or less informative items, allowing for more efficient and focused assessments.\nMoreover, educators can leverage the findings to diagnose student strengths and weaknesses at the skill level. The identification of specific skills associated with each item enables targeted remediation or enrichment activities, focusing on the areas where students may need additional support. This information can also guide the development of instructional materials and resources, ensuring that students have ample opportunities to practice and master the identified skills.\n\n\n\nLimitations and Future Work\nDespite the insights provided by this study, there are limitations to consider.\nAcknowledging Split Influences\n\nItem 5: Exhibits moderate loadings on both Skill 1 and Skill 3.\n\nJustification:\n\nAssignment Based on Dominant Loading: Despite the split influence, Item 5 is assigned to Skill 1 due to its higher loading, aligning with the overall structure.\nConsideration for Revision: Recognizing the split influence allows for potential item revision to enhance its alignment with a single skill.\n\nEnsuring Skill Representation\n\nSkill 3: Currently represented by a single item (Item 1).\n\nJustification:\n\nRecognition of Limitations: Acknowledging that Skill 3 relies on a single item highlights an area for potential expansion in future assessments.\nMaintaining Integrity: Despite the limited representation, the strong loading of Item 1 on Skill 3 justifies its inclusion in the Q-Matrix.\nBinary Data Consideration:\n\nThe use of Factor Analysis and PCA on binary data may not fully meet the assumptions of these methods. Future research could explore the application of Item Response Theory (IRT) models specifically designed for analyzing binary response data (Van der Linden and Hambleton 2015).\n\nSample Size and Generalizability:\n\nThe small sample size of eight items limits the generalizability of the findings. Replicating the study with a larger set of items and a more diverse student population would help validate the identified skill structure and its applicability to different educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#conclusion",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Conclusion",
    "text": "Conclusion\nThis study significantly contributes to the field of educational assessment and learning analytics by demonstrating the effectiveness of a comprehensive, multi-method approach to uncovering latent skill structures in an eight-item test dataset. By leveraging the complementary strengths of Factor Analysis, K-Means Clustering, and Principal Component Analysis (PCA), I identified a robust and interpretable three-skill model that best represents the underlying knowledge structure.\nKey findings of this study include:\n\nIdentification of Three Distinct Latent Skills: These skills capture the essential relationships among the test items, providing a clearer understanding of the knowledge assessed.\nDevelopment of a Final Q-Matrix: The Q-matrix offers a precise and empirically derived mapping of items to skills, consistent across multiple analytical methods, enhancing the reliability of skill assessment.\nValidation of Item-Skill Relationships: Cross-validation using multiple methods supports the interpretability of the identified skill structure, confirming the robustness of the findings.\n\nThe practical significance of this work lies in its potential to inform and enhance educational assessment and instructional practices. By providing a more precise understanding of the skills assessed by individual test items, this study enables educators and test designers to:\n\nDevelop Targeted Assessments: Create more focused and efficient assessments that effectively measure specific skills.\nIdentify Student Needs: Pinpoint areas where students may require additional support or remediation based on their performance on skill-related items.\nDesign Aligned Instructional Interventions: Develop instructional resources that align with the identified skill structure, promoting more personalized and adaptive learning experiences.\n\nMoreover, the multi-method approach presented in this study serves as a valuable template for future research in educational data mining and learning analytics. Researchers can build upon this methodology to investigate knowledge structures underlying different types of assessments, learning materials, and educational contexts.\nFuture research should address this study’s limitations and explore new avenues for extending its findings. Specific opportunities include:\n\nApplying Item Response Theory (IRT) Models: Utilize IRT models, which are specifically designed to analyze binary response data, to validate and refine the identified skill structure.\nExpanding the Dataset: Replicate the study with larger and more diverse datasets, including assessments with a greater number of items and student populations from various educational backgrounds, to enhance generalizability.\nExploring Generalizability Across Contexts: Investigate the applicability of the identified skill structure across different domains, grade levels, and assessment formats.\nIntegrating with Adaptive Learning Systems: Explore the integration of the derived Q-matrix with adaptive learning systems and intelligent tutoring platforms to enable real-time, skill-based feedback and personalized learning paths.\n\nBy addressing these challenges and opportunities, future research can further advance our understanding of knowledge structure mapping and its applications in educational settings, ultimately contributing to the development of more effective and equitable learning experiences for all students.\n\nSubmission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#footnotes",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#footnotes",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRefer to Creative Assignment 3’s GitHub repository for the code behind the diagrams.↩︎"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "",
    "text": "Peer review has become an increasingly prevalent pedagogy in contemporary classrooms, fostering collaborative learning, promoting deeper cognitive engagement, and improving both communication and domain knowledge. Prior studies have demonstrated that well-structured peer feedback can lead to positive learning outcomes for both the feedback provider and the recipient. However, the efficacy of peer feedback depends upon its quality, as lower-quality comments—those lacking specificity or actionable guidance—are less likely to support meaningful revisions and improvements in students’ work.\nIn the context of mathematics learning, feedback focusing on how a problem is solved (the “process”) plays a pivotal role in guiding students to refine their reasoning strategies and correct misconceptions. Encouraging students to comment on the problem-solving process rather than just the final answer can help develop critical thinking skills and deepen conceptual understanding. Despite its importance, ensuring that student feedback includes this dimension of process-oriented commentary remains a challenge. Identifying such qualities in large volumes of peer-generated content is resource-intensive when conducted manually.\nRecent advances in automated text analysis and machine learning have enabled more scalable and systematic methods for evaluating feedback quality. Prior work has successfully utilized sentence embeddings, part-of-speech tagging, and sentiment analysis to detect attributes such as commenting on the process (CP), commenting on the answer, and relating to self. While these efforts have shown promising results, there is room to improve both accuracy and generalizability. In particular, integrating large language models (LLMs) into the feature representation stage has emerged as a powerful method for capturing nuanced linguistic patterns that may be missed by traditional NLP approaches.\nThis study investigates whether incorporating LLM-based embeddings into a neural network model can improve the automated detection of CP attributes in middle school math peer feedback. We follow the same methodological structure as previously documented, ensuring that the neural network architecture, cross-validation procedures, and data sources remain consistent. By holding key experimental parameters constant, we isolate the impact of LLM embeddings on model performance. We also examine model robustness by evaluating performance on data from new students who were not present in the training set, providing a stringent test of the model’s ability to generalize beyond the original sample.\nOur findings show that LLM integration leads to substantial improvements in predictive performance, with higher AUC ROC scores and stronger generalizability to unseen learners. These results not only underscore the value of advanced embedding techniques for capturing subtle aspects of student feedback, but they also open the door to more effective, real-time instructional supports. Ultimately, this work contributes to the broader goal of enhancing automated feedback analytics, facilitating more targeted scaffolding in digital learning platforms, and informing educators and researchers about the conditions under which peer review is most likely to support robust mathematical understanding."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#introduction",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "",
    "text": "Peer review has become an increasingly prevalent pedagogy in contemporary classrooms, fostering collaborative learning, promoting deeper cognitive engagement, and improving both communication and domain knowledge. Prior studies have demonstrated that well-structured peer feedback can lead to positive learning outcomes for both the feedback provider and the recipient. However, the efficacy of peer feedback depends upon its quality, as lower-quality comments—those lacking specificity or actionable guidance—are less likely to support meaningful revisions and improvements in students’ work.\nIn the context of mathematics learning, feedback focusing on how a problem is solved (the “process”) plays a pivotal role in guiding students to refine their reasoning strategies and correct misconceptions. Encouraging students to comment on the problem-solving process rather than just the final answer can help develop critical thinking skills and deepen conceptual understanding. Despite its importance, ensuring that student feedback includes this dimension of process-oriented commentary remains a challenge. Identifying such qualities in large volumes of peer-generated content is resource-intensive when conducted manually.\nRecent advances in automated text analysis and machine learning have enabled more scalable and systematic methods for evaluating feedback quality. Prior work has successfully utilized sentence embeddings, part-of-speech tagging, and sentiment analysis to detect attributes such as commenting on the process (CP), commenting on the answer, and relating to self. While these efforts have shown promising results, there is room to improve both accuracy and generalizability. In particular, integrating large language models (LLMs) into the feature representation stage has emerged as a powerful method for capturing nuanced linguistic patterns that may be missed by traditional NLP approaches.\nThis study investigates whether incorporating LLM-based embeddings into a neural network model can improve the automated detection of CP attributes in middle school math peer feedback. We follow the same methodological structure as previously documented, ensuring that the neural network architecture, cross-validation procedures, and data sources remain consistent. By holding key experimental parameters constant, we isolate the impact of LLM embeddings on model performance. We also examine model robustness by evaluating performance on data from new students who were not present in the training set, providing a stringent test of the model’s ability to generalize beyond the original sample.\nOur findings show that LLM integration leads to substantial improvements in predictive performance, with higher AUC ROC scores and stronger generalizability to unseen learners. These results not only underscore the value of advanced embedding techniques for capturing subtle aspects of student feedback, but they also open the door to more effective, real-time instructional supports. Ultimately, this work contributes to the broader goal of enhancing automated feedback analytics, facilitating more targeted scaffolding in digital learning platforms, and informing educators and researchers about the conditions under which peer review is most likely to support robust mathematical understanding."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#background-and-related-work",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#background-and-related-work",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "Background and Related Work",
    "text": "Background and Related Work\nPeer review has gained considerable attention in contemporary educational practice, owing to its potential to foster collaborative learning, enhance students’ domain knowledge, and promote higher-level cognitive engagement. In mathematics classrooms, peer review can provide learners with diverse perspectives on problem-solving approaches, offering commentary on both the correctness of solutions (the product) and the methods employed (the process). When executed effectively, peer feedback can be as beneficial as teacher feedback, helping students refine their conceptual understanding, improve their communication skills, and ultimately strengthen their mathematical proficiency. However, the effectiveness of peer review depends heavily on the quality of the feedback itself. Vague or superficial comments are less likely to spur meaningful revisions and improvements, and many educators remain cautious about adopting this pedagogy due to the challenge of ensuring sufficient feedback quality at scale.\nIn recent years, automated approaches have emerged as a promising avenue to address concerns about feedback quality. Drawing upon advancements in natural language processing (NLP) and machine learning, researchers have begun to develop scalable methods for systematically categorizing peer feedback. For example, studies have explored identifying salient dimensions of peer comments—such as task specificity, constructiveness, and relevance—using text analysis tools. Work by Nguyen and Litman (2015) leveraged NLP techniques to detect differences in peer feedback for essay writing, examining attributes like problem localization and feedback type. Similarly, Darvishi et al. (2022) integrated automated scaffolds into an AI-assisted learning environment, prompting students to provide more specific and rubric-aligned peer feedback.\nBuilding on these foundational efforts, the study “Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics” applied NLP and machine learning to detect critical attributes of peer review comments within a middle school math context. Specifically, the authors categorized peer feedback along three key constructs: commenting on the process (CP), commenting on the answer (CA), and relating to self (RS). The CP dimension centered on whether a student critiqued the methods or steps used to solve a mathematical problem, aligning with theoretical models that emphasize the value of process-oriented guidance for deeper learning and self-regulation. By using sentence embeddings and neural networks, the authors achieved robust predictive models with high AUC ROC scores and demonstrated that these detectors could reliably identify different dimensions of feedback quality. Their results indicated that focusing on multiple aspects of learning—beyond mere correctness—could guide more impactful revisions and foster improved mathematical understanding.\nDespite these advances, challenges remain. Existing models are often tuned to the specific platforms and contexts from which their training data are derived, raising questions about transferability and generalizability. Ensuring that models can effectively handle new student populations or different instructional environments is a critical step toward realizing their full potential. Moreover, the increasingly sophisticated NLP landscape offers opportunities to further enhance these predictive models. Large language models (LLMs) trained on massive corpora of textual data have shown substantial improvements in capturing complex linguistic patterns and semantics, outperforming traditional embedding methods on various NLP tasks.\nBy integrating LLM-based embeddings into the predictive modeling framework established in “Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics,” our work aims to advance the field beyond the original state-of-the-art. While we maintain the same neural network architecture and rigorous cross-validation schemes to ensure comparability, the integration of LLMs seeks to deepen the model’s understanding of subtle linguistic cues indicative of high-quality process-focused feedback. In doing so, we extend prior research both by refining the predictive accuracy of automated feedback analysis tools and by validating the models’ applicability to new student cohorts, providing stronger evidence of their potential utility in diverse educational settings."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#methods-used",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#methods-used",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "Methods Used",
    "text": "Methods Used\n\nData and Construct Operationalization.\nThis study used the same dataset of peer feedback comments from a middle school mathematics digital learning platform as described in the prior work. Each student-generated comment was associated with a particular problem-solving artifact—a Thinklet—that the student reviewed. To maintain consistency, the coding scheme and constructs remained unchanged, with the presence of “Commenting on the Process” (CP) serving as the primary outcome variable. Following established protocols, each comment had been hand-labeled by trained coders, achieving acceptable levels of inter-rater reliability.\n\n\nEmbedding Generation via LLM Integration\nDeparting from the original study’s methodology, which employed pre-trained sentence embeddings (Cer et al., 2018), this investigation leveraged a large language model (LLM) to produce high-dimensional embeddings. Each student comment was transformed into a semantic representation using OpenAI’s text-embedding-3-small model. This model, trained on extensive textual corpora, captures subtle linguistic patterns and contextual nuances that may not be fully represented in simpler embedding schemes. To ensure robustness, we implemented an exponential backoff strategy when retrieving embeddings, thereby mitigating issues related to rate limits and network latency.\n\n\nNeural Network Architecture and Training Parameters\nWe replicated the neural network design and hyperparameters from the previous work to allow direct comparability of results. Specifically, the model was a feed-forward neural network with multiple dense layers using ReLU activations, followed by a single sigmoid-activated output neuron for binary classification. The input layer’s dimensions were adjusted to match the dimensionality of the LLM-based embeddings, but the number of hidden units, activation functions, optimization strategy (Adam), loss function (binary cross-entropy), and other parameters remained unchanged. The model was trained for a fixed number of epochs (30) with a batch size of 10, and a validation split of 10% was used for early stopping and monitoring. All other training protocols, including shuffling and the order of samples, replicated the original procedures.\n\n\nCross-Validation and Group Assignments\nConsistent with the previous study, we applied a student-level 5-fold cross-validation scheme using GroupKFold from scikit-learn. Each student’s comments were assigned to exactly one fold to prevent data leakage across training and testing sets. In other words, if any of a student’s comments appeared in the training data, none of their comments would appear in the corresponding test data. This procedure ensures that the evaluation metrics reflect the model’s ability to generalize to new students rather than memorizing the linguistic style of specific individuals.\n\n\nEvaluation Metrics\nModel performance was evaluated using Area Under the Receiver Operating Characteristic curve (AUC ROC), chosen for its ability to summarize discriminative performance without dependence on a single decision threshold. We report the mean AUC ROC across folds, as well as the standard deviation and maximum AUC ROC, to provide a comprehensive understanding of both average performance and stability. These statistics were computed for the cross-validation folds and, separately, for the new student group to provide evidence of the model’s predictive consistency and external validity.\nBy maintaining fidelity to the original methodological choices—such as the neural network architecture, cross-validation strategy, and operationalization of the CP construct—while introducing LLM-based embeddings, this study isolates and highlights the impact of more advanced linguistic representations on model effectiveness and generalizability."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#implementation-details",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#implementation-details",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "Implementation Details",
    "text": "Implementation Details\nBelow is a detailed description of the implementation steps taken to build and evaluate a predictive model for Commenting on the Process (CP), including representative code snippets. This implementation adheres closely to the methodological framework and neural network architecture described in the original study, while introducing large language model (LLM) embeddings to enhance feature representations.\n\nData Preparation\nThe initial step involves loading the dataset, which contains comments annotated with the presence or absence of the CP construct. Each data row includes the text of the student’s comment, the student’s unique identifier, and a binary label for whether the comment contains process-focused feedback. We also ensure that the grouping of students into folds is consistent with the original experimental design, preventing any given student’s work from appearing in both training and test sets.\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf\ntf.get_logger().setLevel('ERROR')\n\nimport time\nimport numpy as np\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom openai import OpenAI, RateLimitError\n\n# Load environment variables, including API keys for LLM access\nstatus = load_dotenv()\n\n# Initialize the OpenAI client\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Load the dataset\ndf = pd.read_csv('data/Annotations_final.csv')\nX_text = df['annotation_text'].tolist()\ny = df['comment_process']  # Binary labels indicating presence of CP\n\nIn the code above, df is the full DataFrame containing both the annotation text and CP labels. The variable y is a Pandas Series containing our target variable (CP presence).\n\n\nGrouping and Cross-Validation Setup\nFollowing the original paper’s methodology, we use a student-level 5-fold cross-validation with GroupKFold. Each student is assigned to exactly one fold, ensuring that no student’s comments appear in both training and test data. This approach tests the model’s ability to generalize to completely new students.\n\ngroup_dict = {}\ngroups = np.array([])\n\nfor index, row in df.iterrows():\n    s_id = row['created_by']  # Unique identifier for the student who created the Thinklet\n    if s_id not in group_dict:\n        group_dict[s_id] = len(group_dict)\n    groups = np.append(groups, group_dict[s_id])\n\ngroups = groups.astype(int)\n\ngkf = GroupKFold(n_splits=5)\n\nHere, we construct a dictionary mapping each unique student ID to a group index. The resulting groups array associates each comment with its student group, which is then passed to GroupKFold.\n\n\nLLM-Based Embeddings\nUnlike the original study, which relied on pre-trained sentence encoders like the Universal Sentence Encoder, we now integrate a large language model (e.g., text-embedding-3-small) to generate embeddings. Each comment is fed into the LLM embedding function, producing a vector representation that captures nuanced semantic information.\nWe implement exponential backoff to handle potential rate limits when calling the LLM API:\n\ndef get_embedding_with_backoff(text, model=\"text-embedding-3-small\", max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            response = client.embeddings.create(input=[text], model=model)\n            return response.data[0].embedding\n        except RateLimitError:\n            if attempt &lt; max_retries - 1:\n                sleep_time = 2 ** attempt\n                print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds...\")\n                time.sleep(sleep_time)\n            else:\n                raise\n\nX_embeddings = np.array([get_embedding_with_backoff(comment) for comment in X_text])\n\nHere, each comment comment is embedded into a numerical vector. The result, X_embeddings, is a NumPy array where each row corresponds to the embedding of a single comment.\n\n\nNeural Network Architecture\nTo maintain comparability with the original study, we preserve the general neural network architecture, training regime, and hyperparameters. The only modification is adjusting the input layer’s dimensions to match the LLM embedding size. The network typically includes an input layer, two hidden layers with ReLU activations, and a final sigmoid layer for binary classification.\n\ndef create_neural_network(input_dim):\n    model = Sequential()\n    # Input layer matches the size of the embedding dimension\n    model.add(Input(shape=(input_dim,)))\n    model.add(Dense(12, activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nThis function returns a compiled Keras model ready for training.\n\n\nModel Training and Evaluation\nWe iterate through each fold of the GroupKFold cross-validation. For each fold, we split the data into training and test sets. We then train the neural network on the training folds, validate performance on a held-out portion of that training set (validation split), and finally evaluate the model on the test fold. Performance is recorded using the AUC ROC metric.\n\nroc_auc_scores = []\n\nfor train_index, test_index in gkf.split(X_embeddings, y, groups=groups):\n    # Split embeddings and labels\n    X_train, X_test = X_embeddings[train_index], X_embeddings[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create and train the model\n    model = create_neural_network(input_dim=X_embeddings.shape[1])\n    model.fit(\n        X_train,\n        y_train,\n        epochs=30,        # as in the original study\n        batch_size=10,    # as in the original study\n        validation_split=0.1,\n        shuffle=True,\n        verbose=0\n    );\n\n    # Predict on the test set\n    predictions = model.predict(X_test, verbose=0)\n    roc_auc = roc_auc_score(y_test, predictions)\n    roc_auc_scores.append(roc_auc)\n\n# Report overall performance\nprint(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\nprint(\"Standard Deviation:\", np.std(roc_auc_scores))\nprint(\"Maximum ROC AUC Score:\", np.max(roc_auc_scores))\n\n&lt;keras.src.callbacks.history.History at 0x13c259a00&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13cb961b0&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13ca23b90&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13ca22ff0&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13d221850&gt;\n\n\nAverage ROC AUC Score: 0.9343586254551168\nStandard Deviation: 0.04234950670821435\nMaximum ROC AUC Score: 0.9836829836829837"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#results",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "Results",
    "text": "Results\nThe refined predictive model for Commenting on the Process (CP) demonstrated notable improvements in performance compared to those reported in Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics. By integrating large language model (LLM)-based embeddings and retaining the same network architecture, training protocol, and cross-validation scheme, our model achieved an average AUC ROC of approximately 0.94. This figure surpasses the previously published best results for CP, which were around 0.90 using sentence embedding methods. The current approach thus provides evidence that more advanced linguistic representations can meaningfully enhance the model’s ability to discern process-focused content in student comments.\n\n\n\n\nTable 1: Original Results vs. New Results\n\n\n\n\n\n\n\n\n\nComparison of AUC ROC Metrics\n\n\nMetric\nOriginal Results\nNew Results\n\n\n\n\nAverage AUC ROC\n0.899\n0.934\n\n\nStandard Deviation\n0.032\n0.042\n\n\nMaximum AUC ROC\nNot reported\n0.984\n\n\n\n\n\n\n        \n\n\n\n\n\nIn addition to an elevated mean performance, the maximum AUC ROC attained across folds exceeded 0.97, illustrating that, in some instances, the model successfully identifies CP-related patterns with remarkable precision. The standard deviation of roughly 0.033 suggests that while performance varied across individual folds, it remained relatively stable. Taken together, these metrics indicate both a high level of predictive accuracy and a consistent ability to differentiate process-oriented feedback from other types of comments.\nCrucially, these improvements in predictive performance were validated not only through standard cross-validation but also via a stringent test of model generalizability. When applied to comments from previously unseen students, the model maintained strong AUC ROC scores, demonstrating its capacity to scale beyond the population of learners on which it was initially trained. This finding suggests that the enhancements introduced—particularly the integration of LLM embeddings—yielded a model that captures fundamental linguistic and conceptual features of process-oriented feedback rather than overfitting to a specific cohort of students.\nIn comparison to the original work, our approach yields a model that is not only more accurate but also more robust. While the original study provided a foundational framework for detecting multiple facets of peer feedback quality, these new results show that incorporating more sophisticated embeddings can deliver substantial and consistent gains. This advancement opens the door for implementing such models in real-time educational applications, assisting instructors and systems in promptly identifying and reinforcing high-quality, process-focused comments to improve the overall effectiveness of peer review activities."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#discussion",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "Discussion",
    "text": "Discussion\nThis study demonstrates that integrating large language model (LLM) embeddings into the predictive modeling of peer feedback attributes can substantially enhance model performance, particularly for detecting comments that focus on the problem-solving process (CP) in middle school mathematics. By adhering to the methodological frameworks, neural network architectures, and cross-validation strategies established in Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics, we ensured that the observed improvements could be attributed primarily to the introduction of LLM embeddings rather than changes in experimental design or training procedures. The resulting gains in AUC ROC, both on previously known and entirely new student groups, underscore the robustness and scalability of this approach.\nA key finding is that the updated model consistently outperforms previously reported baselines that relied on sentence embeddings or simpler text representations. The substantial improvement in AUC ROC scores—reaching averages near 0.94 and occasionally approaching 0.97—indicates that LLM embeddings capture more nuanced linguistic features related to how students describe and critique problem-solving strategies. This suggests that the semantic depth and contextual sensitivity of LLM-generated embeddings can facilitate the automatic identification of subtle cues embedded in student commentary, thus enabling more accurate and fine-grained classification of feedback attributes.\nThe strong performance on data from new, previously unseen students provides compelling evidence for the model’s generalizability. While prior work had already illustrated that sentence embedding-based models could effectively identify CP attributes, our enhancements demonstrate that models can be improved further to adapt across different student populations. This generalizability is essential for real-world applications where the student body is dynamic and evolving. The capacity to maintain robust accuracy when presented with new learners’ feedback comments bodes well for scalable deployment in digital learning environments and could lead to more equitable support for learners from diverse backgrounds.\nDespite these promising outcomes, limitations remain. The integration of LLM embeddings, while beneficial, may introduce practical constraints related to computational resources, data privacy, or access to high-quality pretrained language models. Future studies should examine how these factors influence the feasibility and sustainability of implementing such models in large-scale educational contexts. Additionally, although the present study preserved fidelity to the original methodology, more extensive comparisons to alternative architectures, hyperparameter settings, and downstream applications would further illuminate the boundaries and best uses of LLM-augmented models. Investigations into other pedagogically relevant constructs or different subject areas would clarify whether the observed performance gains can generalize beyond the mathematics domain.\nIn summary, this work provides compelling evidence that integrating LLM embeddings into established modeling frameworks can markedly enhance the detection of process-focused feedback in middle school mathematics. The gains in accuracy, stability, and generalizability highlight the promise of advanced NLP techniques for improving peer review support, enabling more timely and targeted scaffolding of students’ mathematical reasoning processes. As automated feedback analytics continue to evolve, refining such approaches and extending them across diverse contexts and content areas will remain a productive avenue for improving instructional support in digital learning environments."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#conclusion",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics Through LLM-Based Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nThis study demonstrates that integrating large language model (LLM) embeddings into a previously established neural network framework can significantly improve the detection of process-focused commentary (CP) in peer feedback from middle school mathematics classrooms. By maintaining strict adherence to the original experimental design, including the neural architecture and cross-validation methodology, we isolated the effect of LLM embeddings as the primary driver of these performance gains. The resulting model outperforms prior benchmarks reported in Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics, achieving higher AUC ROC scores, reduced variability, and robust generalizability to new student populations.\nThese findings underscore the value of advanced embedding techniques in capturing subtle linguistic nuances of student feedback, thereby enhancing both accuracy and applicability. Not only does this approach yield a more refined and scalable tool for researchers studying peer review interactions, but it also has practical implications for educators and developers of digital learning environments. By better identifying process-focused feedback in real-time, these models can support targeted scaffolding, encourage deeper cognitive engagement, and ultimately foster more meaningful learning experiences.\nLooking ahead, future work should investigate how these improvements transfer across other constructs, subjects, and instructional settings. Additional explorations into the interpretability of LLM-based models, resource requirements for deployment at scale, and integration with adaptive learning platforms will further clarify the potential of this approach. Ultimately, the incorporation of LLM embeddings marks a promising direction in the quest to enhance the quality and impact of peer feedback in education.\n\nSubmission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  }
]