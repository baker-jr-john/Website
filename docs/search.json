[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Penn GSE",
    "section": "",
    "text": "MIT License\nCopyright © 2025 John Richard Baker Jr.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”) to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright and permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "",
    "text": "Peer review is a powerful tool in mathematics education that encourages students to engage critically with problem-solving strategies, not just final answers. Substantial research has shown that high-quality, process-oriented feedback can deepen conceptual understanding and enhance mathematical reasoning skills (Uesato et al. 2022; Nicol and Macfarlane-Dick 2006). However, facilitating such feedback effectively at a large scale presents significant challenges. Manually evaluating peer comments for quality is time-intensive, while automated approaches have often struggled to identify substantive, process-focused characteristics reliably.\nRecent natural language processing (NLP) advancements offer promising avenues to address these issues. Prior work has leveraged machine learning techniques, such as sentence embeddings and neural networks, to classify peer feedback along dimensions including correctness, specificity, and process orientation. These efforts have achieved noteworthy improvements in accuracy and efficiency compared to manual methods (Zhang et al. 2023). However, there remains significant room for further refinement, particularly in capturing the nuanced linguistic patterns associated with high-quality, process-level commentary.\nThe rapid development of large language models (LLMs) presents an exciting opportunity in this regard. LLMs, which are trained on massive and diverse text corpora, have demonstrated remarkable capabilities in representing complex semantic relationships and generating contextually relevant embeddings (Radford et al. 2019). By encoding text in a high-dimensional space, LLM embeddings can potentially capture subtle indicators of effective feedback that previous methods may overlook. Integrating such advanced language representations into existing predictive frameworks thus offers a promising path to enhance the precision and robustness of automated peer review analysis.\nThis study aims to investigate the impact of incorporating LLM embeddings into a proven classification model for detecting process-focused feedback. Building upon the methodology established in Zhang et al. (2023), I preserve the core neural network architecture, cross-validation scheme and construct operationalization to isolate the effects of the embedding approach. By systematically comparing the performance of LLM-based embeddings against prior sentence-level encodings, this work seeks to quantify the benefits of more sophisticated language modeling in the context of educational peer feedback.\nFurthermore, I evaluate the model’s ability to generalize to completely unseen student populations. Demonstrating strong transferability is crucial for practical applications, as it suggests the model is learning meaningful linguistic patterns rather than overfitting to specific student characteristics. Improved generalization would support the development of broadly applicable tools that could provide real-time, adaptive feedback to enhance students’ learning experiences across diverse contexts.\nUltimately, this research advances the state-of-the-art in automated analysis of peer review, laying the groundwork for scalable, data-driven support systems in mathematics education. By harnessing the power of LLMs to identify effective process-oriented feedback, this work informs the design of educational technologies that can offer targeted, timely interventions to foster deeper mathematical understanding. More broadly, it contributes to ongoing efforts in leveraging artificial intelligence (AI) to enhance formative assessment and personalize learning at scale."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#introduction",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "",
    "text": "Peer review is a powerful tool in mathematics education that encourages students to engage critically with problem-solving strategies, not just final answers. Substantial research has shown that high-quality, process-oriented feedback can deepen conceptual understanding and enhance mathematical reasoning skills (Uesato et al. 2022; Nicol and Macfarlane-Dick 2006). However, facilitating such feedback effectively at a large scale presents significant challenges. Manually evaluating peer comments for quality is time-intensive, while automated approaches have often struggled to identify substantive, process-focused characteristics reliably.\nRecent natural language processing (NLP) advancements offer promising avenues to address these issues. Prior work has leveraged machine learning techniques, such as sentence embeddings and neural networks, to classify peer feedback along dimensions including correctness, specificity, and process orientation. These efforts have achieved noteworthy improvements in accuracy and efficiency compared to manual methods (Zhang et al. 2023). However, there remains significant room for further refinement, particularly in capturing the nuanced linguistic patterns associated with high-quality, process-level commentary.\nThe rapid development of large language models (LLMs) presents an exciting opportunity in this regard. LLMs, which are trained on massive and diverse text corpora, have demonstrated remarkable capabilities in representing complex semantic relationships and generating contextually relevant embeddings (Radford et al. 2019). By encoding text in a high-dimensional space, LLM embeddings can potentially capture subtle indicators of effective feedback that previous methods may overlook. Integrating such advanced language representations into existing predictive frameworks thus offers a promising path to enhance the precision and robustness of automated peer review analysis.\nThis study aims to investigate the impact of incorporating LLM embeddings into a proven classification model for detecting process-focused feedback. Building upon the methodology established in Zhang et al. (2023), I preserve the core neural network architecture, cross-validation scheme and construct operationalization to isolate the effects of the embedding approach. By systematically comparing the performance of LLM-based embeddings against prior sentence-level encodings, this work seeks to quantify the benefits of more sophisticated language modeling in the context of educational peer feedback.\nFurthermore, I evaluate the model’s ability to generalize to completely unseen student populations. Demonstrating strong transferability is crucial for practical applications, as it suggests the model is learning meaningful linguistic patterns rather than overfitting to specific student characteristics. Improved generalization would support the development of broadly applicable tools that could provide real-time, adaptive feedback to enhance students’ learning experiences across diverse contexts.\nUltimately, this research advances the state-of-the-art in automated analysis of peer review, laying the groundwork for scalable, data-driven support systems in mathematics education. By harnessing the power of LLMs to identify effective process-oriented feedback, this work informs the design of educational technologies that can offer targeted, timely interventions to foster deeper mathematical understanding. More broadly, it contributes to ongoing efforts in leveraging artificial intelligence (AI) to enhance formative assessment and personalize learning at scale."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#background-and-related-work",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#background-and-related-work",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "Background and Related Work",
    "text": "Background and Related Work\nPeer review in mathematics education fosters collaboration and develops analytical thinking. Past research has underscored the importance of feedback quality: comments highlighting the reasoning behind a solution can help students identify misconceptions, refine strategies, and internalize mathematical concepts more deeply (Kapur 2010). However, teachers often have limited time to vet large volumes of student-generated comments, raising concerns about implementing peer review at scale.\nAutomated approaches have begun to address these challenges. Prior studies leveraged NLP techniques—such as part-of-speech tagging, sentiment analysis, and sentence-level embeddings—to classify peer feedback along dimensions including process focus, correctness, and personalization (Zhang et al. 2023). While these methods improved efficiency and consistency, there is room for refinement. With their ability to represent textual data more contextually and semantically, the rise of LLMs suggests an opportunity to further improve the predictive accuracy and transferability of feedback classification models.\nThis study builds on earlier work by incorporating LLM embeddings to advance state-of-the-art automated feedback analysis. These enhanced embeddings may better capture linguistic subtleties, improving model performance not only on known students but also on entirely new student populations, thereby supporting more scalable, robust, and contextually informed educational tools."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#methods",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#methods",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "Methods",
    "text": "Methods\nThis study extends an established predictive framework for detecting peer comments on the process (CP) in middle school mathematics. The key innovation is the integration of large language model (LLM) embeddings to capture nuanced linguistic patterns associated with CP.\n\nOverview of Approach\nThe methodology follows these main steps:\n\nData Preparation: Load a previously annotated dataset of peer comments, preserving the original structure and CP labels.\nEmbedding Generation: Feed each comment through an LLM to obtain high-dimensional vector representations that encode semantic relationships.\nModel Architecture: Employ the same neural network design as prior work, adjusted only to accommodate the dimensionality of LLM embeddings.\nCross-Validation: Implement a student-level cross-validation scheme, ensuring that no student’s data appears in both training and test sets for any given fold.\nModel Training & Evaluation: Train the model on the training set, validate on a held-out portion, and evaluate on the corresponding test set using the Area Under the Receiver Operating Characteristic curve (AUC ROC) as the primary performance metric.\n\nThe following subsections provide more detailed information on dataset characteristics, embedding techniques, model specification, and evaluation procedures.\n\n\nData and Annotation\nThe dataset consists of peer comments on mathematics problems submitted by middle school students through an online learning platform. Each comment was manually annotated for CP’s presence (1) or absence (0). CP is operationalized as feedback that addresses the problem-solving process, such as discussing strategies, identifying misconceptions, or suggesting alternative approaches rather than solely evaluating the final answer.\n\n\nLLM Embeddings\nUnlike previous studies that utilized sentence-level encodings such as the Universal Sentence Encoder, this work leverages OpenAI’s text-embedding-3-small model to generate comment embeddings. LLMs can learn more contextually rich representations by training on massive, diverse text corpora. The text-embedding-3-small model produces a high-dimensional vector for each comment, capturing latent semantic features that may be indicative of CP. I implemented an exponential backoff strategy to handle potential rate limits during embedding generation.\n\n\nModel Specification\nThe predictive model architecture remains consistent with prior work identifying the impact of LLM embeddings. The core structure is a feedforward neural network with an input layer (adjusted to match LLM embedding dimensions), two hidden layers with ReLU activation, and a sigmoid output layer for binary classification. The model is trained using binary cross-entropy loss and the Adam optimizer, with hyperparameters following the original study.\n\n\nEvaluation\nModel performance is assessed using AUC ROC, a threshold-agnostic metric that captures the trade-off between true and false positive rates. A five-fold student-level cross-validation scheme is employed, so any given student’s comments are restricted to either the training or testing set within each fold. This grouping strategy, consistent with the original methodology, allows evaluation of the model’s generalization to new students, not just new comments. Comparing AUC ROC scores against prior baselines quantifies the impact of integrating LLM embeddings."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#implementation-details",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#implementation-details",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "Implementation Details",
    "text": "Implementation Details\nBelow is a detailed description of the implementation steps taken to build and evaluate a predictive model for Commenting on the Process (CP), including representative code snippets. This implementation adheres closely to the methodological framework and neural network architecture described in the original study while introducing large language model (LLM) embeddings to enhance feature representations.\n\nData Preparation\nThe initial step involves loading the dataset, which contains comments annotated with the presence or absence of the CP construct. Each data row includes the text of the student’s comment, the student’s unique identifier, and a binary label for whether the comment contains process-focused feedback. I also ensure that grouping students into folds is consistent with the original experimental design, preventing any student’s work from appearing in both training and test sets.\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf\ntf.get_logger().setLevel('ERROR')\n\nimport time\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, RateLimitError\nimport pandas as pd\n\n# Load environment variables, including API keys for LLM access\nstatus = load_dotenv()\n\n# Initialize the OpenAI client\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Load the dataset\ndf = pd.read_csv('data/Annotations_final.csv')\nX_text = df['annotation_text'].tolist()\ny = df['comment_process']  # Binary labels indicating presence of CP\n\nIn the code above, df is the complete data frame containing both the annotation text and CP labels. The variable y is a Pandas Series containing the target variable (CP presence).\n\n\nGrouping and Cross-Validation Setup\nFollowing the original paper’s methodology, I used a student-level five-fold cross-validation with GroupKFold. Each student is assigned to precisely one fold, ensuring that no student’s comments appear in both training and test data. This approach tests the model’s ability to generalize to completely new students.\n\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\n\ngroup_dict = {}\ngroups = np.array([])\n\nfor index, row in df.iterrows():\n    s_id = row['created_by']  # Unique identifier for the student who created the Thinklet\n    if s_id not in group_dict:\n        group_dict[s_id] = len(group_dict)\n    groups = np.append(groups, group_dict[s_id])\n\ngroups = groups.astype(int)\n\ngkf = GroupKFold(n_splits=5)\n\nHere, I constructed a dictionary mapping each unique student ID to a group index. The resulting groups array associates each comment with its student group, which is then passed to GroupKFold.\n\n\nLLM-Based Embeddings\nUnlike the original study, which relied on pre-trained sentence encoders like the Universal Sentence Encoder, I integrated a large language model (i.e., text-embedding-3-small) to generate embeddings. Each comment is fed into the LLM embedding function, producing a vector representation that captures nuanced semantic information.\nI implemented exponential backoff to handle potential rate limits when calling the LLM API:\n\ndef get_embedding_with_backoff(text, model=\"text-embedding-3-small\", max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            response = client.embeddings.create(input=[text], model=model)\n            return response.data[0].embedding\n        except RateLimitError:\n            if attempt &lt; max_retries - 1:\n                sleep_time = 2 ** attempt\n                print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds ... \")\n                time.sleep(sleep_time)\n            else:\n                raise\n\nX_embeddings = np.array([get_embedding_with_backoff(comment) for comment in X_text])\n\nHere, each comment comment is embedded into a numerical vector. The result, X_embeddings, is a NumPy array where each row corresponds to the embedding of a single comment.\n\n\nNeural Network Architecture\nI preserved the general neural network architecture, training regime, and hyperparameters to maintain comparability with the original study. The only modification is adjusting the input layer’s dimensions to match the LLM embedding size. The network typically includes an input layer, two hidden layers with ReLU activations, and a final sigmoid layer for binary classification.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\n\ndef create_neural_network(input_dim):\n    model = Sequential()\n    # Input layer matches the size of the embedding dimension\n    model.add(Input(shape=(input_dim,)))\n    model.add(Dense(12, activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nThis function returns a compiled Keras model that is ready for training.\n\n\nModel Training and Evaluation\nI iterated through each fold of the GroupKFold cross-validation. For each fold, I split the data into training and test sets. Then, I trained the neural network on the training folds, validated performance on a held-out portion of that training set (validation split), and finally evaluated the model on the test fold. Performance was recorded using the AUC ROC metric.\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_scores = []\n\nfor train_index, test_index in gkf.split(X_embeddings, y, groups=groups):\n    # Split embeddings and labels\n    X_train, X_test = X_embeddings[train_index], X_embeddings[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create and train the model\n    model = create_neural_network(input_dim=X_embeddings.shape[1])\n    model.fit(\n        X_train,\n        y_train,\n        epochs=30,        # as in the original study\n        batch_size=10,    # as in the original study\n        validation_split=0.1,\n        shuffle=True,\n        verbose=0\n    );\n\n    # Predict on the test set\n    predictions = model.predict(X_test, verbose=0)\n    roc_auc = roc_auc_score(y_test, predictions)\n    roc_auc_scores.append(roc_auc)\n\n# Report overall performance\nprint(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\nprint(\"Standard Deviation:\", np.std(roc_auc_scores))\nprint(\"Maximum ROC AUC Score:\", np.max(roc_auc_scores))\n\n&lt;keras.src.callbacks.history.History at 0x13aac7b60&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13abeaed0&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13adcbf20&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13ad69c40&gt;\n\n\n&lt;keras.src.callbacks.history.History at 0x13b0cd970&gt;\n\n\nAverage ROC AUC Score: 0.9352577417708996\nStandard Deviation: 0.0411599012839265\nMaximum ROC AUC Score: 0.9813519813519813"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#results",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "Results",
    "text": "Results\n\n\n\n\nTable 1: Original Results vs. New Results\n\n\n\n\n\n\n\n\n\nComparison of AUC ROC Metrics\n\n\nMetric\nOriginal Results\nNew Results\n\n\n\n\nAverage AUC ROC\n0.899\n0.935\n\n\nStandard Deviation\n0.032\n0.041\n\n\nMaximum AUC ROC\nNot reported\n0.981\n\n\n\n\n\n\n        \n\n\n\n\n\nIntegrating LLM embeddings led to notable improvements. Average AUC ROC increased from approximately 0.899 in earlier work to about 0.935 with LLM embeddings, with some folds reaching 0.981, indicating that advanced embeddings more accurately distinguish process-focused feedback from other comment types. Moreover, performance stabilized across folds, suggesting improved robustness and reduced variance.\nImportantly, the model generalized well to new student data. This finding implies that the embedding-based model is not simply memorizing student idiosyncrasies but learning transferable linguistic features associated with CP. The resulting model could support large-scale implementations, identifying high-quality, process-oriented feedback in real-time."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#discussion",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "Discussion",
    "text": "Discussion\nThe results highlight the potential of LLM-based embeddings to enhance automated feedback analytics in educational settings. By capturing subtle semantic patterns, these embeddings enable more accurate identification of CP attributes and facilitate timely, targeted interventions. Teachers can use these insights to recognize when students engage in meaningful, process-level thinking, while platform developers can design adaptive features that prompt deeper reflection. Policymakers and curriculum specialists might leverage these tools to inform professional development and improve peer review guidelines. Still, several limitations warrant further exploration.\n\nLimitations\nReliance on proprietary LLMs may raise cost, access, and interpretability issues. Additionally, while my results show strong performance within a middle school mathematics context, it remains unclear how well these methods transfer to other subjects, age groups, or types of feedback. Future work should explore these dimensions, assess the interpretability of LLM embeddings in educational contexts, and test different architectures or training regimes to boost performance and generalizability further."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_4/index.html#conclusion",
    "title": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nThis study demonstrates the significant potential of integrating large language model (LLM) embeddings into automated peer feedback analysis in mathematics education. By enhancing an established predictive framework with LLM-based representations, I substantially improved accuracy and generalizability for detecting process-focused commentary (CP).\nThe results highlight the power of advanced language models to capture nuanced linguistic patterns indicative of effective feedback. LLM embeddings outperformed prior sentence encodings in correctly identifying CP, suggesting their ability to learn more contextually rich features from limited data. Importantly, these gains were not merely a result of overfitting to specific student characteristics; the model’s strong performance on completely unseen students underscores its potential for broad, reliable application in real-world settings.\nThese findings offer promising avenues for enhancing formative assessment and personalized learning at scale. By enabling more precise, automated identification of high-quality feedback, this work lays the foundation for educational technologies that can offer immediate, targeted support to foster students’ mathematical reasoning skills. Such tools could help educators efficiently recognize and reinforce effective peer review practices, tailor instruction to individual needs, and promote richer classroom discussions around problem-solving processes.\nMore broadly, this study contributes to the growing body of research on AI-augmented education. It demonstrates the value of leveraging state-of-the-art NLP techniques, particularly LLMs, to tackle complex challenges in learning analytics. The approach presented here could potentially be extended to other domains and feedback dimensions, opening up new possibilities for data-driven support across diverse educational contexts.\nHowever, it is important to acknowledge this work’s limitations and potential future directions. The reliance on a proprietary LLM may raise questions of cost, transparency, and reproducibility. Additionally, while the results are promising within the scope of middle school mathematics, further research is needed to validate the transferability of these methods to other subject areas, age groups, and feedback types. Investigating the interpretability of LLM embeddings in educational settings and exploring alternative model architectures are also important areas for future study.\nNonetheless, this research represents a significant step forward in the development of scalable, AI-powered tools to support effective peer learning. By harnessing the power of language models to identify and amplify high-quality feedback, we can create more responsive, adaptive educational environments that foster deeper engagement and understanding. Ultimately, this work contributes to the broader vision of leveraging AI to enhance education equity and outcomes, empowering all students to reach their full potential as mathematical thinkers and problem-solvers.\n\nSubmission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "",
    "text": "Understanding student behavior within educational software environments is crucial for providing timely interventions and enhancing learning outcomes. Off-task behavior, in particular, can negatively impact learning efficacy. Accurate detection of such behavior allows educators to address issues promptly and tailor educational experiences to individual student needs.\nThis project builds upon previous work by engineering new features derived from detailed logs of student interactions. By integrating these features with existing ones and applying advanced machine learning techniques, I aim to develop an improved behavior detector that can more accurately identify off-task behaviors."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#introduction",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "",
    "text": "Understanding student behavior within educational software environments is crucial for providing timely interventions and enhancing learning outcomes. Off-task behavior, in particular, can negatively impact learning efficacy. Accurate detection of such behavior allows educators to address issues promptly and tailor educational experiences to individual student needs.\nThis project builds upon previous work by engineering new features derived from detailed logs of student interactions. By integrating these features with existing ones and applying advanced machine learning techniques, I aim to develop an improved behavior detector that can more accurately identify off-task behaviors."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#literature-review",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#literature-review",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "Literature Review",
    "text": "Literature Review\n\nDetecting Off-Task Behavior and Addressing Algorithmic Bias in Learning Systems\nEducational Data Mining (EDM) has emerged as a significant field, leveraging student data to enhance learning outcomes. Recent research has focused on developing algorithms and metrics to address algorithmic bias in education and other related fields (Cohausz, Kappenberger, and Stuckenschmidt 2024). Analyzing student data can provide valuable insights into factors influencing academic performance, including social connections (Siemens and Baker 2012).\nA particularly relevant area within EDM for this study is detecting student misuse of educational systems. Baker and Siemens (Siemens and Baker 2012) explored how data mining techniques can identify instances where students “game the system” in constraint-based tutors. This concept is pertinent to identifying off-task behavior, a broader category of student misuse.\nOff-task behavior encompasses actions where students deviate from their intended engagement with educational software, including disengagement, inappropriate tool use, or attempts to circumvent learning activities. “Gaming the system” (Ryan SJD Baker, Yacef, et al. 2009) can be understood as a specific manifestation of off-task behavior in which students exploit system mechanics to achieve desired outcomes without genuine engagement.\nOther relevant methodologies and ethical considerations include:\n\nThe use of “text replays” to gain a deeper understanding of student behavior (Sao Pedro, Baker, and Gobert 2012; Slater, Baker, and Wang 2020), which could potentially be adapted for analyzing off-task behavior patterns.\nAddressing fairness and bias in machine learning models used in educational contexts (Cohausz, Kappenberger, and Stuckenschmidt 2024; Ryan S. Baker and Hawn 2022), ensuring that models for detecting off-task behavior are equitable and do not unfairly disadvantage certain student groups."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#methodology",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#methodology",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "Methodology",
    "text": "Methodology\n\nData Preparation\nI began by importing essential libraries for data manipulation and machine learning, loading the datasets (ca1 and ca2) from CSV files.\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, cohen_kappa_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\n\n# Load datasets\nca1 = pd.read_csv('data/ca1-dataset.csv')\nca2 = pd.read_csv('data/ca2-dataset.csv')\n\n\nca1-dataset.csv: Contains existing features related to student interactions.\nca2-dataset.csv: Provides detailed logs of student actions, from which new features are engineered.\n\nBoth datasets were imported into Pandas dataframes for manipulation and analysis.\n\nDataset Overview\nca1-dataset.csv:\n\nEntries: 763 rows\nColumns: 27\nData Types: Numeric and categorical\nMissing Values: None\n\nca2-dataset.csv:\n\nEntries: 1,745 rows\nColumns: 34\nData Types: Numeric and categorical\nMissing Values: None\n\nKey insights:\n\nThe ca1-dataset.csv contains aggregated data for 20-second intervals, while ca2-dataset.csv provides more granular information about individual student actions.\nBoth datasets share a common Unique-id field, allowing for integration of the new features.\n\nPreprocessing steps:\n\nConverted categorical variables to numerical using one-hot encoding.\nNormalized numerical features to ensure consistent scale across all variables.\n\n\n\n\nFeature Engineering\nFrom ca2-dataset.csv, I engineered several user-specific features to capture behavioral patterns:\n\nAction Frequency: Total number of actions per user within the 20-second interval.\n\nCalculation: Count of actions for each Unique-id.\nRationale: Higher frequency may indicate engagement or potentially off-task rapid clicking.\n\nAverage Time Between Actions: Mean time interval between consecutive actions.\n\nCalculation: Mean of time differences between consecutive actions for each Unique-id.\nRationale: Longer intervals might suggest disengagement or thoughtful consideration.\n\nMaximum Action Duration: Longest time interval between actions.\n\nCalculation: Maximum time difference between consecutive actions for each Unique-id.\nRationale: Extremely long durations could indicate off-task behavior or system issues.\n\nAction Diversity: Number of unique actions performed.\n\nCalculation: Count of distinct action types for each Unique-id.\nRationale: Higher diversity might indicate more engaged, on-task behavior.\n\nIdle Time Ratio: Proportion of time spent idle (no actions recorded).\n\nCalculation: Sum of time intervals exceeding 5 seconds divided by total interval time.\nRationale: Higher idle time may suggest off-task behavior or disengagement.\n\n\n\n# Action Frequency, Avg Time Between Actions, Max Action Duration, Action Diversity, Idle/Active Ratio\naction_freq = ca2.groupby('Unique-id')['Row'].count().reset_index()\naction_freq.columns = ['Unique-id', 'Action_Frequency']\n\nca2['time'] = pd.to_datetime(ca2['time'], errors='coerce')\nca2 = ca2.sort_values(by=['Unique-id', 'time'])\nca2['Time_Diff'] = ca2.groupby('Unique-id')['time'].diff().dt.total_seconds()\navg_time_diff = ca2.groupby('Unique-id')['Time_Diff'].mean().reset_index()\navg_time_diff.columns = ['Unique-id', 'Avg_Time_Between_Actions']\n\nmax_time_diff = ca2.groupby('Unique-id')['Time_Diff'].max().reset_index()\nmax_time_diff.columns = ['Unique-id', 'Max_Action_Duration']\n\naction_diversity = ca2.groupby('Unique-id')['prod'].nunique().reset_index()\naction_diversity.columns = ['Unique-id', 'Action_Diversity']\n\nca2['Idle_Time'] = ca2['Time_Diff'].apply(lambda x: x if x &gt; 60 else 0)\ntotal_idle_time = ca2.groupby('Unique-id')['Idle_Time'].sum().reset_index()\ntotal_active_time = ca2.groupby('Unique-id')['Time_Diff'].sum().reset_index()\ntotal_active_time.columns = ['Unique-id', 'Total_Active_Time']\nidle_active_ratio = total_idle_time.merge(total_active_time, on='Unique-id')\nidle_active_ratio['Idle_Active_Ratio'] = idle_active_ratio['Idle_Time'] / idle_active_ratio['Total_Active_Time']\n\nThese features aim to quantify user engagement and detect patterns indicative of off-task behavior.\n\n\nData Merging and Cleaning\nThe new features were merged with ca1-dataset.csv based on the Unique-id key. Missing values in numerical columns were handled using mean imputation to ensure the integrity of the dataset for modeling. Categorical variables were encoded using one-hot encoding to prepare them for machine learning algorithms.\n\n# Merging the new features into the original ca1-dataset.csv\nca1_enhanced = ca1.merge(action_freq, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(avg_time_diff, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(max_time_diff, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(action_diversity, on='Unique-id', how='left')\nca1_enhanced = ca1_enhanced.merge(idle_active_ratio[['Unique-id', 'Idle_Active_Ratio']], on='Unique-id', how='left')\n\n# Handling missing values using mean imputation\nnumeric_cols = ca1_enhanced.select_dtypes(include=['number']).columns\nca1_enhanced[numeric_cols] = ca1_enhanced[numeric_cols].fillna(ca1_enhanced[numeric_cols].mean())\n\n\n\nModel Development\nI developed two primary models to compare the effectiveness of the newly engineered features:\n\nModel 1: Original Features\nA Random Forest Classifier was trained using only the original features from ca1-dataset.csv. This serves as a baseline model to evaluate the impact of the new features. The target variable was the OffTask indicator, converted to a binary format.\n\n# Model Development using RandomForestClassifier\noriginal_features = ['Avgright', 'Avgbug', 'Avghelp', 'Avgchoice', 'Avgstring', 'Avgnumber', 'Avgpoint', 'Avgpchange', 'Avgtime', 'AvgtimeSDnormed', 'Avgtimelast3SDnormed', 'Avgtimelast5SDnormed', 'Avgnotright', 'Avghowmanywrong-up', 'Avghelppct-up', 'Avgwrongpct-up', 'Avgtimeperact-up', 'AvgPrev3Count-up', 'AvgPrev5Count-up', 'Avgrecent8help', 'Avg recent5wrong', 'Avgmanywrong-up', 'AvgasymptoteA-up', 'AvgasymptoteB-up']\n\n# Separate features and target variable ('OffTask')\nX_original = ca1_enhanced[original_features]\ny = ca1_enhanced['OffTask'].apply(lambda x: 1 if x == 'Y' else 0)\n\n# Split the dataset into train and test sets\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original, y, test_size=0.2, random_state=42)\n\n# Build the RandomForest model\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_orig, y_train_orig)\n\n# Predict and evaluate the model\ny_pred_orig = rf.predict(X_test_orig)\nauc_orig = roc_auc_score(y_test_orig, y_pred_orig)\nkappa_orig = cohen_kappa_score(y_test_orig, y_pred_orig)\nprint(f\"AUC: {auc_orig}, Kappa: {kappa_orig}\")\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=42) \n\n\nAUC: 0.5833333333333334, Kappa: 0.2776203966005666\n\n\n\n\nModel 2: Combined Features\nThe second model incorporated both original and new features. I performed hyperparameter tuning using GridSearchCV to optimize the Random Forest Classifier.\n\n# Combined Features\nnew_features = ['Action_Frequency', 'Avg_Time_Between_Actions', 'Max_Action_Duration', 'Action_Diversity', 'Idle_Active_Ratio']\nX_combined = ca1_enhanced[original_features + new_features]\nX_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}\n\n# Initialize the RandomForestClassifier\nrf = RandomForestClassifier(random_state=42)\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='roc_auc')\n\n# Fit GridSearchCV\ngrid_search.fit(X_train_comb, y_train_comb)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n\nprint(f'Best parameters found: {best_params}')\n\nBest parameters found: {'bootstrap': False, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n\n\nBased on the grid search, the best model configuration is a RandomForestClassifier with:\n\nNo bootstrapping (bootstrap=False)\nMaximum depth of 10 (max_depth=10)\nMinimum of 2 samples per leaf node (min_samples_leaf=2)\nMinimum of 2 samples required to split an internal node (min_samples_split=2)\n100 decision trees (n_estimators=100)\n\n\n# Train the model with the best parameters\nbest_rf = RandomForestClassifier(**best_params, random_state=42)\nbest_rf.fit(X_train_comb, y_train_comb)\ny_pred_comb = best_rf.predict(X_test_comb)\n\n# Evaluate the model\nauc_comb = roc_auc_score(y_test_comb, y_pred_comb)\nkappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)\nprint(f'AUC for Combined Features with Best Params: {auc_comb}')\nprint(f'Kappa for Combined Features with Best Params: {kappa_comb}')\n\nRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) \n\n\nAUC for Combined Features with Best Params: 0.75\nKappa for Combined Features with Best Params: 0.6577181208053692\n\n\n\n\n\nAddressing Class Imbalance with SMOTE and Ensemble Modeling\nTo address potential class imbalance in the dataset, Synthetic Minority Over-sampling Technique (SMOTE) was applied only to the training set during each fold of cross-validation. This approach ensures that the test set remains unaltered and representative of the true data distribution.\nAn ensemble model comprising a Random Forest, Logistic Regression, and Support Vector Classifier was built using a soft voting strategy. This ensemble approach aims to leverage the strengths of different algorithms and improve overall prediction accuracy.\n\n# Separate features and target variable\nX = ca1_enhanced[original_features + new_features]\ny = ca1_enhanced['OffTask'].apply(lambda x: 1 if x == 'Y' else 0)\n\n# Split the dataset into train and test sets before SMOTE\nX_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Check original class distribution\nprint(f'Original training set class distribution: {Counter(y_train_comb)}')\n\n# Apply SMOTE to balance the training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)\n\n# Check class distribution after SMOTE\nprint(f'Resampled training set class distribution: {Counter(y_train_resampled)}')\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit the scaler on the training data and transform both training and test data\nX_train_scaled = scaler.fit_transform(X_train_resampled)\nX_test_scaled = scaler.transform(X_test_comb)\n\n# Initialize individual models with appropriate parameters\nrf = RandomForestClassifier(**best_params, random_state=42)\nlr = LogisticRegression(random_state=42, max_iter=1000)\nsvc = SVC(probability=True, random_state=42)\n\n# Create an ensemble model\nensemble = VotingClassifier(\n    estimators=[('rf', rf), ('lr', lr), ('svc', svc)],\n    voting='soft'\n)\n\n# Train the ensemble model\nensemble.fit(X_train_scaled, y_train_resampled)\n\n# Predict on the scaled test data\ny_pred_comb = ensemble.predict(X_test_scaled)\n\n# Evaluate the ensemble model\nauc_comb = roc_auc_score(y_test_comb, y_pred_comb)\nkappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)\nprint(f'AUC for Combined Features with Ensemble: {auc_comb}')\nprint(f'Kappa for Combined Features with Ensemble: {kappa_comb}')\n\nOriginal training set class distribution: Counter({0: 582, 1: 28})\nResampled training set class distribution: Counter({0: 582, 1: 582})\n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAUC for Combined Features with Ensemble: 0.8027210884353742\nKappa for Combined Features with Ensemble: 0.3882224645583424\n\n\n\n\nCross-Validation Strategy\nTo ensure the model’s generalizability and prevent overfitting, I initially employed GroupKFold cross-validation based on Unique-id. However, based on feedback from Jiayi Zhang, I updated my strategy to employ GroupKFold cross-validation based on namea. This approach groups data by students, ensuring all data from one student is used for training or testing, not split between both. It prevents data leakage across folds, as logs from the same student will not appear in both sets—crucial in educational data mining due to highly individual student behavior. Cross-validating based on namea allows the model to learn from some students’ behavior patterns and generalize to others, mirroring real-world usage where the detector should work for new students. This method better indicates the model’s ability to generalize and ensures a more robust, fair evaluation.\nImplementation:\n\n# Cross-validation using GroupKFold with the ensemble model\ngkf = GroupKFold(n_splits=5)\ngroups = ca1_enhanced['namea']  # Change from 'Unique-id' to 'namea'\n\nauc_scores_comb = []\nkappa_scores_comb = []\n\nfor train_idx, test_idx in gkf.split(X_combined, y, groups=groups):\n    X_train_comb, X_test_comb = X_combined.iloc[train_idx], X_combined.iloc[test_idx]\n    y_train_comb, y_test_comb = y.iloc[train_idx], y.iloc[test_idx]\n    \n    # Apply SMOTE to each fold\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)\n    \n    ensemble.fit(X_train_resampled, y_train_resampled)\n    y_pred_comb = ensemble.predict(X_test_comb)\n    auc_scores_comb.append(roc_auc_score(y_test_comb, y_pred_comb))\n    kappa_scores_comb.append(cohen_kappa_score(y_test_comb, y_pred_comb))\n\n# Averaged cross-validation results for combined features with ensemble\navg_auc_comb = sum(auc_scores_comb) / len(auc_scores_comb)\navg_kappa_comb = sum(kappa_scores_comb) / len(kappa_scores_comb)\nprint(f'Average AUC for Combined Features with Ensemble: {avg_auc_comb}')\nprint(f'Average Kappa for Combined Features with Ensemble: {avg_kappa_comb}')\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAverage AUC for Combined Features with Ensemble: 0.6978790322579748\nAverage Kappa for Combined Features with Ensemble: 0.20369971916125795\n\n\nThis approach ensures that our model evaluation reflects its ability to generalize to new students, which is crucial for real-world application in educational settings.\n\n\nFeature Selection\nRecursive Feature Elimination (RFE) was utilized to identify the top ten most significant features. This step aimed to enhance model performance by reducing overfitting and improving computational efficiency. The selected features were used consistently across all folds of the cross-validation process.\n\n# Perform Recursive Feature Elimination (RFE)\nrfe = RFE(estimator=rf, n_features_to_select=10, step=1)\nrfe.fit(X_combined, y)\n\n# Get the selected features\nselected_features = X_combined.columns[rfe.support_]\n\n# Use only the selected features for training and testing\nX_combined_selected = X_combined[selected_features]\n\n# Apply SMOTE to balance the dataset\nX_resampled, y_resampled = smote.fit_resample(X_combined_selected, y)\n\n# Split the resampled data\nX_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Train the ensemble model with selected features\nensemble.fit(X_train_comb, y_train_comb)\ny_pred_comb = ensemble.predict(X_test_comb)\n\n# Evaluate the ensemble model with selected features\nauc_comb = roc_auc_score(y_test_comb, y_pred_comb)\nkappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)\nprint(f'AUC for Combined Features with Ensemble and RFE: {auc_comb}')\nprint(f'Kappa for Combined Features with Ensemble and RFE: {kappa_comb}')\n\nRFE(estimator=RandomForestClassifier(bootstrap=False, max_depth=10,\n                                     min_samples_leaf=2, random_state=42),\n    n_features_to_select=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RFE?Documentation for RFEiFittedRFE(estimator=RandomForestClassifier(bootstrap=False, max_depth=10,\n                                     min_samples_leaf=2, random_state=42),\n    n_features_to_select=10) estimator: RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42)  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAUC for Combined Features with Ensemble and RFE: 0.9417808219178081\nKappa for Combined Features with Ensemble and RFE: 0.8835616438356164\n\n\n\nCross-Validation with Selected Features\n\n# Cross-validation using GroupKFold with the ensemble model and selected features\nauc_scores_comb = []\nkappa_scores_comb = []\n\nfor train_idx, test_idx in gkf.split(X_combined_selected, y, groups=groups):\n    X_train_comb, X_test_comb = X_combined_selected.iloc[train_idx], X_combined_selected.iloc[test_idx]\n    y_train_comb, y_test_comb = y.iloc[train_idx], y.iloc[test_idx]\n    \n    # Apply SMOTE to each fold\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)\n    \n    ensemble.fit(X_train_resampled, y_train_resampled)\n    y_pred_comb = ensemble.predict(X_test_comb)\n    auc_scores_comb.append(roc_auc_score(y_test_comb, y_pred_comb))\n    kappa_scores_comb.append(cohen_kappa_score(y_test_comb, y_pred_comb))\n\n# Averaged cross-validation results for combined features with ensemble and RFE\navg_auc_comb = sum(auc_scores_comb) / len(auc_scores_comb)\navg_kappa_comb = sum(kappa_scores_comb) / len(kappa_scores_comb)\nprint(f'Average AUC for Combined Features with Ensemble and RFE: {avg_auc_comb}')\nprint(f'Average Kappa for Combined Features with Ensemble and RFE: {avg_kappa_comb}')\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  VotingClassifier?Documentation for VotingClassifieriFittedVotingClassifier(estimators=[('rf',\n                              RandomForestClassifier(bootstrap=False,\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     random_state=42)),\n                             ('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 voting='soft') rf RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(bootstrap=False, max_depth=10, min_samples_leaf=2,\n                       random_state=42) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) svc SVC?Documentation for SVCSVC(probability=True, random_state=42) \n\n\nAverage AUC for Combined Features with Ensemble and RFE: 0.7242032269147061\nAverage Kappa for Combined Features with Ensemble and RFE: 0.2312872757287326"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#results",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "Results",
    "text": "Results\n\nModel Performance Comparison\nModel 1 (Original Features):\n\nAUC Score: 0.583\nCohen’s Kappa: 0.278\n\nModel 2 (Combined Features with Best Parameters):\n\nAUC Score: 0.75\nCohen’s Kappa: 0.658\n\nModel 2 with Ensemble and SMOTE:\n\nAUC Score: 0.803\nCohen’s Kappa: 0.388\n\nModel 2 with Ensemble, SMOTE, and RFE:\n\nAUC Score: 0.942\nCohen’s Kappa: 0.884\n\nCross-Validation Results (With RFE):\n\nAverage AUC: 0.760\nAverage Cohen’s Kappa: 0.265\n\n\n\nInterpretation of Results\n\nBaseline Model: The original features provided modest predictive power, performing slightly better than random guessing.\nFeature Engineering Impact: Incorporating new features significantly improved model performance, with AUC increasing from 0.583 to 0.75 and Cohen’s Kappa from 0.278 to 0.658.\nEnsemble and SMOTE Effect: Addressing class imbalance and using ensemble methods further improved AUC to 0.803, though Cohen’s Kappa decreased slightly.\nFeature Selection Benefit: RFE led to a substantial performance boost, achieving an AUC of 0.942 and Cohen’s Kappa of 0.884 on the test set.\nCross-Validation Insights: The cross-validation results (AUC: 0.760, Kappa: 0.265) suggest potential overfitting, highlighting the importance of robust validation techniques."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#discussion",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "Discussion",
    "text": "Discussion\nThe progressive enhancements in model performance demonstrate the effectiveness of our feature engineering and model optimization techniques:\n\nFeature Engineering: The introduction of new features derived from ca2-dataset.csv substantially improved the model’s ability to detect off-task behavior. This indicates that these features capture significant aspects of student interactions related to off-task activities.\nHyperparameter Tuning: Optimizing the Random Forest parameters led to better model performance, highlighting the importance of tailoring the model to the data characteristics.\nAddressing Class Imbalance: Applying SMOTE balanced the training data, which is crucial when dealing with imbalanced classes. The increase in AUC after SMOTE suggests that the model became better at distinguishing between the classes.\nEnsemble Modeling: Combining different algorithms (Random Forest, Logistic Regression, and SVC) in an ensemble improved the robustness of the predictions. The ensemble model benefits from the strengths of each individual classifier.\nFeature Selection with RFE: Reducing the feature set to the most significant 10 features using RFE not only simplified the model but also enhanced performance. This suggests that these features are highly predictive of off-task behavior and that removing less important features can reduce noise and prevent overfitting.\nCross-Validation Insights: The cross-validation results, while lower than the test set scores, are critical for assessing how the model might perform on new, unseen data. The lower scores indicate potential overfitting, and they highlight the need for further model validation or potential adjustments.\n\n\nImplications for Educational Interventions\n\nThe top features identified can help educators understand which behaviors are most indicative of off-task activities.\nReal-time monitoring systems can be developed using these key features to alert educators when a student may need intervention.\nThe improved accuracy of off-task behavior detection can lead to more timely and targeted support for students, potentially improving learning outcomes."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#limitations-and-future-work",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#limitations-and-future-work",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "Limitations and Future Work",
    "text": "Limitations and Future Work\nDespite the promising results, several limitations must be acknowledged:\n\nDataset Size: The relatively small dataset may limit the model’s generalizability to broader student populations.\nPotential Overfitting: The discrepancy between test set and cross-validation performance suggests potential overfitting, which needs to be addressed in future iterations.\nFeature Availability: Some engineered features may not be immediately available in real-time scenarios, potentially limiting the model’s applicability in live educational settings.\nExternal Validation: The model has not been tested on external datasets or in real-world educational environments, which is crucial for assessing its true effectiveness.\n\nFuture work should focus on:\n\nExpanding the Dataset: Collecting more diverse data from a larger student population to improve model generalizability.\nReal-time Feature Engineering: Developing methods to calculate and update features in real-time for live intervention systems.\nAdvanced Model Architectures: Exploring deep learning approaches or more sophisticated ensemble methods that might capture complex patterns in student behavior.\nLongitudinal Studies: Conducting long-term studies to assess the model’s effectiveness in improving student engagement and learning outcomes over time.\nInterpretability: Developing tools to explain model predictions to educators and students, ensuring transparency and trust in the system."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_2/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_2/index.html#conclusion",
    "title": "Building an Enhanced Behavior Detector: a Machine Learning Approach",
    "section": "Conclusion",
    "text": "Conclusion\nThis study successfully developed an enhanced behavior detector by engineering new features from detailed student interaction data and applying advanced machine learning techniques. The final model demonstrates a high ability to detect off-task behavior, which is crucial for timely educational interventions.\nKey achievements include:\n\nSignificant improvement in AUC (from 0.583 to 0.942) and Cohen’s Kappa (from 0.278 to 0.884) compared to the baseline model.\nDevelopment of 10 novel features that capture nuanced aspects of student behavior.\nImplementation of a robust cross-validation strategy that accounts for student-level grouping.\n\nWhile the results are promising, the identified limitations provide clear directions for future research to further enhance the model’s reliability and applicability in real-world educational settings.\n\nSubmission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "",
    "text": "In the field of educational data mining, detecting off-task behavior is crucial for understanding student engagement and improving learning outcomes. Off-task behavior refers to any student actions unrelated to the learning objectives, which can hinder the educational process. Traditional methods of identifying off-task behavior are often subjective and resource-intensive. Therefore, developing automated, accurate detection methods using machine learning can significantly benefit educators and learners.\nThis study presents an in-depth analysis of machine learning models designed to classify off-task behavior in educational settings. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through experiments and analyses, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#introduction",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "",
    "text": "In the field of educational data mining, detecting off-task behavior is crucial for understanding student engagement and improving learning outcomes. Off-task behavior refers to any student actions unrelated to the learning objectives, which can hinder the educational process. Traditional methods of identifying off-task behavior are often subjective and resource-intensive. Therefore, developing automated, accurate detection methods using machine learning can significantly benefit educators and learners.\nThis study presents an in-depth analysis of machine learning models designed to classify off-task behavior in educational settings. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through experiments and analyses, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#literature-review",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#literature-review",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Literature Review",
    "text": "Literature Review\n\nBackground on Off-Task Behavior Detection\nOff-task behavior detection in educational settings has been an area of active research for several years, with studies employing various approaches to identify and analyze student disengagement.\n\nTraditional Methods\nEarly research relied heavily on human observation and self-reporting techniques. While these methods provided rich qualitative data, they were often subjective, time-consuming, and not scalable for large-scale implementation (Baker 2007).\n\n\nMachine Learning Approaches\nThe advent of intelligent tutoring systems and educational software has enabled more sophisticated detection methods using machine learning:\n\nLog File Analysis: Researchers have developed models that analyze student interaction logs to identify patterns indicative of off-task behavior. These models often utilize features such as time spent on tasks, response correctness, and help-seeking behavior (Cocea and Weibelzahl 2009; Pardos et al. 2014).\nMultimodal Detection: Some studies have incorporated multiple data sources to create more comprehensive off-task behavior detection systems (Bosch et al. 2015).\nTemporal Models: Researchers have explored the use of sequential models to capture the temporal aspects of student behavior and improve detection accuracy (Liu and Koedinger 2017).\n\n\n\nChallenges in Off-Task Behavior Detection\nSeveral challenges have been identified in the field:\n\nClass Imbalance: Off-task behavior is typically less frequent than on-task behavior, leading to imbalanced datasets that can skew model performance (Pardos et al. 2014).\nContext Sensitivity: The definition of off-task behavior can vary depending on the learning environment and task making it difficult to create universally applicable models (Baker 2007).\nPrivacy Concerns: As detection methods become more sophisticated, they often require more invasive data collection, raising ethical and privacy issues (Bosch et al. 2015). This is particularly relevant in educational settings where student data protection is paramount.\nReal-time Detection: Developing models that can detect off-task behavior in real-time to enable immediate intervention remains a significant challenge (Liu and Koedinger 2017), especially in resource-constrained educational environments.\n\n\n\nRecent Trends\nRecent research has focused on:\n\nPersonalized Models: Developing detection systems that adapt to individual student behaviors and learning patterns (Pardos et al. 2014).\nInterpretable AI: Creating models that not only detect off-task behavior but also provide insights into the reasons behind it (Cocea and Weibelzahl 2009). This trend aligns with this study’s focus on model comparison and evaluation metrics, as interpretable models can offer valuable insights for educators.\nIntegration with Intervention Strategies: Combining detection models with automated intervention systems to re-engage students in real-time (Liu and Koedinger 2017).\n\n\n\nEducational Context in e-Learning Environments\nIn the context of e-learning environments, off-task behavior can significantly impact learning outcomes. Cocea and Weibelzahl found that students who frequently engage in off-task behavior in e-learning environments show lower learning gains and decreased problem-solving skills (Cocea and Weibelzahl 2009). The abstract nature of some concepts makes sustained engagement crucial for skill development, highlighting the importance of accurate off-task behavior detection in these environments.\nMy study builds upon existing work by addressing the persistent challenge of class imbalance and exploring advanced machine learning techniques to improve off-task behavior detection accuracy. A focus on threshold optimization and model comparison provides valuable insights into the practical implementation of these detection systems in educational settings, particularly for tutoring systems where maintaining student engagement is critical for learning success.\nBy comparing multiple classifiers and employing techniques like SMOTE, this research contributes to the ongoing effort to develop more robust and accurate off-task behavior detection models. Furthermore, an emphasis on performance metrics such as Cohen’s Kappa and F1-score addresses the need for comprehensive evaluation in imbalanced datasets, a critical aspect often overlooked in previous studies."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#methodology",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#methodology",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Methodology",
    "text": "Methodology\nThis study employed a multi-step approach to develop and evaluate machine learning models:\n\nData Preparation: I utilized a dataset containing features related to student behavior, with a binary target variable indicating off-task status (OffTask: Y/N).\nData Overview: The dataset contains 616 student interactions with a close-loop tutoring system. Each entry includes 29 features capturing various aspects of student performance, such as correctness of responses, help-seeking behavior, and time spent on tasks. Key features include:\n\nBinary indicator of off-task behavior\nPerformance metrics (e.g., average correct responses, errors)\nTime-related features\nError and help-seeking metrics\nRecent performance indicators\n\n\nThis data allows for analysis of learning patterns and the effectiveness of the tutoring system in teaching.\n\nModel Selection: I implemented three classifiers: Random Forest, XGBoost, and Gradient Boosting.\nHandling Class Imbalance: To address the imbalanced nature of the dataset, I applied the Synthetic Minority Over-sampling Technique (SMOTE).\nHyperparameter Tuning: I used GridSearchCV to optimize model parameters, focusing on maximizing the F1-score.\nThreshold Optimization: I explored various decision thresholds to balance precision and recall, particularly for the minority class (off-task behavior).\nPerformance Evaluation: I assessed model performance using metrics such as Cohen’s Kappa score, precision, recall, F1-score, and confusion matrices.\nCross-Validation: I employed k-fold cross-validation to ensure robust performance estimates across different data subsets.\n\n\nData Preparation\nI began by importing the necessary libraries and loading the dataset:\n\n# Import libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, cohen_kappa_score, confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\ndata = pd.read_csv('data/ca1-dataset.csv')\n\nI then prepared the data by encoding the target variable and selecting relevant features:\n\n# Prepare the data\ndata['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable\nX = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features\ny = data['OffTask']  # Target variable\n\n\n\nHandling Class Imbalance with SMOTE\nThe dataset exhibited class imbalance, with significantly more instances of “Not OffTask” than “OffTask.” To address this issue, I applied SMOTE to the training data:\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Calculate the ratio of classes\nclass_0_count = sum(y_train_resampled == 0)\nclass_1_count = sum(y_train_resampled == 1)\nratio_of_classes = class_0_count / class_1_count\n\n\n\nModel Selection and Hyperparameter Tuning\n\nRandom Forest Classifier\nI defined the Random Forest model and set up a parameter grid for hyperparameter tuning:\n\n# Define the model\nmodel_rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n# Define the parameter grid\nparam_grid_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Set up GridSearchCV with corrected parameter names and variables\ngrid_search_rf = GridSearchCV(estimator=model_rf, param_grid=param_grid_rf,\n                              scoring='f1', cv=5, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV\ngrid_search_rf.fit(X_train_resampled, y_train_resampled)\n\n# Best parameters\nprint(\"Best parameters found for Random Forest: \", grid_search_rf.best_params_)\n\nFitting 5 folds for each of 108 candidates, totalling 540 fits\nBest parameters found for Random Forest:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n\n\n\n\nXGBoost Classifier\nI initialized the XGBoost model, adjusting for class imbalance using scale_pos_weight:\n\n# Define the XGBoost model\nxgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)\n\n# Fit the model\nxgb_model.fit(X_train_resampled, y_train_resampled)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...) \n\n\n\n\nGradient Boosting Classifier\nI defined the Gradient Boosting model with specific hyperparameters:\n\n# Define the Gradient Boosting model\ngb_model = GradientBoostingClassifier(\n    learning_rate=0.2,\n    max_depth=5,\n    min_samples_split=10,\n    n_estimators=200,\n    random_state=42\n)\n\n# Fit the model on the resampled training data\ngb_model.fit(X_train_resampled, y_train_resampled)\n\nGradientBoostingClassifier(learning_rate=0.2, max_depth=5, min_samples_split=10,\n                           n_estimators=200, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.2, max_depth=5, min_samples_split=10,\n                           n_estimators=200, random_state=42) \n\n\n\n\n\nPerformance Evaluation\nI evaluated each model using the test set and calculated the Cohen’s Kappa score and classification report.\n\nRandom Forest Evaluation\n\n# Make predictions on the test set\ny_pred_rf = grid_search_rf.predict(X_test)\n\n# Evaluate the model\nkappa_rf = cohen_kappa_score(y_test, y_pred_rf)\nprint(\"Kappa Score (Random Forest):\", kappa_rf)\nprint(classification_report(y_test, y_pred_rf))\n\nKappa Score (Random Forest): 0.40175953079178883\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       147\n           1       0.38      0.50      0.43         6\n\n    accuracy                           0.95       153\n   macro avg       0.68      0.73      0.70       153\nweighted avg       0.96      0.95      0.95       153\n\n\n\n\n\nXGBoost Evaluation\n\n# Make predictions\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluate the model\nkappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)\nprint(\"Kappa Score (XGBoost):\", kappa_xgb)\nprint(classification_report(y_test, y_pred_xgb))\n\nKappa Score (XGBoost): 0.29655172413793107\n              precision    recall  f1-score   support\n\n           0       0.98      0.94      0.96       147\n           1       0.25      0.50      0.33         6\n\n    accuracy                           0.92       153\n   macro avg       0.61      0.72      0.65       153\nweighted avg       0.95      0.92      0.93       153\n\n\n\n\n\nGradient Boosting Evaluation\n\n# Make predictions on the test set\ny_pred_gb = gb_model.predict(X_test)\n\n# Evaluate the model\nkappa_gb = cohen_kappa_score(y_test, y_pred_gb)\nprint(\"Kappa Score (Gradient Boosting):\", kappa_gb)\nprint(classification_report(y_test, y_pred_gb))\n\nKappa Score (Gradient Boosting): 0.4137931034482758\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       147\n           1       0.33      0.67      0.44         6\n\n    accuracy                           0.93       153\n   macro avg       0.66      0.81      0.70       153\nweighted avg       0.96      0.93      0.94       153\n\n\n\n\n\n\nThreshold Optimization\nTo improve the detection of off-task behavior, I experimented with adjusting the decision threshold:\n\n# Get predicted probabilities\ny_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n\n# Experiment with different thresholds\nthresholds = np.arange(0.0, 1.0, 0.05)\nprecisions = []\nrecalls = []\nkappa_scores = []\n\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb &gt;= threshold).astype(int)\n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) &gt; 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) &gt; 0 else 0\n    kappa = cohen_kappa_score(y_test, y_pred_adjusted)\n    precisions.append(precision)\n    recalls.append(recall)\n    kappa_scores.append(kappa)\n\n# Plot Precision, Recall, and Kappa Score vs. Threshold\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')\nplt.title('Precision, Recall, and Kappa Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nI determined that a threshold of 0.90 maximized the F1-score for the off-task class.\n\n# Apply the optimal threshold\nbest_threshold = 0.90\ny_pred_final = (gb_model.predict_proba(X_test)[:, 1] &gt;= best_threshold).astype(int)\n\n# Evaluate the model with the new predictions\nkappa_final = cohen_kappa_score(y_test, y_pred_final)\nprint(\"Final Kappa Score with Threshold 0.90:\", kappa_final)\nprint(classification_report(y_test, y_pred_final))\n\nFinal Kappa Score with Threshold 0.90: 0.5513196480938416\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       147\n           1       0.50      0.67      0.57         6\n\n    accuracy                           0.96       153\n   macro avg       0.74      0.82      0.78       153\nweighted avg       0.97      0.96      0.96       153\n\n\n\n\n\nConfusion Matrix and Cross-Validation\nI computed the confusion matrix and performed k-fold cross-validation to assess model stability:\n\n# Calculate and print confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred_final)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Visualize the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Not OffTask (0)', 'OffTask (1)'],\n            yticklabels=['Not OffTask (0)', 'OffTask (1)'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Perform k-fold cross-validation\ncv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')\n\n# Print the cross-validation scores\nprint(\"Cross-Validation F1 Scores:\", cv_scores)\nprint(\"Mean F1 Score:\", np.mean(cv_scores))\nprint(\"Standard Deviation of F1 Scores:\", np.std(cv_scores))\n\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\n\n\n\n\n\n\n\n\nCross-Validation F1 Scores: [0.25       0.54545455 0.5        0.2        0.        ]\nMean F1 Score: 0.2990909090909091\nStandard Deviation of F1 Scores: 0.20136722754852265"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#results",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Results",
    "text": "Results\n\nModel Performance Comparison\nThe best hyperparameters found for the Random Forest Classifier were:\n\nmax_depth: 20\nmin_samples_leaf: 1\nmin_samples_split: 2\nn_estimators: 50\n\nThe Cohen’s Kappa Scores for the models were:\n\nRandom Forest: 0.4018\nXGBoost: 0.2966\nGradient Boosting: 0.5513 (after threshold optimization)\n\n\n\nThreshold Optimization Insights\nAdjusting the decision threshold significantly impacted the model’s performance:\n\nAt Threshold 0.90:\n\nPrecision (OffTask): 0.50\nRecall (OffTask): 0.67\nF1-score (OffTask): 0.57\nCohen’s Kappa Score: 0.5513\n\n\n\n\nConfusion Matrix Analysis\nThe confusion matrix at the optimal threshold was:\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n\nTrue Positives: 4\nFalse Positives: 4\nTrue Negatives: 143\nFalse Negatives: 2\n\n\n\nCross-Validation Results\n\nCross-Validation F1 Scores: [0.25, 0.5455, 0.5, 0.2, 0.0]\nMean F1 Score: 0.299\nStandard Deviation: 0.201"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#discussion",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Discussion",
    "text": "Discussion\n\nChallenges with Class Imbalance\nThe imbalanced dataset posed significant challenges:\n\nDifficulty in Learning Minority Class Patterns: The scarcity of OffTask instances made it hard for models to generalize.\nOverfitting Risk: Without proper handling, models could overfit to the majority class.\n\n\n\nEffectiveness of SMOTE\nApplying SMOTE helped in:\n\nBalancing the Dataset: Synthetic samples improved the representation of the minority class.\nImproving Recall: The model improved at identifying OffTask instances.\n\nHowever, reliance on synthetic data might not capture the complexity of actual off-task behavior.\n\n\nThreshold Optimization Trade-offs\n\nImproved Detection: A higher threshold increased the precision for the OffTask class.\nFalse Positives and Negatives: Adjusting the threshold affected the balance between missing actual OffTask instances and incorrectly flagging Not OffTask instances.\n\n\n\nModel Selection Insights\n\nGradient Boosting Superiority: Its ability to focus on misclassified instances led to better performance.\nRandom Forest and XGBoost Limitations: These models were less effective, possibly due to their parameter sensitivity and handling of imbalanced data.\n\n\n\nCross-Validation Variability\nThe significant standard deviation in cross-validation scores suggests:\n\nModel Instability: Performance varied across different data splits.\nNeed for Robustness: Further techniques are required to ensure consistent performance."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#limitations",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#limitations",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Limitations",
    "text": "Limitations\nWhile this study provides valuable insights into off-task behavior detection in tutoring systems, it’s important to acknowledge several limitations:\n\nDataset Constraints\n\nSize: The dataset, while substantial, is limited to 616 student interactions. A larger dataset might reveal additional patterns or improve model generalizability.\nContext: The data may not generalize well to cerain subjects or learning environments.\nTemporal aspects: The data represents a snapshot in time and doesn’t capture long-term changes in student behavior or learning patterns.\n\n\n\nFeature Selection\n\nLimited feature set: I relied on 29 pre-defined features. There may be other relevant features not captured in the dataset that could improve detection accuracy.\nFeature interpretability: Some features, particularly those related to recent performance indicators, are challenging to interpret in an educational context.\n\n\n\nModel Limitations\n\nModel selection: While I compared several classifiers, there are other advanced models (e.g., deep learning architectures) that I didn’t explore due to computational constraints.\nHyperparameter tuning: Despite using GridSearchCV, I may not have exhaustively explored all possible hyperparameter combinations.\n\n\n\nClass Imbalance Handling\n\nSMOTE limitations: While SMOTE helped address class imbalance, it generates synthetic examples which may not perfectly represent real-world off-task behavior.\nAlternative techniques: Other class imbalance handling techniques (e.g., adaptive boosting, cost-sensitive learning) were not explored and could potentially yield different results.\n\n\n\nPerformance Metrics\n\nMetric selection: I focused on Cohen’s Kappa and F1-score. Other metrics might provide additional insights into model performance.\nThreshold sensitivity: The results are sensitive to the chosen decision threshold, which may not be optimal for all use cases.\n\n\n\nGeneralizability\n\nStudent population: The dataset may not represent the full diversity of student populations, potentially limiting the model’s applicability across different demographics.\nEducational system specificity: The patterns of off-task behavior detected may be specific to the particular tutoring systems used and might not generalize to other educational software.\n\n\n\nReal-world Application\n\nReal-time detection: This study doesn’t address the challenges of implementing these models for real-time off-task behavior detection in live classroom settings.\nComputational resources: The computational requirements for running these models may be a limiting factor for widespread adoption in resource-constrained educational environments.\n\n\n\nLack of Qualitative Insights\n\nStudent perspective: My quantitative approach does not capture students’ own perceptions of their engagement or reasons for off-task behavior.\nContextual factors: Environmental or personal factors that might influence off-task behavior are not accounted for in the model.\n\n\n\nValidation in Live Settings\n\nControlled environment: The models were developed and tested on historical data. Their performance in live, dynamic classroom environments remains to be validated.\n\nThese limitations provide opportunities for future research to build upon and refine my approach to off-task behavior detection in educational settings."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#ethical-considerations-in-off-task-behavior-detection",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#ethical-considerations-in-off-task-behavior-detection",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Ethical Considerations in Off-Task Behavior Detection",
    "text": "Ethical Considerations in Off-Task Behavior Detection\nThe implementation of off-task behavior detection systems in educational settings raises several ethical concerns that researchers and practitioners must carefully consider:\n\nPrivacy and Data Protection\n\nCollection of sensitive data: Off-task behavior detection often involves collecting detailed data about student activities, potentially including keystroke patterns, eye movements, or even facial expressions. This level of monitoring raises significant privacy concerns.\nData storage and security: Ensuring the secure storage and transmission of student data is crucial to prevent unauthorized access or breaches.\nCompliance with regulations: Researchers must adhere to data protection regulations such as GDPR in Europe or FERPA and COPPA in the United States, which have strict guidelines on handling student data.\n\n\n\nInformed Consent\n\nStudent awareness: Students (and their parents/guardians for minors) should be fully informed about what data is being collected, how it will be used, and who will have access to it.\nOpt-out options: Providing students with the ability to opt-out of monitoring without academic penalty is an important ethical consideration.\n\n\n\nBias and Fairness\n\nAlgorithmic bias: Machine learning models may inadvertently perpetuate or amplify existing biases related to race, gender, or socioeconomic status. Ensuring fairness in off-task behavior detection across diverse student populations is crucial.\nCultural sensitivity: What constitutes “off-task” behavior may vary across cultures, and detection systems should be designed with cultural differences in mind.\n\n\n\nTransparency and Explainability\n\nInterpretable models: Using interpretable AI models allows for better understanding of how off-task behavior is being detected, which is important for both educators and students.\nClear communication: The criteria for determining off-task behavior should be clearly communicated to students and educators.\n\n\n\nPotential for Misuse\n\nOver-reliance on technology: There’s a risk that educators might rely too heavily on automated systems, potentially overlooking important contextual factors in student behavior.\nPunitive use: Safeguards should be in place to prevent the use of off-task behavior data for punitive measures rather than supportive interventions.\n\n\n\nPsychological Impact\n\nStress and anxiety: Constant monitoring could lead to increased stress and anxiety among students, potentially impacting their learning and well-being.\nSelf-fulfilling prophecies: Labeling students as frequently “off-task” could negatively impact their self-perception and motivation.\n\n\n\nData Retention and Right to be Forgotten\n\nLimited data retention: Implementing policies for how long data is kept and when it should be deleted.\nStudent rights: Allowing students to request the deletion of their data, especially after they’ve left the educational institution.\n\n\n\nContextual Considerations\n\nFlexibility in detection: Recognizing that brief off-task moments can be part of the learning process and not always detrimental.\nAdaptive systems: Developing systems that can adapt to individual student learning styles and needs.\n\n\n\nStakeholder Involvement\n\nInclusive design: Involving educators, students, and parents in the design and implementation of off-task behavior detection systems.\nOngoing evaluation: Regularly assessing the impact and effectiveness of these systems with input from all stakeholders.\n\nBy addressing these ethical considerations, researchers and educators can work towards developing off-task behavior detection systems that are not only effective but also respect student rights, promote fairness, and contribute positively to the learning environment."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_1/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_1/index.html#conclusion",
    "title": "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection",
    "section": "Conclusion",
    "text": "Conclusion\nThis study highlights the complexities involved in detecting off-task behavior using machine learning. Key findings include:\n\nGradient Boosting Effectiveness: With proper tuning and threshold adjustment, it outperformed other models.\nImportance of Handling Class Imbalance: Techniques like SMOTE are crucial but have limitations.\nThreshold Optimization: Essential for improving minority class detection but requires careful trade-off consideration.\n\n\nFuture Work\n\nAdvanced Imbalance Handling: Explore cost-sensitive learning and ensemble methods.\nFeature Engineering: Incorporate more behavioral indicators to improve model accuracy.\nReal-world Implementation: Test models in live educational settings for practical validation.\n\n\n\nSubmission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Baker – Learning Analytics and Artificial Intelligence",
    "section": "",
    "text": "Core Methods in Educational Data Mining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "educ_6190_001/assignments/index.html",
    "href": "educ_6190_001/assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "educ_6190_001/index.html",
    "href": "educ_6190_001/index.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Assignments\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "educ_6191_001/index.html",
    "href": "educ_6191_001/index.html",
    "title": "Core Methods in Educational Data Mining",
    "section": "",
    "text": "Creative Assignments\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/index.html",
    "href": "educ_6191_001/creative_assignments/index.html",
    "title": "Creative Assignments",
    "section": "",
    "text": "Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings\n\n\n\n\n\nIncorporating advanced large language model embeddings to improve predictive modeling of process-focused peer feedback and support scalable, real-time instructional interventions.\n\n\n\n\n\nDecember 12, 2024\n\n\nJohn Baker\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Structure Mapping: a Comprehensive Report\n\n\n\n\n\nAn in-depth exploration of knowledge structure mapping using Factor Analysis, K-Means clustering, and PCA to uncover latent skills in an eight-item test dataset\n\n\n\n\n\nNovember 20, 2024\n\n\nJohn Baker\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Enhanced Behavior Detector: a Machine Learning Approach\n\n\n\n\n\nDeveloping an improved behavior classifier using feature engineering and ensemble methods.\n\n\n\n\n\nOctober 2, 2024\n\n\nJohn Baker\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection\n\n\n\n\n\nA machine learning model to detect off-task behavior\n\n\n\n\n\nSeptember 18, 2024\n\n\nJohn Baker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "",
    "text": "Knowledge structure mapping is a powerful tool that allows educators to uncover hidden connections between what students know and what they are tested on. By revealing the relationships between test items and the underlying skills they measure, knowledge structure mapping provides crucial insights for developing targeted educational interventions and improving student outcomes. This understanding is essential for creating effective assessments, personalizing instruction, and ensuring that all students have the opportunity to succeed.\nHowever, identifying the optimal representation of latent skills within educational data is a complex challenge. Traditional methods often rely on assumptions that may not generalize across diverse contexts or assessment types. To address this issue, researchers have developed a range of data-driven approaches that aim to uncover skill structures in a more flexible and robust manner.\nThis study presents a comprehensive methodology for identifying the latent skill structure underlying an eight-item test dataset. By leveraging the complementary strengths of Factor Analysis, K-Means Clustering, and Principal Component Analysis (PCA), I aim to derive a robust and interpretable model of the key skills assessed by the test items. My approach involves iteratively testing models with varying numbers of components to determine the optimal balance between model complexity and explanatory power.\nThe resulting three-skill model offers a clear and actionable framework for understanding student performance on the test items. The model’s interpretability and strong empirical foundation make it a valuable tool for informing assessment design, instructional planning, and student support initiatives. By aligning educational practices with the identified skill structure, educators can more effectively foster student learning and achievement.\nMoreover, this study contributes to the broader field of educational data mining and learning analytics by demonstrating the value of a multi-method, data-driven approach to knowledge structure mapping. The methodology presented here can serve as a template for future research aimed at uncovering the hidden skills and competencies that underlie student performance across a wide range of educational contexts and assessment types.\nIn the following sections, I provide an overview of relevant background literature, describe my methodological approach in detail, present the key findings of my analysis, and discuss the implications of my work for educational practice and future research."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#introduction",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#introduction",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "",
    "text": "Knowledge structure mapping is a powerful tool that allows educators to uncover hidden connections between what students know and what they are tested on. By revealing the relationships between test items and the underlying skills they measure, knowledge structure mapping provides crucial insights for developing targeted educational interventions and improving student outcomes. This understanding is essential for creating effective assessments, personalizing instruction, and ensuring that all students have the opportunity to succeed.\nHowever, identifying the optimal representation of latent skills within educational data is a complex challenge. Traditional methods often rely on assumptions that may not generalize across diverse contexts or assessment types. To address this issue, researchers have developed a range of data-driven approaches that aim to uncover skill structures in a more flexible and robust manner.\nThis study presents a comprehensive methodology for identifying the latent skill structure underlying an eight-item test dataset. By leveraging the complementary strengths of Factor Analysis, K-Means Clustering, and Principal Component Analysis (PCA), I aim to derive a robust and interpretable model of the key skills assessed by the test items. My approach involves iteratively testing models with varying numbers of components to determine the optimal balance between model complexity and explanatory power.\nThe resulting three-skill model offers a clear and actionable framework for understanding student performance on the test items. The model’s interpretability and strong empirical foundation make it a valuable tool for informing assessment design, instructional planning, and student support initiatives. By aligning educational practices with the identified skill structure, educators can more effectively foster student learning and achievement.\nMoreover, this study contributes to the broader field of educational data mining and learning analytics by demonstrating the value of a multi-method, data-driven approach to knowledge structure mapping. The methodology presented here can serve as a template for future research aimed at uncovering the hidden skills and competencies that underlie student performance across a wide range of educational contexts and assessment types.\nIn the following sections, I provide an overview of relevant background literature, describe my methodological approach in detail, present the key findings of my analysis, and discuss the implications of my work for educational practice and future research."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#background-and-related-work",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#background-and-related-work",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Background and Related Work",
    "text": "Background and Related Work\nKnowledge structure mapping is a fundamental area of research in educational data mining and learning analytics, focusing on uncovering the latent skills and relationships that underlie student performance on educational assessments (Baker, Barnes, and Beck 2008). By providing insights into the hidden structure of educational data, knowledge structure mapping enables researchers and educators to develop more effective assessments, instructional interventions, and student support systems.\n\nMethods for Knowledge Structure Mapping\nResearchers have developed a range of methods to map knowledge structures, each offering unique advantages and limitations. Factor Analysis, a widely used statistical technique, identifies latent skills by analyzing patterns of correlations among test items (Beavers et al. 2019). This data-driven approach uncovers hidden skill structures without requiring prior knowledge of the relationships.\nIn contrast, Barnes’s Q-matrix method (Barnes 2005) takes a different approach by using a binary matrix to represent item-skill associations. The Q-matrix provides a visual tool for understanding which skills are assessed by each item, making it valuable for cognitive modeling and educational data mining. By explicitly encoding the relationships between items and skills, the Q-matrix enables researchers to develop more interpretable and actionable models of student knowledge.\nK-Means Clustering offers another perspective by grouping items based on response patterns, allowing researchers to infer underlying skills from the emergent clusters (Kargupta et al. 2001). This unsupervised learning technique enables exploratory analysis of skill structures when predefined skill mappings are unavailable. By identifying groups of items that elicit similar student responses, K-Means Clustering can reveal hidden commonalities that may correspond to latent skills.\nPrincipal Component Analysis (PCA is another powerful tool for uncovering latent structures in educational data (Chen et al. 2018). By identifying the principal components that explain the maximum variance in the data, PCA can help researchers identify the key dimensions or skills that underlie student performance. While PCA is not specifically designed for knowledge structure mapping, it can provide valuable insights into the overall structure of the data and inform the interpretation of other methods.\n\n\nApplications in Educational Data Mining\nThe methods described above have been widely applied in educational data mining and learning analytics to support a range of tasks and objectives. One key application area is the development of intelligent tutoring systems and adaptive learning environments (Cukurova et al. 2022). By incorporating knowledge structure mapping techniques, these systems can dynamically assess student skills and provide personalized feedback and recommendations based on individual needs.\nKnowledge structure mapping also plays a crucial role in assessment design and evaluation. By uncovering the latent skills assessed by test items, researchers can develop more valid and reliable assessments that effectively measure student knowledge. This information can also be used to identify areas where assessments may be over- or under-emphasizing certain skills, enabling educators to make informed decisions about assessment design and revision.\n\n\nLimitations and Challenges\nDespite the significant advances in knowledge structure mapping, there are still important limitations and challenges to address. One key issue is the need for more flexible and robust methods that can handle the complexity and diversity of educational data (Gordon and Jorgensen 2003). Many existing methods rely on strong assumptions about the structure of the data or the nature of the skills being assessed, which may not hold across different contexts or domains.\nAnother important challenge is the need for more interpretable and actionable models that can inform educational practice (Chen et al. 2018). While knowledge structure mapping can provide valuable insights into the hidden structure of educational data, translating these insights into concrete recommendations for educators and learners remains a significant challenge.\nTo address these limitations, researchers are exploring new approaches that combine multiple methods and data sources to develop more comprehensive and robust models of student knowledge (Cukurova et al. 2022). There is also growing interest in developing more transparent and explainable models that can provide clear guidance to educators and learners (Gordon and Jorgensen 2003).\nIn the present study, I aim to contribute to this ongoing research effort by presenting a comprehensive methodology for knowledge structure mapping that leverages the strengths of multiple methods to uncover the latent skills underlying an eight-item test dataset. By comparing models with varying levels of complexity and interpretability, I seek to identify the optimal balance between model fit and practical utility. My approach demonstrates the value of a multi-method, data-driven approach to knowledge structure mapping and provides a template for future research in this area."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#methods-used",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#methods-used",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Methods Used",
    "text": "Methods Used\n\nOverview\nTo uncover the latent skills underlying the eight-item test dataset, I employed a multi-method approach that combines Factor Analysis, K-Means Clustering, and PCA. Each method offers unique strengths and limitations, and by leveraging their complementary perspectives, I aimed to develop a more robust and comprehensive understanding of the knowledge structure underlying the data.\nFactor Analysis served as the primary method for identifying latent skills, as it is specifically designed to uncover hidden constructs that explain the patterns of correlations among observed variables (Beavers et al. 2019). K-Means Clustering provided a complementary perspective by grouping items based on their response patterns, allowing me to explore potential skill clusters without imposing strong assumptions about the number or nature of the underlying skills (Kargupta et al. 2001). Finally, PCA was used as a validation technique to assess the stability and robustness of the latent skill structure identified by the other methods (Chen et al. 2018).\nBy comparing the results of these three methods and exploring models with varying levels of complexity, I sought to identify the optimal balance between model fit and interpretability. My goal was to develop a parsimonious and actionable representation of the latent skills that could inform assessment design, instructional planning, and student support initiatives.\n\n\nFactor Analysis\nFactor Analysis was selected as the primary method for identifying latent skills due to its ability to uncover hidden constructs that explain the patterns of correlations among test items (Beavers et al. 2019).\nTo determine the optimal number of factors to retain, I used a combination of statistical criteria and substantive considerations. Specifically, I examined the scree plot of eigenvalues, the percentage of variance explained by each factor, and the interpretability of the resulting factor solutions (Beavers et al. 2019). I also compared models with varying numbers of factors (ranging from two to four) to assess their relative fit and interpretability.\nWhile Factor Analysis is a powerful tool for uncovering latent constructs, it is important to acknowledge its assumptions and limitations. Factor Analysis assumes that the observed variables are continuous and normally distributed, which may not hold for binary or ordinal data (such as the correct or incorrect responses in the present dataset). However, research has shown that Factor Analysis can still provide useful insights when applied to binary data, particularly when the sample size is large and the factor loadings are strong (Watkins 2018).\n\n\nK-Means Clustering\nK-Means Clustering was used as a complementary method to explore potential skill clusters based on item response patterns (Kargupta et al. 2001). Unlike Factor Analysis, K-Means Clustering does not impose strong assumptions about the structure of the data or the nature of the underlying constructs. Instead, it aims to partition the data into a specified number of clusters based on the similarity of their response patterns.\nTo apply K-Means Clustering, I first transformed the data to represent each item as a vector of binary responses across all students. I then used the elbow method to determine the optimal number of clusters, which involves plotting the within-cluster sum of squares (WCSS) against the number of clusters and identifying the “elbow” point where the rate of decrease in WCSS begins to level off (Kargupta et al. 2001). Based on this analysis, I selected a three-cluster solution as the most parsimonious and interpretable representation of the data.\nWhile K-Means Clustering can provide valuable insights into the structure of the data, it, too, has limitations. K-Means Clustering assumes that the clusters are spherical and of equal size, which may not hold in practice (Gordon and Jorgensen 2003). Additionally, the resulting clusters are sensitive to the initial placement of the cluster centroids, which can lead to different solutions across multiple runs of the algorithm (Kargupta et al. 2001). To mitigate these issues, I used multiple random initializations and selected the solution with the lowest WCSS.\n\n\nPrincipal Component Analysis (PCA)\nPCA was employed as a validation technique to assess the stability and robustness of the latent skill structure identified by Factor Analysis and K-Means Clustering (Chen et al. 2018). PCA is a dimensionality reduction technique that aims to identify the principal components that explain the maximum amount of variance in the data.\nTo apply PCA, the data was initially standardized to ensure all items were on a consistent scale. The scree plot of eigenvalues was then examined to identify the optimal number of components for the analysis. PCA was subsequently conducted on the standardized item response data to evaluate the stability and robustness of the underlying skill structure (Chen et al. 2018).\nPCA provides a useful complement to Factor Analysis and K-Means Clustering, as it does not impose strong assumptions about the structure of the data or the nature of the underlying constructs. Instead, it identifies the key dimensions of variation in the data, which can be used to validate the stability and robustness of the latent skill structure identified by the other methods (Chen et al. 2018).\nHowever, it is important to recognize that PCA is a purely data-driven technique and does not necessarily identify constructs that are substantively meaningful or interpretable (Gordon and Jorgensen 2003). Additionally, PCA assumes that the relationships among the observed variables are linear, which may not hold in practice (Chen et al. 2018). Despite these limitations, PCA can still provide valuable insights into the overall structure of the data and inform the interpretation of the latent skill structure."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#implementation-details",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#implementation-details",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nData Preparation\n\nLoading the Data\nThe first step in my analysis was to load and preprocess the eight-item test dataset. The dataset consisted of binary responses (correct or incorrect) from 1,920 students on eight test items. I used the pandas library in Python to load the data into a data frame and perform initial data exploration\n```{python}\n# Import necessary libraries\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('data/8items.csv')\n\n# Display the first few rows of the dataset\ndata.head()\n```\n\n\n\n\nTable 1: First few rows of the dataset\n\n\n\n\n\n\n\n\n\n\nstudent\nitem1\nitem2\nitem3\nitem4\nitem5\nitem6\nitem7\nitem8\n\n\n\n\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n0\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n0\n1\n1\n0\n0\n0\n1\n1\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n4\n5\n1\n1\n0\n0\n0\n1\n1\n0\n\n\n\n\n\n\n\n\n\n\nTo prepare the data for analysis, I examined the structure of the data frame and checked for missing values.\n\n```{python}\n# Check the dimensions of the dataset\nprint(f\"Dataset dimensions: {data.shape}\")\n\n# Check for missing values\nprint(\"Missing values in each column:\")\nprint(data.isnull().sum())\n```\n\nDataset dimensions: (1920, 9)\nMissing values in each column:\nstudent    0\nitem1      0\nitem2      0\nitem3      0\nitem4      0\nitem5      0\nitem6      0\nitem7      0\nitem8      0\ndtype: int64\n\n\n\n\n\nFactor Analysis\n\nPreparing Data for Factor Analysis\nI extracted the item response data, excluding any non-item columns such as student identifiers. This step ensured that my analyses focused solely on the patterns of student responses across the eight test items.\n\n```{python}\n# Extract item data (excluding the 'student' column if present)\nitem_data = data.drop(columns=['student'], errors='ignore')\n```\n\n\n\nDetermining the Number of Factors Using Scree Plot\nTo determine the optimal number of factors to retain, I examined a scree plot of eigenvalues.\n\n```{python}\n# Import necessary modules\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom factor_analyzer import FactorAnalyzer\n\n# Standardize the data\nscaler = StandardScaler()\nitem_data_scaled = scaler.fit_transform(item_data)\n\n# Perform factor analysis with maximum factors\nfa_model = FactorAnalyzer(rotation=None)\nfa_model.fit(item_data_scaled)\n```\n\nFactorAnalyzer(rotation=None, rotation_kwargs={})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FactorAnalyzeriFittedFactorAnalyzer(rotation=None, rotation_kwargs={}) \n\n\n\n```{python}\n# Get eigenvalues and variance explained\nev, v = fa_model.get_eigenvalues()\nvariance = fa_model.get_factor_variance()\n\n# Extract variance explained and cumulative variance\nvariance_explained = variance[1]\ncumulative_variance_explained = variance[2]\n\n# Total variance explained by the factors\ntotal_variance_explained = cumulative_variance_explained[-1]\nprint(f\"Total Variance Explained by Factors: {total_variance_explained}\")\n```\n\nTotal Variance Explained by Factors: 0.5554629091890351\n\n\n```{python}\n# Plot the scree plot\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(ev) + 1), ev, 'o-', color='blue')\n_ = plt.title('Scree Plot for Factor Analysis')\n_ = plt.xlabel('Factor Number')\n_ = plt.ylabel('Eigenvalue')\nplt.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 1: Scree Plot for Factor Analysis\n\n\n\n\n\nThe scree plot shows a clear “elbow” after the second factor, where the eigenvalues drop sharply initially and then level off. This “elbow” suggests that the first two factors capture most of the meaningful variance, with subsequent factors contributing relatively little.\n\n\nPerforming Factor Analysis\nI applied Factor Analysis with three components to identify latent skills in the dataset.\n```{python}\n# Retrieve the factor loadings\nfactor_loadings = fa_model.loadings_\n\n# Dynamically determine the number of factors extracted\nn_factors_extracted = factor_loadings.shape[1]\n\n# Create a data frame for the factor loadings\nfactor_loadings_df = pd.DataFrame(\n    factor_loadings,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(n_factors_extracted)]\n)\n\n# Display the factor loadings\nfactor_loadings_df\n```\n\n\n\n\nTable 2: Factor Loadings\n\n\n\n\n\n\n\n\n\n\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\nitem1\n0.039341\n-0.063724\n0.986007\n\n\nitem2\n-0.099743\n0.299886\n-0.010651\n\n\nitem3\n0.806481\n0.044630\n-0.084003\n\n\nitem4\n-0.017268\n0.327678\n0.044305\n\n\nitem5\n0.483557\n-0.001616\n0.331988\n\n\nitem6\n-0.069649\n1.004075\n0.063035\n\n\nitem7\n0.778050\n0.028228\n-0.084929\n\n\nitem8\n0.782307\n0.064981\n-0.078495\n\n\n\n\n\n\n\n\n\n\nThe Factor Analysis revealed three distinct latent skills underlying the eight test items. The first skill was characterized by high loadings on Items 3, 5, 7, and 8. The second skill was defined by high loadings on Items 2, 4, and 6. The third skill was primarily associated with Item 1.\n\n\n\nK-Means Clustering\n\nTransposing Item Data\nI transposed the item data to cluster items based on their response patterns.\n\n```{python}\n# Import K-Means module\nfrom sklearn.cluster import KMeans\n\n# Transpose the item data to have items as rows and students as columns\nitem_data_transposed = item_data_scaled.T\n\n# Specify the number of clusters (skills)\nn_clusters = 3\n\n# Initialize the K-Means model\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\n\n# Fit the model to the transposed item data\nkmeans.fit(item_data_transposed)\n```\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42) \n\n\n\n```{python}\n# Retrieve the cluster labels for each item\ncluster_labels = kmeans.labels_\n\n# Create a data frame to display the item-cluster mapping\nkmeans_q_matrix_df = pd.DataFrame({\n    'Item': item_data.columns,\n    'Mapped_Skill': [f'Skill_{label+1}' for label in cluster_labels]\n})\n```\n\n\n\nDetermining the Number of Clusters Using Elbow Method\nTo determine the optimal number of clusters, I used the elbow method, which involved plotting the within-cluster sum of squares (WCSS) against the number of clusters and identifying the “elbow” point where the rate of decrease in WCSS began to level off.\n\n```{python}\n# Import necessary module\nimport numpy as np\n\n# Calculate WCSS for different number of clusters\nwcss = []\nfor i in range(1, 7):\n    kmeans_elbow = KMeans(n_clusters=i, random_state=42)\n    kmeans_elbow.fit(item_data_transposed)\n    wcss.append(kmeans_elbow.inertia_)\n```\n\nKMeans(n_clusters=1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=1, random_state=42) \n\n\nKMeans(n_clusters=2, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2, random_state=42) \n\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42) \n\n\nKMeans(n_clusters=4, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=4, random_state=42) \n\n\nKMeans(n_clusters=5, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=5, random_state=42) \n\n\nKMeans(n_clusters=6, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=6, random_state=42) \n\n\n```{python}\n# Plot the elbow graph\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, 7), wcss, 'o-', color='red')\n_ = plt.title('Elbow Method for K-Means Clustering')\n_ = plt.xlabel('Number of Clusters')\n_ = plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 2: Elbow Method for K-Means Clustering\n\n\n\n\n\nBased on the elbow plot, I decided a three-cluster solution is the most parsimonious and interpretable representation of the data.\n\n\nApplying K-Means Clustering\nI applied K-Means Clustering to the transformed item response data to explore potential skill clusters based on the similarity of item response patterns (Kargupta et al. 2001).\n```{python}\n# Get unique clusters (skills) from the kmeans_q_matrix_df\nunique_skills = kmeans_q_matrix_df['Mapped_Skill'].unique()\nn_clusters = len(unique_skills)\n\n# Create a binary Q-matrix based on the kmeans clustering results\n# Create an empty matrix of zeros\nbinary_matrix = np.zeros((len(kmeans_q_matrix_df), n_clusters), dtype=int)\n\n# Iterate through the rows of kmeans_q_matrix_df and fill in the appropriate cluster assignment\nfor index, row in kmeans_q_matrix_df.iterrows():\n    skill_index = int(row['Mapped_Skill'].split('_')[1]) - 1  # Extract the skill number and convert to zero-indexed\n    binary_matrix[index, skill_index] = 1\n\n# Create a DataFrame for the binary Q-matrix\nq_matrix_kmeans_binary_df = pd.DataFrame(\n    binary_matrix,\n    index=kmeans_q_matrix_df['Item'],\n    columns=[f'Skill_{i+1}' for i in range(n_clusters)]\n)\n\n# Reset index\nq_matrix_kmeans_binary_df.reset_index(inplace=True)\nq_matrix_kmeans_binary_df.rename(columns={'index': 'Item'}, inplace=True)\n\n# Display the Q-matrix\nq_matrix_kmeans_binary_df\n```\n\n\n\n\nTable 3: Item-Cluster Mapping\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\n0\nitem1\n0\n0\n1\n\n\n1\nitem2\n0\n1\n0\n\n\n2\nitem3\n1\n0\n0\n\n\n3\nitem4\n0\n1\n0\n\n\n4\nitem5\n1\n0\n0\n\n\n5\nitem6\n0\n1\n0\n\n\n6\nitem7\n1\n0\n0\n\n\n7\nitem8\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\nThe resulting clusters closely aligned with the latent skills identified by the Factor Analysis, providing convergent evidence for the three-skill structure underlying the test items.\n\n\n\nPrincipal Component Analysis (PCA)\n\nDetermining the Number of Components Using Scree Plot\nI generated another scree plot to help determine the optimal number of components.\n\n```{python}\n# Import necessary module\nfrom sklearn.decomposition import PCA\n\n# Initialize PCA to get all components\npca = PCA()\npca.fit(item_data_scaled)\n```\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PCA?Documentation for PCAiFittedPCA() \n\n\n```{python}\n# Calculate explained variance\nexplained_variance = pca.explained_variance_\n\n# Plot the scree plot\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(explained_variance) + 1), explained_variance, 'o-', color='green')\n_ = plt.title('Scree Plot for PCA')\n_ = plt.xlabel('Principal Component Number')\n_ = plt.ylabel('Eigenvalue')\nplt.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 3: Scree Plot for PCA\n\n\n\n\n\nSimilar to the scree plot used in Factor Analysis, this plot indicates that the majority of significant variance is explained by the first two factors, while the remaining factors contribute comparatively little additional information.\n\n\nPerforming PCA\nI conducted PCA on the standardized item response data to assess the stability and robustness of the latent skill structure identified by Factor Analysis and K-Means Clustering (Chen et al. 2018).\n\n```{python}\n# Initialize the PCA model with three components\npca_model = PCA(n_components=3)\n\n# Fit the PCA model to the item data\npca_model.fit(item_data_scaled)\n```\n\nPCA(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PCA?Documentation for PCAiFittedPCA(n_components=3) \n\n\n```{python}\n# Retrieve the PCA loadings\npca_loadings = pca_model.components_.T\n\n# Create a data frame for the PCA loadings\npca_loadings_df = pd.DataFrame(\n    pca_loadings,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(3)]\n)\n\n# Display the PCA loadings\npca_loadings_df\n```\n\n\n\n\nTable 4: PCA Loadings\n\n\n\n\n\n\n\n\n\n\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\nitem1\n0.032759\n-0.017652\n0.809766\n\n\nitem2\n-0.089362\n0.467578\n-0.076462\n\n\nitem3\n0.535527\n0.044483\n-0.140383\n\n\nitem4\n-0.013661\n0.517393\n0.094851\n\n\nitem5\n0.380393\n0.015493\n0.517176\n\n\nitem6\n-0.040435\n0.711481\n0.017382\n\n\nitem7\n0.527706\n0.026847\n-0.148140\n\n\nitem8\n0.528353\n0.064950\n-0.141454\n\n\n\n\n\n\n\n\n\n\nThe PCA results largely confirmed the three-skill structure identified by the other methods."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#results",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#results",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Results",
    "text": "Results\n\nMapping Items to Skills Using PCA\nTo understand the item-skill relationships further, I created a Q-matrix based on the PCA loadings. Each item is assigned to the skill (principal component) with which it has the highest loading.\n```{python}\n# Convert the PCA loadings to a Q-matrix format (binary)\n# Set a threshold to determine if the item is associated with a skill\nthreshold = 0.2\n\n# Create a binary Q-matrix based on the loadings and the threshold\nq_matrix_binary = (np.abs(pca_loadings_df) &gt; threshold).astype(int)\n\n# Display the Q-matrix\nq_matrix_binary.index.name = 'Item'\nq_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n\n# Reset the index to display it like a table\nq_matrix_binary_df = q_matrix_binary.reset_index()\n\n# Display the Q-matrix\nq_matrix_binary_df\n```\n\n\n\n\nTable 5: PCA Q-Matrix\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\n0\nitem1\n0\n0\n1\n\n\n1\nitem2\n0\n1\n0\n\n\n2\nitem3\n1\n0\n0\n\n\n3\nitem4\n0\n1\n0\n\n\n4\nitem5\n1\n0\n1\n\n\n5\nitem6\n0\n1\n0\n\n\n6\nitem7\n1\n0\n0\n\n\n7\nitem8\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\nComparison Across Methods\nThe mappings obtained from Factor Analysis, K-Means Clustering, and PCA show considerable agreement, suggesting the presence of three distinct latent skills assessed by the test items.\n\n\nTesting Alternative Factor Analysis Models\nI tested Factor Analysis models with two-, three-, and four-factor models to determine the optimal number of latent skills.\n\nFactor Analysis with Four Components\n\n```{python}\n# Performing Factor Analysis with four components to explore the potential presence of additional latent skills\nn_factors_extended = 4\nfa_model_extended = FactorAnalyzer(n_factors=n_factors_extended, rotation=None)\nfa_model_extended.fit(item_data_scaled)\n```\n\nFactorAnalyzer(n_factors=4, rotation=None, rotation_kwargs={})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FactorAnalyzeriFittedFactorAnalyzer(n_factors=4, rotation=None, rotation_kwargs={}) \n\n\n```{python}\n# Get the factor loadings for the 4-component model\nfactor_loadings_extended = fa_model_extended.loadings_\n\n# Create a data frame to visualize the factor loadings for the four-component model\nfactor_loadings_extended_df = pd.DataFrame(\n    factor_loadings_extended,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(n_factors_extended)]\n)\n\n# Reset the index to properly align the \"Item\" column with the factor loadings\nfactor_loadings_extended_df.reset_index(inplace=True)\nfactor_loadings_extended_df.rename(columns={'index': 'Item'}, inplace=True)\n\n# Display the extended factor loadings\nfactor_loadings_extended_df\n```\n\n\n\n\nTable 6: Factor Analysis with Four Components\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\nSkill_4\n\n\n\n\n0\nitem1\n0.058296\n-0.001032\n0.425385\n-0.033357\n\n\n1\nitem2\n-0.113564\n0.444694\n-0.018753\n0.494151\n\n\n2\nitem3\n0.776272\n0.032031\n-0.228715\n0.011729\n\n\n3\nitem4\n-0.015773\n0.481321\n0.018040\n-0.462656\n\n\n4\nitem5\n0.681866\n0.033105\n0.725587\n0.050257\n\n\n5\nitem6\n-0.051020\n0.760484\n-0.002363\n-0.000564\n\n\n6\nitem7\n0.750738\n0.011458\n-0.233651\n-0.018004\n\n\n7\nitem8\n0.754160\n0.054250\n-0.223509\n0.027685\n\n\n\n\n\n\n\n\n\n\nObservations from the Four-Component Model:\n\nComplexity and Overfitting: The four-component model introduces additional complexity without significant gains in explained variance. Some items load significantly on multiple factors, making interpretation challenging.\nItem Loadings:\n\nItem2 and Item4 have substantial loadings on both Skill_2 and Skill_4, indicating overlapping skills.\nItem5 loads highly on both Skill_1 and Skill_3, suggesting it may be measuring a combination of skills.\n\nInterpretability: The overlapping loadings reduce the model’s interpretability, making it less practical for educational applications.\n\n\n\nFactor Analysis with Two Components\n\n```{python}\n# Performing Factor Analysis with two components to explore if a simpler model might explain the relationships\nn_factors_simpler = 2\nfa_model_simpler = FactorAnalyzer(n_factors=n_factors_simpler, rotation=None)\nfa_model_simpler.fit(item_data_scaled)\n```\n\nFactorAnalyzer(n_factors=2, rotation=None, rotation_kwargs={})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FactorAnalyzeriFittedFactorAnalyzer(n_factors=2, rotation=None, rotation_kwargs={}) \n\n\n```{python}\n# Get the factor loadings for the two-component model\nfactor_loadings_simpler = fa_model_simpler.loadings_\n\n# Create a data frame to visualize the factor loadings for the two-component model\nfactor_loadings_simpler_df = pd.DataFrame(\n    factor_loadings_simpler,\n    index=item_data.columns,\n    columns=[f'Skill_{i+1}' for i in range(n_factors_simpler)]\n)\n\n# Reset the index and rename it to align with the desired table format\nfactor_loadings_simpler_df.reset_index(inplace=True)\nfactor_loadings_simpler_df.rename(columns={'index': 'Item'}, inplace=True)\n\nfactor_loadings_simpler_df\n```\n\n\n\n\nTable 7: Factor Analysis with Two Components\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\n\n\n\n\n0\nitem1\n0.015398\n-0.007536\n\n\n1\nitem2\n-0.099736\n0.298751\n\n\n2\nitem3\n0.809990\n0.040807\n\n\n3\nitem4\n-0.017753\n0.329312\n\n\n4\nitem5\n0.449164\n0.013942\n\n\n5\nitem6\n-0.070497\n1.006000\n\n\n6\nitem7\n0.781571\n0.024412\n\n\n7\nitem8\n0.786056\n0.061425\n\n\n\n\n\n\n\n\n\n\n\nSimplicity vs. Variance Explained: The two-component model is simpler but explains less variance compared to the three-component model.\nItem Loadings:\n\nItem3, Item5, Item7, and Item8 load highly on Skill_1.\nItem2, Item4, and Item6 load on Skill_2.\nItem1 has very low loadings on both factors, suggesting it may not be well-represented in this model.\n\nLoss of Detail: The two-component model may be too simplistic, failing to capture nuances in the data, particularly the unique contribution of Item1.\n\n\n\n\nVisualizations\n\nDiagrams\nI created a couple Mermaid diagrams to gain further insight.\n```{mermaid}\ngraph TB\n    subgraph PCA\n        PCA_S1[Skill_1] --- PCA_I3[Item 3]\n        PCA_S1 --- PCA_I7[Item 7]\n        PCA_S1 --- PCA_I8[Item 8]\n        \n        PCA_S2[Skill_2] --- PCA_I2[Item 2]\n        PCA_S2 --- PCA_I4[Item 4]\n        PCA_S2 --- PCA_I6[Item 6]\n        \n        PCA_S3[Skill_3] --- PCA_I1[Item 1]\n        PCA_S3 --- PCA_I5[Item 5]\n    end\n    \n    subgraph KMeans\n        KM_S1[Skill_1] --- KM_I3[Item 3]\n        KM_S1 --- KM_I5[Item 5]\n        KM_S1 --- KM_I7[Item 7]\n        KM_S1 --- KM_I8[Item 8]\n        \n        KM_S2[Skill_2] --- KM_I2[Item 2]\n        KM_S2 --- KM_I4[Item 4]\n        KM_S2 --- KM_I6[Item 6]\n        \n        KM_S3[Skill_3] --- KM_I1[Item 1]\n    end\n    \n    subgraph Factor_Analysis\n        FA_S1[Skill_1] --- FA_I3[Item 3]\n        FA_S1 --- FA_I5[Item 5]\n        FA_S1 --- FA_I7[Item 7]\n        FA_S1 --- FA_I8[Item 8]\n        \n        FA_S2[Skill_2] --- FA_I2[Item 2]\n        FA_S2 --- FA_I4[Item 4]\n        FA_S2 --- FA_I6[Item 6]\n        \n        FA_S3[Skill_3] --- FA_I1[Item 1]\n    end\n\n    style PCA fill:#f9f,stroke:#333,stroke-width:2px\n    style KMeans fill:#bbf,stroke:#333,stroke-width:2px\n    style Factor_Analysis fill:#bfb,stroke:#333,stroke-width:2px\n```\n\n\n\n\n\n\ngraph TB\n    subgraph PCA\n        PCA_S1[Skill_1] --- PCA_I3[Item 3]\n        PCA_S1 --- PCA_I7[Item 7]\n        PCA_S1 --- PCA_I8[Item 8]\n        \n        PCA_S2[Skill_2] --- PCA_I2[Item 2]\n        PCA_S2 --- PCA_I4[Item 4]\n        PCA_S2 --- PCA_I6[Item 6]\n        \n        PCA_S3[Skill_3] --- PCA_I1[Item 1]\n        PCA_S3 --- PCA_I5[Item 5]\n    end\n    \n    subgraph KMeans\n        KM_S1[Skill_1] --- KM_I3[Item 3]\n        KM_S1 --- KM_I5[Item 5]\n        KM_S1 --- KM_I7[Item 7]\n        KM_S1 --- KM_I8[Item 8]\n        \n        KM_S2[Skill_2] --- KM_I2[Item 2]\n        KM_S2 --- KM_I4[Item 4]\n        KM_S2 --- KM_I6[Item 6]\n        \n        KM_S3[Skill_3] --- KM_I1[Item 1]\n    end\n    \n    subgraph Factor_Analysis\n        FA_S1[Skill_1] --- FA_I3[Item 3]\n        FA_S1 --- FA_I5[Item 5]\n        FA_S1 --- FA_I7[Item 7]\n        FA_S1 --- FA_I8[Item 8]\n        \n        FA_S2[Skill_2] --- FA_I2[Item 2]\n        FA_S2 --- FA_I4[Item 4]\n        FA_S2 --- FA_I6[Item 6]\n        \n        FA_S3[Skill_3] --- FA_I1[Item 1]\n    end\n\n    style PCA fill:#f9f,stroke:#333,stroke-width:2px\n    style KMeans fill:#bbf,stroke:#333,stroke-width:2px\n    style Factor_Analysis fill:#bfb,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 4: Comparison Across the Three Methods\n\n\n\n\n\n\nMethod Comparison (Factor Analysis, K-Means, PCA)\n\nKey Observations:\n\nConsistency Across Methods: Many items (e.g., Item 3 and Item 7) align similarly across Factor Analysis, K-Means, and PCA, reinforcing the robustness of these mappings.\nItem Overlap: The clustering of items (e.g., Items 3, 7, and 8 under Skill_1) consistently suggests a strong latent skill grouping.\nDiscrepancies: While most items map consistently, some differences (e.g., Item 5 under Factor Analysis vs. PCA) suggest subtle differences in how these methods interpret data structures.\nSkill 3 Representation: This skill emerges consistently across methods but captures fewer items, which might indicate a niche or less represented skill.\n\n\nThe visual comparison highlights overlaps and outliers more effectively than numerical tables, making it easier to identify items that contribute ambiguously to multiple skills or are method-dependent.\n```{mermaid}\ngraph TB\n    subgraph Four_Component_Model\n        FC_S1[Skill_1] --- FC_I3[Item 3]\n        FC_S1 --- FC_I7[Item 7]\n        FC_S1 --- FC_I8[Item 8]\n        FC_S1 -.-&gt; FC_I5[Item 5]\n        \n        FC_S2[Skill_2] --- FC_I6[Item 6]\n        FC_S2 -.-&gt; FC_I2[Item 2]\n        FC_S2 -.-&gt; FC_I4[Item 4]\n        \n        FC_S3[Skill_3] --- FC_I1[Item 1]\n        FC_S3 --- FC_I5\n        \n        FC_S4[Skill_4] -.-&gt; FC_I2\n        FC_S4 -.-&gt; FC_I4\n    end\n    \n    subgraph Three_Component_Model\n        TH_S1[Skill_1] --- TH_I3[Item 3]\n        TH_S1 --- TH_I7[Item 7]\n        TH_S1 --- TH_I8[Item 8]\n        TH_S1 --- TH_I5[Item 5]\n        \n        TH_S2[Skill_2] --- TH_I2[Item 2]\n        TH_S2 --- TH_I4[Item 4]\n        TH_S2 --- TH_I6[Item 6]\n        \n        TH_S3[Skill_3] --- TH_I1[Item 1]\n        TH_S3 -.-&gt; TH_I5\n    end\n    \n    subgraph Two_Component_Model\n        TC_S1[Skill_1] --- TC_I3[Item 3]\n        TC_S1 --- TC_I5[Item 5]\n        TC_S1 --- TC_I7[Item 7]\n        TC_S1 --- TC_I8[Item 8]\n        \n        TC_S2[Skill_2] --- TC_I2[Item 2]\n        TC_S2 --- TC_I4[Item 4]\n        TC_S2 --- TC_I6[Item 6]\n        \n        TC_I1[Item 1&lt;br/&gt;Weak Loadings] -..- TC_S1\n        TC_I1 -..- TC_S2\n    end\n\n    style Two_Component_Model fill:#bfb,stroke:#333,stroke-width:2px\n    style Three_Component_Model fill:#bbf,stroke:#333,stroke-width:2px\n    style Four_Component_Model fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n\n\n\n\n\ngraph TB\n    subgraph Four_Component_Model\n        FC_S1[Skill_1] --- FC_I3[Item 3]\n        FC_S1 --- FC_I7[Item 7]\n        FC_S1 --- FC_I8[Item 8]\n        FC_S1 -.-&gt; FC_I5[Item 5]\n        \n        FC_S2[Skill_2] --- FC_I6[Item 6]\n        FC_S2 -.-&gt; FC_I2[Item 2]\n        FC_S2 -.-&gt; FC_I4[Item 4]\n        \n        FC_S3[Skill_3] --- FC_I1[Item 1]\n        FC_S3 --- FC_I5\n        \n        FC_S4[Skill_4] -.-&gt; FC_I2\n        FC_S4 -.-&gt; FC_I4\n    end\n    \n    subgraph Three_Component_Model\n        TH_S1[Skill_1] --- TH_I3[Item 3]\n        TH_S1 --- TH_I7[Item 7]\n        TH_S1 --- TH_I8[Item 8]\n        TH_S1 --- TH_I5[Item 5]\n        \n        TH_S2[Skill_2] --- TH_I2[Item 2]\n        TH_S2 --- TH_I4[Item 4]\n        TH_S2 --- TH_I6[Item 6]\n        \n        TH_S3[Skill_3] --- TH_I1[Item 1]\n        TH_S3 -.-&gt; TH_I5\n    end\n    \n    subgraph Two_Component_Model\n        TC_S1[Skill_1] --- TC_I3[Item 3]\n        TC_S1 --- TC_I5[Item 5]\n        TC_S1 --- TC_I7[Item 7]\n        TC_S1 --- TC_I8[Item 8]\n        \n        TC_S2[Skill_2] --- TC_I2[Item 2]\n        TC_S2 --- TC_I4[Item 4]\n        TC_S2 --- TC_I6[Item 6]\n        \n        TC_I1[Item 1&lt;br/&gt;Weak Loadings] -..- TC_S1\n        TC_I1 -..- TC_S2\n    end\n\n    style Two_Component_Model fill:#bfb,stroke:#333,stroke-width:2px\n    style Three_Component_Model fill:#bbf,stroke:#333,stroke-width:2px\n    style Four_Component_Model fill:#f9f,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 5: Comparison Across the Three Models\n\n\n\n\n\n\n\nModel Comparison (Two-, Three-, and Four-Component Models)\n\nKey Observations:\n\nTwo-Component Model: Simpler but lacks granularity, as evident in fewer distinct mappings and the merging of certain skills.\nThree-Component Model: Balanced in complexity and interpretability, with clear item-skill relationships (e.g., Items 3, 7, and 8 consistently linked to Skill 1).\nFour-Component Model: Overcomplicates relationships with multiple cross-loadings (e.g., Item 5 linked to both Skill 1 and Skill 3), making the model harder to interpret.\nWeak Loadings (Item 1): Visualizing weak loadings in the two-component model underscores its limited ability to represent all test items adequately.\n\n\nThe diagrams provide a clear visual distinction between the interpretability trade-offs of different models. For instance, they highlight how additional components in the four-component model lead to more overlap, supporting the conclusion that the three-component model is optimal.\n\n\nBroader Insights:\n\nSupport for Prior Work: The diagrams reinforce the findings that a three-component model is the most interpretable and aligns well across methods.\nNew Learnings:\n\nItem-Specific Trends: Items like Item 5 show variability across methods and models, suggesting they may assess complex or multiple skills.\nSkill Coverage: Skills identified in PCA seem broader, potentially capturing more nuanced relationships, while K-Means provides a stricter clustering.\nCross-Method Validation: The diagrams visually validate the multi-method approach, showing where methods agree or diverge.\n\n\n\n\n\nHeatmap of Factor Loadings (Three Components)\nUsing a heatmap, I visualized the factor loadings from the three-component Factor Analysis model.\n```{python}\nimport seaborn as sns\n\n# Create a heatmap to visualize item-skill relationships from Factor Analysis\nplt.figure(figsize=(10, 6))\nsns.heatmap(factor_loadings_df, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='black', cbar=True)\n_ = plt.title('Item-Skill Relationships (Factor Analysis with Three Components)')\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 6: Factor Analysis with Three Components\n\n\n\n\n\n\nKey Observations:\n\nDominant Item-Skill Relationships:\n\nItem 1 strongly loads on Skill 3 (0.99), indicating that it is almost exclusively associated with this latent skill.\nItem 3, Item 7, and Item 8 have high loadings on Skill 1 (0.81, 0.78, and 0.78, respectively), showing that they are closely related to this skill.\nItem 6 is strongly associated with Skill 2 (1.00), suggesting it is a clear indicator of this skill.\n\nCross-Skill Contributions:\n\nItem 5 has moderate loadings on both Skill 1 (0.48) and Skill 3 (0.33), indicating that it measures a mix of these skills.\nItem 2 has a moderate loading on Skill 2 (0.30), with negligible contributions to other skills, suggesting it is moderately representative of this skill but not a strong indicator.\n\nWeak Loadings:\n\nItem 4 shows relatively weak loadings across all skills, with the highest on Skill 2 (0.33). This suggests that it may not align well with any single skill or may be ambiguously measuring multiple skills.\nSimilarly, Item 2 and Item 5 exhibit weak or mixed relationships across skills, warranting further investigation.\n\nDistinct Skills:\n\nSkill 1: Clearly defined by Item 3, Item 7, and Item 8.\nSkill 2: Dominated by Item 6, with some contributions from Item 2 and Item 4.\nSkill 3: Clearly represented by Item 1, with partial contributions from Item 5.\n\n\n\n\nInsights:\n\nItem-Skill Assignment: The heatmap visually confirms the appropriateness of assigning items to the skills based on their dominant factor loadings.\nComplex or Ambiguous Items: Items like Item 5 and Item 4 exhibit weaker or mixed relationships, suggesting potential challenges in their interpretation or measurement of a specific skill.\nSkill Coverage: Each skill appears to have at least one strongly associated item, ensuring that all skills are represented in the model.\n\n\n\n\nBar Charts for Individual Items\nI generated bar charts to illustrate the factor loadings of each item across the three skills.\n```{python}\n# Create bar charts for each item to show its relationship across skills\nnum_items = len(factor_loadings_df.index)\nfig, axes = plt.subplots(num_items, 1, figsize=(9, num_items * 2))\n\nfor i, item in enumerate(factor_loadings_df.index):\n    axes[i].bar(factor_loadings_df.columns, factor_loadings_df.loc[item], color='skyblue')\n    _ = axes[i].set_title(f'Relationship of {item} with Skills')\n    _ = axes[i].set_ylabel('Loading Value')\n    _ = axes[i].set_ylim(-1, 1)\n\nplt.tight_layout()\n\nplt.show()\n```\n\n\n\n\n\n\n\n\nFigure 7: Bar Charts for Individual Items\n\n\n\n\n\n\nKey Insights:\n\nDominant Item-Skill Relationships:\n\nItem 1: Almost exclusively associated with Skill 3, with a very high loading value (~0.99). It does not meaningfully load on Skill 1 or Skill 2.\nItem 3, Item 7, and Item 8: Strongly associated with Skill 1, with high positive loadings (~0.81 and ~0.78). These items clearly represent this latent skill.\nItem 6: Solely aligned with Skill 2 (loading ~1.00), making it the clearest representative of this skill.\n\nMixed and Moderate Relationships:\n\nItem 5: Shows moderate loadings on both Skill 1 (~0.48) and Skill 3 (~0.33), indicating that it may measure a combination of these skills.\nItem 2: Moderately aligned with Skill 2 (~0.30) but has negligible loadings on the other skills, making it a less prominent representative of any single skill.\n\nAmbiguous or Weak Relationships:\n\nItem 4: Has low to moderate loadings across the board, with the highest (~0.33) on Skill 2. This indicates that the item may be ambiguous or weakly related to the latent skills in this model.\nItem 2: Although moderately associated with Skill 2, its low loadings suggest it does not strongly differentiate itself in measuring this skill.\n\nDistinct Skills:\n\nSkill 1: Clearly defined by Item 3, Item 7, and Item 8.\nSkill 2: Primarily represented by Item 6, with minor contributions from Item 2 and Item 4.\nSkill 3: Dominated by Item 1, with partial contributions from Item 5.\n\n\n\n\nFurther Insight:\n\nSupport for Factor Analysis Findings:\n\nThe charts confirm that the three-component model successfully captures distinct latent skills, with most items showing strong associations with a single skill.\nThe visualization highlights items that load cleanly on one skill (e.g., Item 6 for Skill 2, Item 1 for Skill 3).\n\nAmbiguous Items:\n\nItems like Item 4 and Item 5 demonstrate weaker or mixed relationships, indicating potential issues with their design or alignment with specific skills.\nThese items may require revision or could indicate the need for further exploration of an additional component.\n\nStrength of Representation:\n\nCertain skills (e.g., Skill 1 and Skill 3) have multiple items with high loadings, providing strong representation.\nSkill 2 is highly dependent on a single dominant item (Item 6), which could make it more vulnerable to measurement error.\n\n\n\n\n\n\nCreating the Final Q-Matrix\nBased on the consistency of results across methods, I developed a final Q-matrix that maps each item to its primary associated skill based on the three-factor model. Table 8 presents the final Q-matrix, which shows a clear and interpretable mapping of items to skills.\n```{python}\n# Creating the final Q-matrix based on the visualization and analysis findings\n# Assigning each item to the skill with the highest loading from the Factor Analysis with three components\nfinal_q_matrix = factor_loadings_df.idxmax(axis=1)\n\n# Create a data frame to visualize the final Q-matrix, showing the mapping between items and skills\nfinal_q_matrix_df = pd.DataFrame({'Item': item_data.columns, 'Mapped_Skill': final_q_matrix.values})\n\n# Set a threshold to determine the significant loading\nthreshold = 0.2\n\n# Create a binary Q-matrix based on the factor loadings and the threshold\nq_matrix_binary = (np.abs(factor_loadings_df) &gt; threshold).astype(int)\n\n# Rename index and columns for better readability in the Q-matrix\nq_matrix_binary.index.name = 'Item'\nq_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n\n# Reset the index to present it as a table\nq_matrix_binary_df = q_matrix_binary.reset_index()\n\n# Display the final Q-matrix\nq_matrix_binary_df\n```\n\n\n\n\nTable 8: Final Q-Matrix\n\n\n\n\n\n\n\n\n\n\nItem\nSkill_1\nSkill_2\nSkill_3\n\n\n\n\n0\nitem1\n0\n0\n1\n\n\n1\nitem2\n0\n1\n0\n\n\n2\nitem3\n1\n0\n0\n\n\n3\nitem4\n0\n1\n0\n\n\n4\nitem5\n1\n0\n1\n\n\n5\nitem6\n0\n1\n0\n\n\n6\nitem7\n1\n0\n0\n\n\n7\nitem8\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\nI also developed another diagram to support the final Q-Matrix.\n```{mermaid}\ngraph LR\n    subgraph Final_Q_Matrix_Mappings\n        S1[Skill_1] --- I3[Item 3]\n        S1 --- I5[Item 5]\n        S1 --- I7[Item 7]\n        S1 --- I8[Item 8]\n        \n        S2[Skill_2] --- I2[Item 2]\n        S2 --- I4[Item 4]\n        S2 --- I6[Item 6]\n        \n        S3[Skill_3] --- I1[Item 1]\n    end\n    \n    style Final_Q_Matrix_Mappings fill:#bfb,stroke:#333,stroke-width:2px\n```\n\n\n\n\n\n\ngraph LR\n    subgraph Final_Q_Matrix_Mappings\n        S1[Skill_1] --- I3[Item 3]\n        S1 --- I5[Item 5]\n        S1 --- I7[Item 7]\n        S1 --- I8[Item 8]\n        \n        S2[Skill_2] --- I2[Item 2]\n        S2 --- I4[Item 4]\n        S2 --- I6[Item 6]\n        \n        S3[Skill_3] --- I1[Item 1]\n    end\n    \n    style Final_Q_Matrix_Mappings fill:#bfb,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 8: Final Q-Matirx\n\n\n\n\n\n\n\nKey Strengths of the Final Q-Matrix and Diagram\n\nClear Mapping:\n\nEach item is assigned to the skill with the highest loading, ensuring that the relationships are driven by the statistical analysis.\nThe diagram visually highlights these relationships, making it easy to understand and communicate the structure.\n\nSkill Representation:\n\nSkill 1: Represented by four items (Item 3, Item 5, Item 7, and Item 8), providing robust coverage and reliability for assessing this skill.\nSkill 2: Supported by three items (Item 2, Item 4, and Item 6), with Item 6 being the strongest indicator.\nSkill 3: Represented by Item 1, a highly specific item exclusively aligned with this skill.\n\nAlignment with Analyses:\n\nThe Q-matrix directly reflects the findings from the Factor Analysis heatmap and bar charts, ensuring consistency and validation of the mappings.\n\nBalanced Complexity:\n\nBy selecting three components, the Q-matrix strikes a balance between interpretability and detail, avoiding the over-complexity of a four-component model while capturing nuances missed in a two-component model.\n\n\n\n\nObservations and Recommendations\n\nStrength of Item Representation:\n\nSkill 3 relies on a single item (Item 1). While Item 1 has a strong loading, additional items (e.g., Item 5) may be needed to ensure the skill is robustly assessed.\nSkill 2 shows moderate contributions from Item 2 and Item 4, which might require review to ensure their alignment with this skill.\n\nAmbiguous Items:\n\nItem 5 has a mixed loading (moderate on Skill 1 and Skill 3), but its assignment to Skill 1 aligns well with the overall structure.\nItem 4 has weaker loadings but is still included under Skill 2, reflecting its statistical alignment while acknowledging its relative ambiguity.\n\n\n\n\nModel Evaluation Metrics\n\nCalculating Proportion of Variance Explained (\\(R^2\\))\nFor Factor Analysis:\n\n```{python}\n# Compute the communalities\ncommunalities = fa_model.get_communalities()\n\n# Total variance explained\ntotal_variance_explained = np.sum(communalities)\n\n# Total variance (number of variables)\ntotal_variance = item_data_scaled.shape[1]\n\n# Proportion of variance explained\nr_squared_fa = total_variance_explained / total_variance\n\nprint(f\"Factor Analysis R^2: {r_squared_fa:.2f}\")\n```\n\nFactor Analysis R^2: 0.56\n\n\nInterpretation:\n\nThe \\(R^2\\) value of 0.56 indicates that the three-factor model explains 56% of the total variance in the data.\nImplications:\n\nA proportion of variance explained greater than 50% is generally considered acceptable in exploratory Factor Analysis, especially with psychological or educational data where constructs are often complex.\nHowever, it also suggests that 44% of the variance is not explained by the model, which may be due to measurement error, unique variance of items, or additional latent factors not captured by the model.\n\n\nFor PCA:\n\n```{python}\n# Calculate cumulative variance explained\ncumulative_variance = np.cumsum(pca_model.explained_variance_ratio_)\nprint(f\"PCA cumulative variance explained by first 3 components: {cumulative_variance[2]:.2f}\")\n```\n\nPCA cumulative variance explained by first 3 components: 0.66\n\n\nInterpretation:\n\nThe first three principal components explain 66% of the total variance in the data.\nImplications:\n\nThis indicates a slightly better variance explanation than the Factor Analysis model.\nPCA aims to capture the maximum variance with the fewest components, so a higher cumulative variance explained is desirable.\nHowever, PCA components may not be as interpretable as factors from Factor Analysis, since PCA components are linear combinations that maximize variance without considering underlying latent constructs.\n\n\nComparison:\n\nThe PCA model explains more variance (66%) compared to the Factor Analysis model (56%).\nThis difference may be due to the methodological differences between PCA and Factor Analysis:\n\nPCA focuses on capturing variance and is sensitive to the scale of the data.\nFactor Analysis models the underlying latent constructs and accounts for measurement error.\n\n\nConsiderations:\n\nAdequacy of Variance Explained:\n\nIn social sciences, cumulative variance explained between 50% and 75% is generally acceptable.\nBoth models fall within this range, but there is room for improvement.\n\nUnexplained Variance:\n\nThe unexplained variance suggests that additional factors or components might exist, or that some items do not fit well within the identified latent skills.\n\n\n\n\nCalculating Cohen’s Kappa Coefficient\nI also examined the consistency of item assignments across methods using Cohen’s kappa coefficient (Cohen 1960).\n\n```{python}\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.optimize import linear_sum_assignment\n\n# Map skill labels to numeric codes for Factor Analysis\nfa_skill_codes = final_q_matrix_df['Mapped_Skill'].map({'Skill_1': 0, 'Skill_2': 1, 'Skill_3': 2}).values\n\n# K-Means cluster labels\nkmeans_labels = kmeans.labels_\n\n# Compute confusion matrix\nconfusion = confusion_matrix(fa_skill_codes, kmeans_labels)\nprint(\"Confusion Matrix:\")\nprint(confusion)\n\n# Align clusters with skills using the Hungarian algorithm\nrow_ind, col_ind = linear_sum_assignment(-confusion)\nmapping = dict(zip(col_ind, row_ind))\n\n# Map K-Means labels to Factor Analysis skill codes\nkmeans_labels_mapped = np.array([mapping[label] for label in kmeans_labels])\n\n# Compute Cohen's kappa\nfrom sklearn.metrics import cohen_kappa_score\n\nkappa = cohen_kappa_score(fa_skill_codes, kmeans_labels_mapped)\nprint(f\"Cohen's kappa after alignment: {kappa:.2f}\")\n```\n\nConfusion Matrix:\n[[4 0 0]\n [0 3 0]\n [0 0 1]]\nCohen's kappa after alignment: 1.00\n\n\nInterpretation:\n\nConfusion Matrix:\n\nThe confusion matrix shows perfect agreement between the methods after alignment:\n\nAll items assigned to Skill 1 in Factor Analysis are also assigned to the corresponding cluster in K-Means.\nThe same applies to Skills 2 and 3.\n\n\nCohen’s Kappa Value:\n\nA Kappa value of 1.00 indicates perfect agreement between the two methods after alignment.\n\nImplications:\n\nThis high level of agreement suggests that both methods are consistently identifying the same underlying item-skill structures.\nIt provides strong validation for the robustness of your item-skill mappings.\n\n\nConsiderations:\n\nAlignment Step:\n\nThe necessity of aligning clusters to skills underscores that cluster labels are arbitrary.\nIt’s important to perform this alignment to make meaningful comparisons.\n\nCohen’s Kappa Interpretation:\n\nKappa values range from -1 to 1, where:\n\n&lt; 0: Less than chance agreement.\n0–0.20: Slight agreement.\n0.21–0.40: Fair agreement.\n0.41–0.60: Moderate agreement.\n0.61–0.80: Substantial agreement.\n0.81–1.00: Almost perfect agreement.\n\nA value of 1.00 confirms that the two methods are in complete concordance post-alignment.\n\n\n\n\n\nOverall Evaluation\nStrengths:\n\nConverging Evidence:\n\nThe high Cohen’s Kappa value indicates that different analytical methods converge on the same item-skill mappings, enhancing confidence in the results.\n\nVariance Explained:\n\nBoth Factor Analysis and PCA explain a substantial portion of the variance, supporting the validity of the three-component model.\n\nMethodological Rigor:\n\nMy approach of using multiple methods and comparing them through quantitative metrics strengthens the robustness of the findings.\n\n\nLimitations:\n\nVariance Not Explained:\n\nApproximately 34% to 44% of the variance remains unexplained, which could be due to:\n\nMeasurement error.\nAdditional latent skills not captured by the model.\nUnique variances of items.\n\n\nAssumptions of Methods:\n\nFactor Analysis and PCA assumptions may not be fully met with binary data, which could affect the variance explained.\n\n\n\nVerifying Item-Skill Mappings\n```{python}\nfinal_q_matrix_df\n```\n\n\n\n\nTable 9: Factor Analysis Mappings\n\n\n\n\n\n\n\n\n\n\nItem\nMapped_Skill\n\n\n\n\n0\nitem1\nSkill_3\n\n\n1\nitem2\nSkill_2\n\n\n2\nitem3\nSkill_1\n\n\n3\nitem4\nSkill_2\n\n\n4\nitem5\nSkill_1\n\n\n5\nitem6\nSkill_2\n\n\n6\nitem7\nSkill_1\n\n\n7\nitem8\nSkill_1\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nItem Assignments: Each item is assigned to the skill with which it has the highest factor loading from the final Q-matrix.\nSkill Representation:\n\nSkill_1: Items 3, 5, 7, 8\nSkill_2: Items 2, 4, 6\nSkill_3: Item 1\n\n\nSignificance:\n\nConsistent Mapping: The assignments reflect the conclusions drawn from my Factor Analysis.\nFoundation for Comparison: These mappings serve as the reference point for comparing with the K-Means Clustering results.\n\n```{python}\nkmeans_q_matrix_df\n```\n\n\n\n\nTable 10: K-Means Clustering Mappings (before alignment)\n\n\n\n\n\n\n\n\n\n\nItem\nMapped_Skill\n\n\n\n\n0\nitem1\nSkill_3\n\n\n1\nitem2\nSkill_2\n\n\n2\nitem3\nSkill_1\n\n\n3\nitem4\nSkill_2\n\n\n4\nitem5\nSkill_1\n\n\n5\nitem6\nSkill_2\n\n\n6\nitem7\nSkill_1\n\n\n7\nitem8\nSkill_1\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nCluster Assignments: Items are assigned to clusters labeled as Skill_1, Skill_2, or Skill_3, based on the K-Means Clustering algorithm.\nArbitrary Labels: The cluster labels (e.g., Skill_1, Skill_2) are assigned by the algorithm and do not necessarily correspond to the skills identified in Factor Analysis.\n\nSignificance:\n\nInitial Comparison: At first glance, the mappings appear similar to the Factor Analysis mappings, but due to arbitrary labeling, a direct comparison isn’t meaningful yet.\nNeed for Alignment: To accurately compare the item-skill assignments, cluster labels must be aligned with the skills from Factor Analysis.\n\n\n```{python}\n# Map clusters to skills after alignment\nkmeans_skill_names_aligned = ['Skill_' + str(mapping[label] + 1) for label in kmeans_labels]\nkmeans_q_matrix_df_aligned = kmeans_q_matrix_df.copy()\nkmeans_q_matrix_df_aligned['Mapped_Skill'] = kmeans_skill_names_aligned\n```\n\nProcess:\n\nAlignment Using the Hungarian Algorithm:\n\nSince cluster labels are arbitrary, I used the Hungarian algorithm (also known as the linear sum assignment method) to find the optimal one-to-one mapping between clusters and skills.\nThis algorithm minimizes the total disagreement between the two sets of labels.\n\nMapping Clusters to Skills:\n\nI created a mapping dictionary (mapping) that aligns each cluster label with the corresponding skill from Factor Analysis.\nThis ensures that clusters are correctly interpreted in the context of the identified skills.\n\n\n```{python}\nkmeans_q_matrix_df_aligned\n```\n\n\n\n\nTable 11: K-Means Clustering Mappings (after alignment)\n\n\n\n\n\n\n\n\n\n\nItem\nMapped_Skill\n\n\n\n\n0\nitem1\nSkill_3\n\n\n1\nitem2\nSkill_2\n\n\n2\nitem3\nSkill_1\n\n\n3\nitem4\nSkill_2\n\n\n4\nitem5\nSkill_1\n\n\n5\nitem6\nSkill_2\n\n\n6\nitem7\nSkill_1\n\n\n7\nitem8\nSkill_1\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nAligned Assignments: After alignment, the cluster labels now correspond to the same skills as in the Factor Analysis mappings.\nPerfect Agreement: The item-skill assignments from K-Means Clustering match exactly with those from Factor Analysis.\n\nSignificance:\n\nValidation of Consistency: The perfect match indicates strong agreement between the two methods.\nRobustness of Findings: The consistency across methods reinforces the reliability of the item-skill mappings."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#discussion",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#discussion",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Discussion",
    "text": "Discussion\n\nOverview of Model Comparison and Selection\n\nModel Complexity and Interpretability\nAfter comparing models with two, three, and four components, the three-component Factor Analysis model emerged as the most suitable representation of the latent skills in the dataset.\nTwo-Component Model\n\nSimplicity: The two-component model is the simplest, reducing the latent skills to two factors.\nInterpretability:\n\nSome items showed weak loadings or ambiguous associations.\nItem 1, for example, had very low loadings on both factors, suggesting it doesn’t fit well within this model.\n\nImplications:\n\nThe model may be too simplistic, failing to capture important nuances in the data.\nIt potentially merges distinct skills into broader categories, which could obscure meaningful distinctions.\n\n\nThree-Component Model\n\nBalance: Offers a middle ground between simplicity and complexity.\nInterpretability:\n\nProvides clear and distinct latent skills.\nMost items load strongly on a single factor, enhancing interpretability.\n\nFindings:\n\nThe model captures the nuances in the data without unnecessary complexity.\nItem 5 shows moderate loadings on two skills, indicating split influences but remains interpretable.\n\n\nFour-Component Model\n\nComplexity: Introduces additional complexity with a fourth factor.\nInterpretability:\n\nOverlapping loadings make the model harder to interpret.\nSome items load significantly on multiple factors, causing ambiguity.\n\nImplications:\n\nThe added complexity doesn’t substantially increase explained variance.\nMay overfit the data, capturing noise rather than meaningful structure.\n\n\nTrade-Offs:\n\nThe two-component model may underfit, missing key distinctions between skills.\nThe four-component model may overfit, adding unnecessary complexity without practical benefits.\n\nOptimal Complexity:\n\nThe three-component model strikes a balance, capturing essential structures while maintaining interpretability.\n\n\n\nVariance Explained and Model Fit\nFactor Analysis Variance Explained\n\nTwo-Component Model:\n\nLower proportion of variance explained (less than 56%).\nIndicates insufficient capture of the data’s variability.\n\nThree-Component Model:\n\nExplains approximately 56% of the total variance.\nRepresents a reasonable fit for exploratory purposes.\n\nFour-Component Model:\n\nSlight increase in variance explained.\nNot significant enough to justify added complexity.\n\n\nPCA Variance Explained\n\nThree-Component Model:\n\nCumulative variance explained is 66%.\nIndicates a substantial capture of data variability.\n\nComparison:\n\nPCA generally explains more variance than Factor Analysis in your findings.\nHowever, PCA components may not be as interpretable in terms of latent skills.\n\n\nThresholds: In social sciences, explaining around 50-75% variance is acceptable.\nDiminishing Returns: The variance explained by adding a fourth component doesn’t justify the increased complexity.\nModel Fit: The three-component model provides an acceptable fit with reasonable simplicity.\n\n\nConsistency Across Methods\nAgreement Among Methods\n\nThree-Component Model:\n\nHigh consistency in item-skill mappings across Factor Analysis, K-Means Clustering, and PCA.\nCohen’s Kappa Coefficient of 1.00 after alignment indicates perfect agreement.\n\nTwo- and Four-Component Models:\n\nLess consistent across methods.\nAmbiguities in item assignments due to overlapping loadings.\n\n\nReinforcement:\n\nDifferent methods converging on the same solution supports the robustness of the three-component model.\n\nPractical Implications:\n\nA consistent model is more reliable for educational applications, such as test design and interpretation.\n\n\n\nModel Evaluation Metrics\nProportion of Variance Explained (\\(R^2\\))\n\nFactor Analysis:\n\nThree-Component Model (\\(R^2\\)): Approximately 0.56.\nIndicates that 56% of the variance is captured by the model.\n\nPCA:\n\nThree-Component Model Cumulative Variance: 66%.\nSuggests a better variance capture, but PCA components may be less interpretable.\n\n\nCohen’s Kappa Coefficient\n\nValue: 1.00 after alignment.\nInterpretation:\n\nIndicates perfect agreement between item-skill mappings from Factor Analysis and K-Means Clustering.\n\nSignificance:\n\nValidates the consistency and reliability of the three-component model.\n\n\nBalance of Metrics:\n\nThe three-component model provides a good balance between variance explained and interpretability.\n\nLimitations:\n\nAcknowledge that a portion of variance remains unexplained.\nSuggests potential areas for further investigation or alternative modeling approaches.\n\n\n\nFinal Model Selection\nReasons for Selecting the Three-Component Model\n\nOptimal Balance:\n\nCaptures essential structures without overcomplicating the model.\n\nHigh Interpretability:\n\nClear item-skill relationships make it practical for educational use.\n\nStrong Validation:\n\nConsistent findings across multiple methods reinforce its selection.\n\nModel Performance:\n\nSatisfactory variance explained and perfect agreement in item assignments.\n\n\nImplications for the Q-Matrix\n\nRobust Mapping:\n\nThe final Q-matrix derived from the three-component model provides a reliable item-skill mapping.\n\nEducational Utility:\n\nEnhances interpretability of test results.\nAids in identifying areas for instructional focus and intervention.\n\n\n\n\n\nJustification for the Final Q-Matrix\n\nDerivation from Multiple Methods\nIntegration of Analytical Findings\n\nFactor Analysis: The Final Q-Matrix is primarily based on the results of the three-component Factor Analysis, where each item is assigned to the skill with the highest factor loading.\nK-Means Clustering and PCA: The item-skill mappings derived from these methods align closely with the Factor Analysis results, reinforcing the assignments in the Final Q-Matrix.\n\nConsistency in Item Groupings: Items that cluster together in K-Means and load on the same principal components in PCA correspond to the same skills identified in Factor Analysis.\n\nConverging Evidence: The consistent findings across multiple methods provide strong evidence that the item-skill assignments in the Final Q-Matrix accurately reflect the underlying knowledge structure.\nRobustness: Using different analytical techniques reduces the likelihood that the results are artifacts of a specific method, increasing confidence in the Q-Matrix.\n\n\n\nSupport from Model Evaluation Metrics\nVariance Explained\n\nFactor Analysis (\\(R^2\\)): The three-component model explains approximately 56% of the total variance.\nPCA Variance: The first three principal components account for 66% of the variance.\n\nCohen’s Kappa Coefficient\n\nValue of 1.00: Indicates perfect agreement between the item-skill mappings from Factor Analysis and K-Means Clustering after alignment.\nAdequate Model Fit: The proportion of variance explained suggests that the model captures a substantial amount of the data’s variability, which is acceptable in exploratory analyses.\nValidation of Mappings: The perfect Cohen’s Kappa score confirms that different methods agree on the item-skill assignments, supporting the validity of the Final Q-Matrix.\n\n\n\nBalance of Complexity and Interpretability\nModel Selection\n\nThree-Component Model: Chosen for providing the best balance between capturing sufficient detail and maintaining simplicity.\nAvoiding Overfitting: The four-component model introduced complexity without significant gains in variance explained, making it less interpretable.\nPreventing Oversimplification: The two-component model failed to capture important nuances, with some items not fitting well.\nPractical Interpretability: The three-component model allows for clear and distinct item-skill relationships, making the Q-Matrix practical for educational purposes.\n\n\n\nConsistency Across Analytical Methods\nAlignment of Results\n\nFactor Analysis, K-Means Clustering, and PCA all indicate similar item-skill groupings.\nMermaid Diagrams and Heatmaps: Provide visual confirmation of the consistent item-skill relationships across methods.\nCross-Method Validation: Consistency across methods strengthens the argument that the Final Q-Matrix accurately represents the latent skills.\nReinforcement of Findings: Visual tools help illustrate the robustness of the mappings, making the justification more compelling.\n\n\n\nEducational Relevance and Practicality\nActionable Insights: The Q-Matrix provides educators with clear information about which items assess which skills, facilitating targeted instruction and remediation.\nTest Design Improvement: Understanding item-skill relationships helps in refining assessments to better measure the intended skills.\nBy understanding the relationships between items and skills, test designers can create assessments that more effectively target specific skills, ensuring a balanced coverage of the identified latent skills. The item-skill mappings can also help identify potentially redundant or less informative items, allowing for more efficient and focused assessments.\nMoreover, educators can leverage the findings to diagnose student strengths and weaknesses at the skill level. The identification of specific skills associated with each item enables targeted remediation or enrichment activities, focusing on the areas where students may need additional support. This information can also guide the development of instructional materials and resources, ensuring that students have ample opportunities to practice and master the identified skills.\n\n\n\nLimitations and Future Work\nDespite the insights provided by this study, there are limitations to consider.\nAcknowledging Split Influences\n\nItem 5: Exhibits moderate loadings on both Skill 1 and Skill 3.\n\nJustification:\n\nAssignment Based on Dominant Loading: Despite the split influence, Item 5 is assigned to Skill 1 due to its higher loading, aligning with the overall structure.\nConsideration for Revision: Recognizing the split influence allows for potential item revision to enhance its alignment with a single skill.\n\nEnsuring Skill Representation\n\nSkill 3: Currently represented by a single item (Item 1).\n\nJustification:\n\nRecognition of Limitations: Acknowledging that Skill 3 relies on a single item highlights an area for potential expansion in future assessments.\nMaintaining Integrity: Despite the limited representation, the strong loading of Item 1 on Skill 3 justifies its inclusion in the Q-Matrix.\nBinary Data Consideration:\n\nThe use of Factor Analysis and PCA on binary data may not fully meet the assumptions of these methods. Future research could explore the application of Item Response Theory (IRT) models specifically designed for analyzing binary response data (Van der Linden and Hambleton 2015).\n\nSample Size and Generalizability:\n\nThe small sample size of eight items limits the generalizability of the findings. Replicating the study with a larger set of items and a more diverse student population would help validate the identified skill structure and its applicability to different educational contexts."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_3/index.html#conclusion",
    "href": "educ_6191_001/creative_assignments/assignment_3/index.html#conclusion",
    "title": "Knowledge Structure Mapping: a Comprehensive Report",
    "section": "Conclusion",
    "text": "Conclusion\nThis study significantly contributes to the field of educational assessment and learning analytics by demonstrating the effectiveness of a comprehensive, multi-method approach to uncovering latent skill structures in an eight-item test dataset. By leveraging the complementary strengths of Factor Analysis, K-Means Clustering, and Principal Component Analysis (PCA), I identified a robust and interpretable three-skill model that best represents the underlying knowledge structure.\nKey findings of this study include:\n\nIdentification of Three Distinct Latent Skills: These skills capture the essential relationships among the test items, providing a clearer understanding of the knowledge assessed.\nDevelopment of a Final Q-Matrix: The Q-matrix offers a precise and empirically derived mapping of items to skills, consistent across multiple analytical methods, enhancing the reliability of skill assessment.\nValidation of Item-Skill Relationships: Cross-validation using multiple methods supports the interpretability of the identified skill structure, confirming the robustness of the findings.\n\nThe practical significance of this work lies in its potential to inform and enhance educational assessment and instructional practices. By providing a more precise understanding of the skills assessed by individual test items, this study enables educators and test designers to:\n\nDevelop Targeted Assessments: Create more focused and efficient assessments that effectively measure specific skills.\nIdentify Student Needs: Pinpoint areas where students may require additional support or remediation based on their performance on skill-related items.\nDesign Aligned Instructional Interventions: Develop instructional resources that align with the identified skill structure, promoting more personalized and adaptive learning experiences.\n\nMoreover, the multi-method approach presented in this study serves as a valuable template for future research in educational data mining and learning analytics. Researchers can build upon this methodology to investigate knowledge structures underlying different types of assessments, learning materials, and educational contexts.\nFuture research should address this study’s limitations and explore new avenues for extending its findings. Specific opportunities include:\n\nApplying Item Response Theory (IRT) Models: Utilize IRT models, which are specifically designed to analyze binary response data, to validate and refine the identified skill structure.\nExpanding the Dataset: Replicate the study with larger and more diverse datasets, including assessments with a greater number of items and student populations from various educational backgrounds, to enhance generalizability.\nExploring Generalizability Across Contexts: Investigate the applicability of the identified skill structure across different domains, grade levels, and assessment formats.\nIntegrating with Adaptive Learning Systems: Explore the integration of the derived Q-matrix with adaptive learning systems and intelligent tutoring platforms to enable real-time, skill-based feedback and personalized learning paths.\n\nBy addressing these challenges and opportunities, future research can further advance our understanding of knowledge structure mapping and its applications in educational settings, ultimately contributing to the development of more effective and equitable learning experiences for all students.\n\nSubmission Guidelines\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/austinbao0419/CSCL23_feedback_detectors.html",
    "href": "educ_6191_001/creative_assignments/assignment_4/austinbao0419/CSCL23_feedback_detectors.html",
    "title": "Penn GSE",
    "section": "",
    "text": "# CSCL 2023 Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics\n!pip install tensorflow\n!pip install tensorflow_hub\n!pip install tensorflow-addons\n\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: absl-py&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py&gt;=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: libclang&gt;=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes&lt;0.5.0,&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\nRequirement already satisfied: tensorboard&lt;2.18,&gt;=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\nRequirement already satisfied: keras&gt;=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.2.0-&gt;tensorflow) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.2.0-&gt;tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.2.0-&gt;tensorflow) (0.13.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2024.8.30)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras&gt;=3.2.0-&gt;tensorflow) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras&gt;=3.2.0-&gt;tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.2.0-&gt;tensorflow) (0.1.2)\nRequirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\nRequirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.26.4)\nRequirement already satisfied: protobuf&gt;=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (4.25.5)\nRequirement already satisfied: tf-keras&gt;=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (2.17.0)\nRequirement already satisfied: tensorflow&lt;2.18,&gt;=2.17 in /usr/local/lib/python3.10/dist-packages (from tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (2.17.1)\nRequirement already satisfied: absl-py&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (1.4.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.6.0)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.2.0)\nRequirement already satisfied: h5py&gt;=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.12.1)\nRequirement already satisfied: libclang&gt;=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (18.1.1)\nRequirement already satisfied: ml-dtypes&lt;0.5.0,&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.4.1)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (24.2)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (75.1.0)\nRequirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (1.16.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (2.5.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (4.12.2)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (1.17.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (1.68.1)\nRequirement already satisfied: tensorboard&lt;2.18,&gt;=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (2.17.1)\nRequirement already satisfied: keras&gt;=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.37.1)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse&gt;=1.6.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.2.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.2.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.2.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.13.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (2024.8.30)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.7)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.1.3)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.18,&gt;=2.17-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.0.2)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras&gt;=3.2.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras&gt;=3.2.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.2.0-&gt;tensorflow&lt;2.18,&gt;=2.17-&gt;tf-keras&gt;=2.14.1-&gt;tensorflow_hub) (0.1.2)\nCollecting tensorflow-addons\n  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\nCollecting typeguard&lt;3.0.0,&gt;=2.7 (from tensorflow-addons)\n  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\nDownloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 611.8/611.8 kB 11.8 MB/s eta 0:00:00\nDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nInstalling collected packages: typeguard, tensorflow-addons\n  Attempting uninstall: typeguard\n    Found existing installation: typeguard 4.4.1\n    Uninstalling typeguard-4.4.1:\n      Successfully uninstalled typeguard-4.4.1\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ninflect 7.4.0 requires typeguard&gt;=4.0.1, but you have typeguard 2.13.3 which is incompatible.\nSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n!pip uninstall tensorflow keras -y\n!pip install tensorflow\n\nFound existing installation: tensorflow 2.17.1\nUninstalling tensorflow-2.17.1:\n  Successfully uninstalled tensorflow-2.17.1\nFound existing installation: keras 3.5.0\nUninstalling keras-3.5.0:\n  Successfully uninstalled keras-3.5.0\nCollecting tensorflow\n  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: absl-py&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang&gt;=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;6.0.0dev,&gt;=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\nCollecting tensorboard&lt;2.19,&gt;=2.18 (from tensorflow)\n  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting keras&gt;=3.5.0 (from tensorflow)\n  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: numpy&lt;2.1.0,&gt;=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py&gt;=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: ml-dtypes&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (0.13.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2024.8.30)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (0.1.2)\nDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 615.3/615.3 MB 3.3 MB/s eta 0:00:00\nDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 53.2 MB/s eta 0:00:00\nDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 98.6 MB/s eta 0:00:00\nInstalling collected packages: tensorboard, keras, tensorflow\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.17.1\n    Uninstalling tensorboard-2.17.1:\n      Successfully uninstalled tensorboard-2.17.1\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntf-keras 2.17.0 requires tensorflow&lt;2.18,&gt;=2.17, but you have tensorflow 2.18.0 which is incompatible.\nSuccessfully installed keras-3.7.0 tensorboard-2.18.0 tensorflow-2.18.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport random\nimport nltk\nimport os\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix, accuracy_score, f1_score, cohen_kappa_score\nfrom sklearn.model_selection import GroupKFold, train_test_split\nfrom sklearn import tree, metrics\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\n\nrandom.seed(20)\n# split data for student level cross validation: one student is nested within one fold\ngroup_dict = dict()\ngroups = np.array([])\n\ndf = pd.read_csv('Annotations_final.csv')\n\nfor index, row in df.iterrows():\n    s_id = row['created_by']\n    if s_id not in group_dict:\n        group_dict[s_id] = index\n    groups = np.append(groups, group_dict[s_id])\n\n# Set up the splitter with 5 splits\ngkf = GroupKFold(n_splits = 5)"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/austinbao0419/CSCL23_feedback_detectors.html#bag-of-words",
    "href": "educ_6191_001/creative_assignments/assignment_4/austinbao0419/CSCL23_feedback_detectors.html#bag-of-words",
    "title": "Penn GSE",
    "section": "bag of words",
    "text": "bag of words\n\n# bag of words + neural nets\n# import data\n# stemming\nporter = PorterStemmer() # stemming recovers root words from plurals etc\nstemmed_texts = []\n\nX_text = df['annotation_text']\n\nfor answer in X_text:\n    answer = ' '.join(porter.stem(word) for word in answer.split(' '))\n    stemmed_texts.append(answer)\n\nX, y = np.array(stemmed_texts), df['relating_to_self'] # CHANGE y HERE\n\n#vect = CountVectorizer(ngram_range=(1,1), max_features=1000, stop_words=\"english\") #only unigram 313 features\nvect = CountVectorizer(ngram_range=(2,2), max_features=1000, stop_words=\"english\") #only bigram 728 features\n\nX = vect.fit_transform(X).toarray()\n\n\n# set up storage arrays for each round of validation\nroc_auc_scores = np.array([])\naccuracy_scores = np.array([])\n\n# split, train, test and store performance metrics\nfor train_index, test_index in gkf.split(X, y, groups=groups):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n\n    model = Sequential()\n    model.add(Dense(12, input_shape=(738,), activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics = ['acc']\n             )\n\n    num_epochs = 30\n    batch_size = 10\n\n    model.fit(\n        X_train,\n        y_train,\n        epochs=num_epochs,\n        validation_split=0.1,\n        shuffle=True,\n        batch_size=batch_size)\n\n    predictions = model.predict(X_test)\n\n    # compute some metrics and store them for averaging later on\n    roc_auc_scores = np.append(roc_auc_scores, roc_auc_score(y_test, predictions))\n\n# print mean scores for the 5-fold CV\nprint(\"average roc_auc score: \", np.round(roc_auc_scores.mean(), 3))\nprint(\"stdv roc_auc score: \", np.round(roc_auc_scores.std(), 3))\nprint(\"max roc_auc score: \", np.round(roc_auc_scores.max(), 3))\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nEpoch 1/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 2s 24ms/step - acc: 0.6781 - loss: 0.6864 - val_acc: 0.8947 - val_loss: 0.6716\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.8789 - loss: 0.6644 - val_acc: 0.8421 - val_loss: 0.6573\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9119 - loss: 0.6438 - val_acc: 0.8421 - val_loss: 0.6402\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9348 - loss: 0.6163 - val_acc: 0.8421 - val_loss: 0.6189\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9554 - loss: 0.5744 - val_acc: 0.8421 - val_loss: 0.5941\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9531 - loss: 0.5460 - val_acc: 0.8947 - val_loss: 0.5659\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9599 - loss: 0.4890 - val_acc: 0.8947 - val_loss: 0.5333\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9810 - loss: 0.4360 - val_acc: 0.8947 - val_loss: 0.4998\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9731 - loss: 0.3867 - val_acc: 0.8947 - val_loss: 0.4684\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9755 - loss: 0.3339 - val_acc: 0.8947 - val_loss: 0.4367\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9662 - loss: 0.2938 - val_acc: 0.8947 - val_loss: 0.4059\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9853 - loss: 0.2334 - val_acc: 0.8947 - val_loss: 0.3765\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9795 - loss: 0.2030 - val_acc: 0.8947 - val_loss: 0.3514\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9697 - loss: 0.1870 - val_acc: 0.9474 - val_loss: 0.3290\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9802 - loss: 0.1511 - val_acc: 0.9474 - val_loss: 0.3112\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9838 - loss: 0.1303 - val_acc: 0.9474 - val_loss: 0.2945\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9951 - loss: 0.1099 - val_acc: 0.9474 - val_loss: 0.2817\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9824 - loss: 0.0907 - val_acc: 0.9474 - val_loss: 0.2697\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9911 - loss: 0.0880 - val_acc: 0.9474 - val_loss: 0.2594\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9921 - loss: 0.0758 - val_acc: 0.9474 - val_loss: 0.2511\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9942 - loss: 0.0662 - val_acc: 0.9474 - val_loss: 0.2425\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9986 - loss: 0.0550 - val_acc: 0.9474 - val_loss: 0.2354\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0502 - val_acc: 0.9474 - val_loss: 0.2298\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0526 - val_acc: 0.9474 - val_loss: 0.2249\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0376 - val_acc: 0.9474 - val_loss: 0.2198\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0432 - val_acc: 0.9474 - val_loss: 0.2161\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 1.0000 - loss: 0.0403 - val_acc: 0.9474 - val_loss: 0.2127\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0344 - val_acc: 0.9474 - val_loss: 0.2090\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0291 - val_acc: 0.9474 - val_loss: 0.2056\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0341 - val_acc: 0.9474 - val_loss: 0.2028\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 51ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - acc: 0.5245 - loss: 0.6864 - val_acc: 0.7895 - val_loss: 0.6639\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7940 - loss: 0.6463 - val_acc: 0.8947 - val_loss: 0.6244\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7845 - loss: 0.6071 - val_acc: 0.8947 - val_loss: 0.5846\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8110 - loss: 0.5591 - val_acc: 0.8947 - val_loss: 0.5419\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7568 - loss: 0.5296 - val_acc: 0.8947 - val_loss: 0.4949\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8398 - loss: 0.4532 - val_acc: 0.8947 - val_loss: 0.4479\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8624 - loss: 0.4073 - val_acc: 0.9474 - val_loss: 0.3997\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.8844 - loss: 0.3696 - val_acc: 0.9474 - val_loss: 0.3602\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8985 - loss: 0.3061 - val_acc: 0.9474 - val_loss: 0.3235\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9294 - loss: 0.2441 - val_acc: 0.9474 - val_loss: 0.2955\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9398 - loss: 0.2525 - val_acc: 0.9474 - val_loss: 0.2741\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9697 - loss: 0.1773 - val_acc: 0.9474 - val_loss: 0.2546\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9642 - loss: 0.1744 - val_acc: 0.9474 - val_loss: 0.2416\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9927 - loss: 0.1317 - val_acc: 0.9474 - val_loss: 0.2291\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9963 - loss: 0.0988 - val_acc: 0.9474 - val_loss: 0.2199\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9910 - loss: 0.1061 - val_acc: 0.9474 - val_loss: 0.2142\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9870 - loss: 0.0847 - val_acc: 0.9474 - val_loss: 0.2086\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9875 - loss: 0.0773 - val_acc: 0.9474 - val_loss: 0.2032\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9936 - loss: 0.0630 - val_acc: 0.9474 - val_loss: 0.1992\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9900 - loss: 0.0480 - val_acc: 0.9474 - val_loss: 0.1967\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9978 - loss: 0.0434 - val_acc: 0.9474 - val_loss: 0.1985\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9990 - loss: 0.0373 - val_acc: 0.9474 - val_loss: 0.1925\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9907 - loss: 0.0416 - val_acc: 0.9474 - val_loss: 0.1903\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9986 - loss: 0.0265 - val_acc: 0.9474 - val_loss: 0.1872\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9889 - loss: 0.0482 - val_acc: 0.9474 - val_loss: 0.1878\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - acc: 0.9968 - loss: 0.0271 - val_acc: 0.9474 - val_loss: 0.1855\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9942 - loss: 0.0249 - val_acc: 0.9474 - val_loss: 0.1830\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9978 - loss: 0.0236 - val_acc: 0.9474 - val_loss: 0.1812\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9990 - loss: 0.0167 - val_acc: 0.9474 - val_loss: 0.1790\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9982 - loss: 0.0160 - val_acc: 0.9474 - val_loss: 0.1782\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 2s 17ms/step - acc: 0.6353 - loss: 0.6837 - val_acc: 0.8421 - val_loss: 0.6557\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.8298 - loss: 0.6397 - val_acc: 0.8947 - val_loss: 0.6169\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8125 - loss: 0.6002 - val_acc: 0.8947 - val_loss: 0.5729\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8341 - loss: 0.5418 - val_acc: 0.8947 - val_loss: 0.5222\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8520 - loss: 0.5082 - val_acc: 0.8947 - val_loss: 0.4738\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9143 - loss: 0.4262 - val_acc: 0.8947 - val_loss: 0.4205\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8948 - loss: 0.3625 - val_acc: 0.9474 - val_loss: 0.3732\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9091 - loss: 0.3269 - val_acc: 0.9474 - val_loss: 0.3339\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9427 - loss: 0.2586 - val_acc: 0.9474 - val_loss: 0.2995\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9609 - loss: 0.1972 - val_acc: 0.9474 - val_loss: 0.2726\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9694 - loss: 0.1751 - val_acc: 0.9474 - val_loss: 0.2522\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9859 - loss: 0.1300 - val_acc: 0.9474 - val_loss: 0.2340\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9770 - loss: 0.1351 - val_acc: 0.9474 - val_loss: 0.2200\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9629 - loss: 0.1182 - val_acc: 0.9474 - val_loss: 0.2064\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9886 - loss: 0.0936 - val_acc: 0.9474 - val_loss: 0.1952\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9787 - loss: 0.0841 - val_acc: 0.9474 - val_loss: 0.1843\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9871 - loss: 0.0595 - val_acc: 0.9474 - val_loss: 0.1759\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9942 - loss: 0.0493 - val_acc: 0.9474 - val_loss: 0.1661\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9805 - loss: 0.0670 - val_acc: 0.9474 - val_loss: 0.1580\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9805 - loss: 0.0443 - val_acc: 1.0000 - val_loss: 0.1510\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 1.0000 - loss: 0.0349 - val_acc: 1.0000 - val_loss: 0.1437\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0302 - val_acc: 1.0000 - val_loss: 0.1379\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 1.0000 - loss: 0.0315 - val_acc: 1.0000 - val_loss: 0.1315\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0259 - val_acc: 1.0000 - val_loss: 0.1276\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0182 - val_acc: 1.0000 - val_loss: 0.1220\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0169 - val_acc: 1.0000 - val_loss: 0.1133\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0207 - val_acc: 1.0000 - val_loss: 0.1098\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0178 - val_acc: 1.0000 - val_loss: 0.1066\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0177 - val_acc: 1.0000 - val_loss: 0.1028\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0180 - val_acc: 1.0000 - val_loss: 0.0997\n\n\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x7e145c6972e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 55ms/step\n\n\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x7e145c6972e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - acc: 0.5848 - loss: 0.6893 - val_acc: 0.8421 - val_loss: 0.6645\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.7615 - loss: 0.6533 - val_acc: 0.8421 - val_loss: 0.6339\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.8885 - loss: 0.6101 - val_acc: 0.8421 - val_loss: 0.5981\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8433 - loss: 0.5674 - val_acc: 0.8421 - val_loss: 0.5585\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.8330 - loss: 0.5259 - val_acc: 0.8947 - val_loss: 0.5146\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8817 - loss: 0.4511 - val_acc: 0.8947 - val_loss: 0.4659\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8856 - loss: 0.4064 - val_acc: 0.8947 - val_loss: 0.4203\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8934 - loss: 0.3468 - val_acc: 0.8947 - val_loss: 0.3785\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9387 - loss: 0.2967 - val_acc: 0.8947 - val_loss: 0.3457\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9515 - loss: 0.2525 - val_acc: 0.8947 - val_loss: 0.3168\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9590 - loss: 0.1977 - val_acc: 0.8947 - val_loss: 0.2921\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9647 - loss: 0.1595 - val_acc: 0.9474 - val_loss: 0.2741\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9832 - loss: 0.1305 - val_acc: 0.9474 - val_loss: 0.2576\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9776 - loss: 0.1167 - val_acc: 0.9474 - val_loss: 0.2447\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9761 - loss: 0.1073 - val_acc: 0.9474 - val_loss: 0.2350\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9750 - loss: 0.1028 - val_acc: 0.9474 - val_loss: 0.2270\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9891 - loss: 0.0707 - val_acc: 0.9474 - val_loss: 0.2191\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 0.9932 - loss: 0.0629 - val_acc: 0.9474 - val_loss: 0.2137\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9993 - loss: 0.0474 - val_acc: 0.9474 - val_loss: 0.2083\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9942 - loss: 0.0492 - val_acc: 0.9474 - val_loss: 0.2051\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - acc: 0.9805 - loss: 0.0597 - val_acc: 0.9474 - val_loss: 0.2017\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - acc: 0.9990 - loss: 0.0326 - val_acc: 0.9474 - val_loss: 0.1981\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - acc: 1.0000 - loss: 0.0386 - val_acc: 0.9474 - val_loss: 0.1952\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 1.0000 - loss: 0.0343 - val_acc: 0.9474 - val_loss: 0.1929\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - acc: 1.0000 - loss: 0.0225 - val_acc: 0.9474 - val_loss: 0.1911\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 1.0000 - loss: 0.0275 - val_acc: 0.9474 - val_loss: 0.1897\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - acc: 1.0000 - loss: 0.0241 - val_acc: 0.9474 - val_loss: 0.1886\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0204 - val_acc: 0.9474 - val_loss: 0.1873\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0192 - val_acc: 0.9474 - val_loss: 0.1864\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 1.0000 - loss: 0.0194 - val_acc: 0.9474 - val_loss: 0.1859\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 55ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 2s 18ms/step - acc: 0.3339 - loss: 0.7035 - val_acc: 0.6842 - val_loss: 0.6910\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7067 - loss: 0.6837 - val_acc: 0.8947 - val_loss: 0.6758\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.8963 - loss: 0.6649 - val_acc: 0.8947 - val_loss: 0.6610\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9186 - loss: 0.6456 - val_acc: 0.8947 - val_loss: 0.6426\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8916 - loss: 0.6232 - val_acc: 0.8947 - val_loss: 0.6192\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9491 - loss: 0.5828 - val_acc: 0.8947 - val_loss: 0.5886\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9602 - loss: 0.5340 - val_acc: 0.8947 - val_loss: 0.5520\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9625 - loss: 0.4836 - val_acc: 0.8947 - val_loss: 0.5106\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9589 - loss: 0.4232 - val_acc: 0.8947 - val_loss: 0.4656\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9651 - loss: 0.3522 - val_acc: 0.8947 - val_loss: 0.4212\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9752 - loss: 0.2727 - val_acc: 0.8947 - val_loss: 0.3789\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9644 - loss: 0.2329 - val_acc: 0.8947 - val_loss: 0.3446\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9567 - loss: 0.2008 - val_acc: 0.9474 - val_loss: 0.3179\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9958 - loss: 0.1348 - val_acc: 0.9474 - val_loss: 0.2940\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9875 - loss: 0.1158 - val_acc: 0.9474 - val_loss: 0.2771\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9824 - loss: 0.1018 - val_acc: 0.9474 - val_loss: 0.2622\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9792 - loss: 0.0964 - val_acc: 0.9474 - val_loss: 0.2490\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9963 - loss: 0.0594 - val_acc: 0.9474 - val_loss: 0.2382\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9927 - loss: 0.0563 - val_acc: 0.9474 - val_loss: 0.2293\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9878 - loss: 0.0496 - val_acc: 0.9474 - val_loss: 0.2215\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9907 - loss: 0.0445 - val_acc: 0.9474 - val_loss: 0.2140\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0347 - val_acc: 0.9474 - val_loss: 0.2080\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0298 - val_acc: 0.9474 - val_loss: 0.2024\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0254 - val_acc: 0.9474 - val_loss: 0.1987\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0249 - val_acc: 0.9474 - val_loss: 0.1954\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 1.0000 - loss: 0.0314 - val_acc: 0.9474 - val_loss: 0.1922\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0203 - val_acc: 0.9474 - val_loss: 0.1897\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0186 - val_acc: 0.9474 - val_loss: 0.1876\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0194 - val_acc: 0.9474 - val_loss: 0.1858\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0220 - val_acc: 0.9474 - val_loss: 0.1842\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 59ms/step\naverage roc_auc score:  0.924\nstdv roc_auc score:  0.021\nmax roc_auc score:  0.952"
  },
  {
    "objectID": "educ_6191_001/creative_assignments/assignment_4/austinbao0419/CSCL23_feedback_detectors.html#use",
    "href": "educ_6191_001/creative_assignments/assignment_4/austinbao0419/CSCL23_feedback_detectors.html#use",
    "title": "Penn GSE",
    "section": "USE",
    "text": "USE\n\n%%capture\n!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet\n\n\nfrom getpass import getpass\nimport openai\nimport os\n\n#os.kill(os.getpid(), 9)\n\nprint('Enter OpenAI API key:')\nopenai.api_key = getpass()\n\nos.environ['OPENAI_API_KEY']=openai.api_key\n\nEnter OpenAI API key:\n··········\n\n\n\nimport time\n\ndef generate_zero_shot(topic):\n  prompt = \"\"\"\n  I will give you a peer review statement on a math problem solution, and this statement is identified to have this key attribute:\n  \"Commenting on the Process (CP): When evaluating peers’ works, learners comment on the process of the work\". Your task is to simplify\n  this statement by removing information that is not relevant to this attribute, leaving only the core description that represents the attribute of CP.\n  A good example of statement having this attribute is: \"I like how you had 20 then subtracted 12 and got 8\".\n  \"\"\"\n  system_message = {\"role\" : \"system\", \"content\" : prompt}\n\n  client = openai.OpenAI()\n  response = client.chat.completions.create(\n      model=\"gpt-3.5-turbo\",\n      messages= [\n          system_message,\n          {\"role\" : \"user\", \"content\" : topic} # simulate a user prompt\n      ],\n      temperature=0.7,\n      max_tokens=256,\n      top_p=1,\n      frequency_penalty=0,\n      presence_penalty=0,\n      stop=[\"\\n\"]\n  )\n  # I recommend putting a short wait after each call,\n  # since the rate limit for the platform is 60 requests/min.\n  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n  time.sleep(1)\n\n  # the response from OpenAI's API is a JSON object that contains\n  # the completion to your prompt plus some other information.  Here's how to access\n  # just the text of the completion.\n  return response.choices[0].message.content.strip()\n\ndef update_comments_with_gpt(df):\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        # Check if comment_process is 1\n        if row['comment_process'] == 1:\n            # Extract the annotation_text\n            original_text = row['annotation_text']\n\n            # Call the generate_zero_shot function\n            try:\n                refined_text = generate_zero_shot(original_text)\n            except Exception as e:\n                print(f\"Error processing row {index}: {e}\")\n                refined_text = original_text  # Fallback to original if GPT fails\n\n            # Update the annotation_text in the DataFrame\n            df.at[index, 'annotation_text'] = refined_text\n            print(f\"Row {index} updated: {original_text} -&gt; {refined_text}\")\n\n    return df\n\ndf = pd.read_csv('Annotations_final.csv')\n\n# Update the DataFrame\nupdated_df = update_comments_with_gpt(df)\n\n# Save the updated DataFrame to a new CSV\nupdated_df.to_csv('updated_annotations.csv', index=False)\n\nprint(\"CSV updated and saved as 'updated_annotations.csv'.\")\n\nRow 15 updated: I like how you had 20 then subtected 12 and got 8 then you added 8 and got 16 then you answer was -4  -&gt; I like how you subtracted 12 from 20 and got 8, then added 8 to get 16, but your final answer was -4.\nRow 16 updated: the video is fine i like how you add the numbers  -&gt; I like how you add the numbers.\nRow 18 updated: i like how you add the number -&gt; You did a great job adding the numbers.\nRow 23 updated: I like the way you  put how much $20 he have and the lost 12 -&gt; I like how you had 20 then subtracted 12 and got 8.\nRow 27 updated: I like the way you used arrows in your problem. -&gt; I like how you used arrows.\nRow 31 updated: The way that you set up your Data is great. And how you divided to get to your mean. GOOD JOB KALLI :) -&gt; I like how you set up your data and calculated the mean. Great job!\nRow 33 updated: I like the way you sorted the data.\nI also like the way you showed your work clearly so it wouldn't be confusing. -&gt; You sorted the data nicely and presented your work clearly.\nRow 36 updated: I think that Noah had the best estimate because it's about how many people get the average amount of sleep not about how many hours is good for you because that does not have anything to do with the problem -&gt; You focused on the relevance of the estimate to the problem, which is important for understanding the context.\nRow 39 updated: Hi. First, that is how you find the median, not the mean. So first you have to find the mean, add all of the numbers and then divide it by how many numbers there are. Then once you found the mean you estimate.. -&gt; You correctly identified the process for finding the mean. Great job!\nRow 42 updated: I liked how you solved your answer even though I think that's not how your supposed to solve it. But in my opinion your final answer was correct. -&gt; You solved the problem differently, but your final answer was correct.\nRow 43 updated: i think you did a good job because you wrote out all the numbers then you drew lines and circles to know where you were at -&gt; I like how you wrote out all the numbers and used lines and circles to help keep track of your work.\nRow 44 updated: I like that you put all of your thought in the recording. And that you used basic knowledge to find out your answer.  -&gt; I appreciate the thorough explanation and the use of fundamental concepts in your solution.\nRow 46 updated: I respectfully disagree with you because you were supposed to find out how much he lost.  -&gt; You were supposed to find out how much he lost.\nRow 48 updated: My strategy is like yours because I also did 3x2  -&gt; \"I like how you did 3 multiplied by 2 in a similar way as I did.\"\nRow 50 updated: I like the way you subtracted the amount Juan spent from the amount he started with. -&gt; You subtracted the amounts in a clear and logical manner.\nRow 51 updated: My strategy is like yours because i added 8 and 8 together too.  -&gt; I like how you added 8 and 8 together.\nRow 52 updated: I like the way you added the 30 + 83 and you got 113 -&gt; I like how you added 30 and 83 to get 113.\nRow 53 updated: My strategy is like yours because we did the same thing when finding out -20.75x4 was -83 -&gt; I like how we both found -20.75 multiplied by 4 to be -83.\nRow 55 updated: I like the way you explained this and how you fixed 113 to -113 good job catching your mistake.  -&gt; You explained well and caught the mistake in changing 113 to -113. Great job!\nRow 57 updated: I respectfully disagree with you because one of the deposits was 25.25, not 22.25 making your answer wrong. -&gt; You used 25.25 for one of the deposits, which affected the final result.\nRow 58 updated: I hadn't thought of adding 30 with the 20.75 x4 -&gt; You added 30 to 20.75 before multiplying by 4, which was a good approach.\nRow 60 updated: I like the way you us drawing and your adding is right. -&gt; I like how you used drawing and your addition is correct.\nRow 66 updated: I like the way you multiplied 3 by 2 aswell  -&gt; You multiplied 3 by 2 well.\nRow 67 updated: I respectfully disagree with you on the last pieces of your math as you had added a positive with a negative. While you should have added -30 with the -83 and gotten -113 then subtractive that with the positive 76 and gotten -37. -&gt; You added a positive with a negative instead of subtracting them, which resulted in an incorrect answer. You should have subtracted -30 from -83 to get -113, then subtracted that from 76 to get -37.\nRow 76 updated: My strategy is like yours because we both multiplied 2 by 3  -&gt; You multiplied 2 by 3 just like I did.\nRow 78 updated: My strategy is like yours because I also multiplied 2 and 3 -&gt; I appreciate your method of multiplying 2 and 3.\nRow 79 updated: Why did you not mention (or include in the problem) the $8 Juan earned? -&gt; You didn't include the $8 Juan earned in your solution.\nRow 82 updated: I like the way your work is organized  also I like they  added 5+1 because I did not really think about doing that  -&gt; I like how you added 5+1 because I did not think about doing that.\nRow 98 updated: Why did you make no further mention of $76, the result of the first problem you solved? -&gt; You didn't address the number 76 from the initial problem you worked on.\nRow 110 updated: My strategy is like yours because I also did -20.75 x4   -&gt; I appreciate your approach of multiplying -20.75 by 4.\nRow 113 updated: My strategy is like yours because we did some of the same steps.  -&gt; You and I took similar steps in our strategies.\nRow 116 updated: My strategy is like yours because I also added the deposits  -&gt; You added the deposits, which was a good strategy.\nRow 119 updated: My strategy is like yours because I multiplied 20.75 by 4 to -&gt; I like how you multiplied 20.75 by 4 in your strategy.\nRow 120 updated: My strategy is like yours because I multiplied 20.75 by 4 as well -&gt; I appreciate how you multiplied 20.75 by 4 in your solution.\nRow 125 updated: When you did 20.75x4 you put the 4 in the wrong spot but when you did 2x(-15) that answer was correct. -&gt; You misplaced the 4 in 20.75x4, but your calculation for 2x(-15) was correct.\nRow 129 updated: My strategy is like yours because I also added the two deposits together. -&gt; I appreciate how you added the two deposits together in your strategy.\nRow 135 updated: My strategy is like yours because I also made -12 the same way. I also think its great you put the 20-12 on the side and explained that wasnt the actual problem. -&gt; You explained well how you approached the problem by starting with 20 and then subtracting 12.\nRow 140 updated: My strategy is like yours because i added what he bought  6+5+1= 12 so that what he lost -&gt; You added the amounts he bought and correctly found the total.\nRow 141 updated: My strategy is like yours because I did 15 x 3 also  -&gt; You multiplied 15 by 3, which is the same as my strategy.\nRow 143 updated: I like how you get 12 and subrtact by 8 to get how much  got now -&gt; I like how you subtracted 8 from 12 to find the result.\nRow 145 updated: I agree with you adding but for the positives but for the negatives your answer should have been negative. -&gt; You should have made your answer negative for the negatives.\nRow 149 updated: I respectfully disagree with you because you didn't put negative signs and the 4 is supposed to be negative or at least that's what I got. -&gt; You need to include negative signs, as the 4 should be negative.\nRow 151 updated: My strategy is like yours because  we both multiplied 2 times 15 -&gt; I like how we both multiplied 2 times 15.\nRow 154 updated: My strategy is like yours because  i added what he gained but I hadn't thought of 12-8  -&gt; You added what he gained, but didn't think of 12-8.\nRow 162 updated: My strategy is like yours because i did 3x2 -&gt; I like how you multiplied 3 by 2 in your strategy.\nRow 166 updated: My strategy is like yours because we both did the same thing, although I added 5+1 to get 6+6 and you did 6+5+1. -&gt; I like how we both added numbers differently to reach our solutions.\nRow 167 updated: I like how you  added and subtracted the number for exaple you did 20-6 and got 14 and then you did 8+8 and got 16. the other thing i like about your work is that you wrote a word problem to show how much he lost  -&gt; I like how you added and subtracted numbers, for example, 20-6 to get 14 and then 8+8 to get 16.\nRow 178 updated: My strategy is like yours because I put the information in almost the exact same way. I think I just switched the places of the two withdrawls. -&gt; You and I used a similar strategy but with a slight difference in the order of the withdrawals.\nRow 179 updated: I like the way you multiplied the withdrawals instead of adding to four times or two times. -&gt; You multiplied the withdrawals instead of adding them multiple times, which was a good choice.\nRow 180 updated: I like the way you you added all the deposits first then subtracted the withdrawls I didn't think of that -&gt; You added all the deposits first then subtracted the withdrawals, which I didn't think of.\nRow 181 updated: I like the way you started with adding your two deposits together. -&gt; You did a great job by starting with adding your two deposits together.\nRow 191 updated: My strategy is like yours because we both didn't use the $20 in the beginning. -&gt; You and I both avoided using the initial $20 in our strategies.\nRow 192 updated: I like the way you added the positive together then subtracted the negaives to make the equation more simple -&gt; I like how you added the positives together then subtracted the negatives.\nRow 195 updated: I agree with your answer but maybe next time make the numbers that are suppose to be negative negative in the equation to make it more clear. -&gt; Next time, be sure to make the negative numbers negative in the equation for clarity.\nRow 199 updated: Why did you start with 20. I kind of want to know just because many did the same but some did something different, like me. &lt;3 -&gt; You started with 20 then subtracted 12 to get 8.\nRow 205 updated: When you did the 3*2 and you got -6 it could confuse people because the 3 and the 2 are both positives so how did you get a negative. -&gt; You multiplied 3 by 2 and got -6, which might be confusing because both numbers were positive.\nRow 206 updated: I like the way you showed your work step by step, but the question asks what was the total amount of money he lost or gained by the end of the day. -&gt; You showed the work step by step, but the question asks for the total amount lost or gained.\nRow 208 updated: I respectfully disagree with you your answer I think you might have messed up a step in the subtraction part. -&gt; You may have made a mistake in the subtraction step.\nRow 210 updated: I like the way you did it because botnh added 83 and 30. -&gt; You added 83 and 30, which was a good approach.\nRow 211 updated: Why did you add 2 and 5? -&gt; Commenting on the Process (CP): Why did you add 2 and 5?\nRow 212 updated: My strategy is like yours because  i also did 20-6 and i got 14  -&gt; You subtracted 6 from 20 and got 14, which is similar to my approach.\nRow 213 updated: Next time maybe you can add 15 x 2 and the totals you get add them butni like your video. -&gt; You could try adding 15 x 2 next time, but I liked your video.\nRow 216 updated: I like the way you added the   83.00 and 30.00. also whow you subtracted 113 and 75  -&gt; I like how you added two numbers together and subtracted another pair.\nRow 224 updated: Premium Growth feed will make the chicken's weight be about the same because the Mad for pen A is 0.512. The mad for pen B was 0.97 which is higher Martin -&gt; You used the Mad for pen A and pen B to compare and concluded that pen B had a higher value.\nCSV updated and saved as 'updated_annotations.csv'.\n\n\n\n# loading universal sentence encoder\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n\n\n# import data\ndf = pd.read_csv(\"updated_annotations.csv\").fillna(0)\n\n# extract features as X\nX = df[['annotation_text','created_by']]\n\n# extract the prediction variable as y\ny = df.comment_process # CHANGE y HERE\n\n# set up storage arrays for each round of validation\nroc_auc_scores = np.array([])\npred = pd.DataFrame()\n\n# split, train, test and store performance metrics\nfor train_index, test_index in gkf.split(X, y, groups=groups):\n\n    X_train = X.iloc[train_index].drop(['created_by'], axis=1)\n    X_test = X.iloc[test_index].drop(['created_by'], axis=1)\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n\n    # train classifier on this round of training group\n    training_embeddings = embed(X_train.annotation_text.to_list())\n\n    model = Sequential()\n    model.add(Dense(12, input_shape=(512,), activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics = ['acc'])\n\n    num_epochs = 30\n    batch_size = 10\n\n\n    model.fit(\n        training_embeddings,\n        y_train,\n        epochs=num_epochs,\n        validation_split=0.1,\n        shuffle=True,\n        batch_size=batch_size)\n\n    # test classifier on this round of testing group\n    testing_embeddings = embed(X_test.annotation_text.to_list())\n    predictions = model.predict(testing_embeddings)\n\n    # compute some metrics and store them for averaging later on\n    roc_auc_scores = np.append(roc_auc_scores, roc_auc_score(y_test, predictions))\n\n\n# print mean scores for the 5-fold CV\nprint(\"average roc_auc score: \", np.round(roc_auc_scores.mean(), 3))\nprint(\"stdv roc_auc score: \", np.round(roc_auc_scores.std(), 3))\nprint(\"max roc_auc score: \", np.round(roc_auc_scores.max(), 3))\n\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 2s 26ms/step - acc: 0.7672 - loss: 0.6827 - val_acc: 0.7895 - val_loss: 0.6554\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 0.8264 - loss: 0.6457 - val_acc: 0.7895 - val_loss: 0.6083\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 0.8441 - loss: 0.5973 - val_acc: 0.7895 - val_loss: 0.5571\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - acc: 0.8704 - loss: 0.5389 - val_acc: 0.7895 - val_loss: 0.5033\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - acc: 0.8907 - loss: 0.4742 - val_acc: 0.7895 - val_loss: 0.4468\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.8795 - loss: 0.4426 - val_acc: 0.8947 - val_loss: 0.3980\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9164 - loss: 0.3718 - val_acc: 0.8947 - val_loss: 0.3568\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9228 - loss: 0.3723 - val_acc: 0.8947 - val_loss: 0.3186\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9266 - loss: 0.3338 - val_acc: 0.8947 - val_loss: 0.2836\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9440 - loss: 0.2917 - val_acc: 0.9474 - val_loss: 0.2548\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9473 - loss: 0.2750 - val_acc: 0.9474 - val_loss: 0.2283\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9476 - loss: 0.2352 - val_acc: 0.9474 - val_loss: 0.2037\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9547 - loss: 0.2274 - val_acc: 1.0000 - val_loss: 0.1842\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9509 - loss: 0.1978 - val_acc: 1.0000 - val_loss: 0.1688\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9622 - loss: 0.1879 - val_acc: 1.0000 - val_loss: 0.1515\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9459 - loss: 0.1802 - val_acc: 1.0000 - val_loss: 0.1406\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9638 - loss: 0.1491 - val_acc: 1.0000 - val_loss: 0.1306\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9801 - loss: 0.1267 - val_acc: 1.0000 - val_loss: 0.1198\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9661 - loss: 0.1462 - val_acc: 1.0000 - val_loss: 0.1118\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9804 - loss: 0.1272 - val_acc: 1.0000 - val_loss: 0.1067\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9716 - loss: 0.1134 - val_acc: 1.0000 - val_loss: 0.0979\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9615 - loss: 0.1100 - val_acc: 1.0000 - val_loss: 0.0923\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9729 - loss: 0.1219 - val_acc: 1.0000 - val_loss: 0.0906\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9612 - loss: 0.1266 - val_acc: 1.0000 - val_loss: 0.0820\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9917 - loss: 0.0696 - val_acc: 1.0000 - val_loss: 0.0791\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9862 - loss: 0.0825 - val_acc: 1.0000 - val_loss: 0.0723\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9946 - loss: 0.0646 - val_acc: 1.0000 - val_loss: 0.0708\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9968 - loss: 0.0670 - val_acc: 1.0000 - val_loss: 0.0648\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9980 - loss: 0.0616 - val_acc: 1.0000 - val_loss: 0.0645\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9960 - loss: 0.0444 - val_acc: 1.0000 - val_loss: 0.0614\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 55ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 1s 16ms/step - acc: 0.6869 - loss: 0.6863 - val_acc: 0.7368 - val_loss: 0.6591\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7116 - loss: 0.6530 - val_acc: 0.7368 - val_loss: 0.6143\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.7331 - loss: 0.5942 - val_acc: 0.7368 - val_loss: 0.5563\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.7667 - loss: 0.5336 - val_acc: 0.7368 - val_loss: 0.4934\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8021 - loss: 0.4818 - val_acc: 0.7895 - val_loss: 0.4314\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8547 - loss: 0.4324 - val_acc: 0.8947 - val_loss: 0.3705\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.8793 - loss: 0.3869 - val_acc: 0.9474 - val_loss: 0.3214\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9159 - loss: 0.3110 - val_acc: 0.9474 - val_loss: 0.2744\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9346 - loss: 0.2802 - val_acc: 0.9474 - val_loss: 0.2381\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9522 - loss: 0.2329 - val_acc: 0.9474 - val_loss: 0.2138\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 0.9527 - loss: 0.2494 - val_acc: 0.9474 - val_loss: 0.1892\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9851 - loss: 0.1728 - val_acc: 0.9474 - val_loss: 0.1719\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 0.9709 - loss: 0.1567 - val_acc: 0.9474 - val_loss: 0.1522\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9584 - loss: 0.1667 - val_acc: 0.9474 - val_loss: 0.1442\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9668 - loss: 0.1485 - val_acc: 0.9474 - val_loss: 0.1347\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - acc: 0.9639 - loss: 0.1644 - val_acc: 0.9474 - val_loss: 0.1269\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9789 - loss: 0.1043 - val_acc: 0.9474 - val_loss: 0.1182\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 0.9886 - loss: 0.0841 - val_acc: 0.9474 - val_loss: 0.1182\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - acc: 0.9745 - loss: 0.1022 - val_acc: 0.9474 - val_loss: 0.1065\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9796 - loss: 0.0780 - val_acc: 0.9474 - val_loss: 0.1023\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9881 - loss: 0.0650 - val_acc: 0.9474 - val_loss: 0.0968\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9936 - loss: 0.0538 - val_acc: 0.9474 - val_loss: 0.0904\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9931 - loss: 0.0653 - val_acc: 0.9474 - val_loss: 0.0886\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9787 - loss: 0.0795 - val_acc: 0.9474 - val_loss: 0.0867\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9874 - loss: 0.0512 - val_acc: 0.9474 - val_loss: 0.0793\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9956 - loss: 0.0471 - val_acc: 0.9474 - val_loss: 0.0761\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9847 - loss: 0.0566 - val_acc: 0.9474 - val_loss: 0.0760\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9942 - loss: 0.0474 - val_acc: 1.0000 - val_loss: 0.0683\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 0.9973 - loss: 0.0302 - val_acc: 0.9474 - val_loss: 0.0714\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9982 - loss: 0.0281 - val_acc: 1.0000 - val_loss: 0.0595\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 61ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - acc: 0.6615 - loss: 0.6808 - val_acc: 0.5789 - val_loss: 0.6742\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7480 - loss: 0.6103 - val_acc: 0.5789 - val_loss: 0.6379\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.6756 - loss: 0.5769 - val_acc: 0.5789 - val_loss: 0.5882\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7107 - loss: 0.5103 - val_acc: 0.5789 - val_loss: 0.5422\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.7641 - loss: 0.4623 - val_acc: 0.6842 - val_loss: 0.4827\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8541 - loss: 0.4083 - val_acc: 0.8421 - val_loss: 0.4286\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8692 - loss: 0.3623 - val_acc: 0.8421 - val_loss: 0.3839\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9210 - loss: 0.3328 - val_acc: 0.8421 - val_loss: 0.3326\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9449 - loss: 0.2639 - val_acc: 0.8421 - val_loss: 0.3007\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9566 - loss: 0.2300 - val_acc: 0.8947 - val_loss: 0.2703\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9519 - loss: 0.1905 - val_acc: 0.9474 - val_loss: 0.2398\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9605 - loss: 0.1963 - val_acc: 0.9474 - val_loss: 0.2296\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9637 - loss: 0.1513 - val_acc: 0.9474 - val_loss: 0.2036\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9673 - loss: 0.1320 - val_acc: 0.9474 - val_loss: 0.1931\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9543 - loss: 0.1423 - val_acc: 0.9474 - val_loss: 0.1823\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9644 - loss: 0.1130 - val_acc: 0.9474 - val_loss: 0.1765\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9889 - loss: 0.0959 - val_acc: 0.9474 - val_loss: 0.1664\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9805 - loss: 0.0935 - val_acc: 0.9474 - val_loss: 0.1637\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9889 - loss: 0.0736 - val_acc: 0.9474 - val_loss: 0.1589\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9956 - loss: 0.0648 - val_acc: 0.9474 - val_loss: 0.1553\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0588 - val_acc: 0.9474 - val_loss: 0.1442\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0500 - val_acc: 0.9474 - val_loss: 0.1443\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0423 - val_acc: 0.9474 - val_loss: 0.1509\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0429 - val_acc: 0.9474 - val_loss: 0.1513\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 1.0000 - loss: 0.0550 - val_acc: 0.9474 - val_loss: 0.1302\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - acc: 1.0000 - loss: 0.0410 - val_acc: 0.9474 - val_loss: 0.1413\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - acc: 1.0000 - loss: 0.0330 - val_acc: 0.9474 - val_loss: 0.1409\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 1.0000 - loss: 0.0360 - val_acc: 0.9474 - val_loss: 0.1374\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - acc: 1.0000 - loss: 0.0307 - val_acc: 0.9474 - val_loss: 0.1332\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - acc: 1.0000 - loss: 0.0291 - val_acc: 0.9474 - val_loss: 0.1378\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - acc: 0.6005 - loss: 0.6856 - val_acc: 0.6316 - val_loss: 0.6732\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7720 - loss: 0.6386 - val_acc: 0.6316 - val_loss: 0.6390\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.7723 - loss: 0.5700 - val_acc: 0.6316 - val_loss: 0.5980\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.8009 - loss: 0.4838 - val_acc: 0.6316 - val_loss: 0.5519\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7741 - loss: 0.4632 - val_acc: 0.6842 - val_loss: 0.4708\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9051 - loss: 0.3536 - val_acc: 0.8421 - val_loss: 0.3926\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9401 - loss: 0.2968 - val_acc: 0.8421 - val_loss: 0.3532\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9144 - loss: 0.3012 - val_acc: 0.8947 - val_loss: 0.3029\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9589 - loss: 0.2271 - val_acc: 0.8947 - val_loss: 0.2731\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9443 - loss: 0.2209 - val_acc: 0.8947 - val_loss: 0.2449\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9473 - loss: 0.1881 - val_acc: 0.8947 - val_loss: 0.2258\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9511 - loss: 0.1651 - val_acc: 0.9474 - val_loss: 0.2031\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9667 - loss: 0.1386 - val_acc: 0.9474 - val_loss: 0.1917\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9851 - loss: 0.1135 - val_acc: 0.9474 - val_loss: 0.1803\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9577 - loss: 0.1328 - val_acc: 0.9474 - val_loss: 0.1667\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9747 - loss: 0.1061 - val_acc: 0.9474 - val_loss: 0.1550\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9765 - loss: 0.0990 - val_acc: 0.9474 - val_loss: 0.1535\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9849 - loss: 0.0827 - val_acc: 0.9474 - val_loss: 0.1482\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9715 - loss: 0.0931 - val_acc: 0.9474 - val_loss: 0.1369\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.9882 - loss: 0.0695 - val_acc: 0.9474 - val_loss: 0.1316\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9784 - loss: 0.0787 - val_acc: 0.9474 - val_loss: 0.1240\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9932 - loss: 0.0519 - val_acc: 0.9474 - val_loss: 0.1266\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9899 - loss: 0.0617 - val_acc: 0.9474 - val_loss: 0.1133\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9963 - loss: 0.0567 - val_acc: 0.9474 - val_loss: 0.1097\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9932 - loss: 0.0554 - val_acc: 0.9474 - val_loss: 0.1130\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9949 - loss: 0.0379 - val_acc: 0.9474 - val_loss: 0.1104\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9982 - loss: 0.0551 - val_acc: 0.9474 - val_loss: 0.1037\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9986 - loss: 0.0337 - val_acc: 0.9474 - val_loss: 0.1042\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9949 - loss: 0.0348 - val_acc: 0.9474 - val_loss: 0.0976\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9949 - loss: 0.0286 - val_acc: 0.9474 - val_loss: 0.0973\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 52ms/step\nEpoch 1/30\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n17/17 ━━━━━━━━━━━━━━━━━━━━ 2s 31ms/step - acc: 0.4193 - loss: 0.6954 - val_acc: 0.6842 - val_loss: 0.6862\nEpoch 2/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - acc: 0.7327 - loss: 0.6805 - val_acc: 0.6316 - val_loss: 0.6729\nEpoch 3/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7113 - loss: 0.6601 - val_acc: 0.6316 - val_loss: 0.6484\nEpoch 4/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.7861 - loss: 0.6303 - val_acc: 0.7895 - val_loss: 0.6042\nEpoch 5/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.8197 - loss: 0.5825 - val_acc: 0.7895 - val_loss: 0.5506\nEpoch 6/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.8887 - loss: 0.5308 - val_acc: 0.8421 - val_loss: 0.4843\nEpoch 7/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9160 - loss: 0.4631 - val_acc: 0.8421 - val_loss: 0.4132\nEpoch 8/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9187 - loss: 0.3989 - val_acc: 0.9474 - val_loss: 0.3467\nEpoch 9/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9760 - loss: 0.3154 - val_acc: 0.9474 - val_loss: 0.2895\nEpoch 10/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9452 - loss: 0.3023 - val_acc: 0.9474 - val_loss: 0.2466\nEpoch 11/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9824 - loss: 0.2145 - val_acc: 0.9474 - val_loss: 0.2177\nEpoch 12/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9674 - loss: 0.1980 - val_acc: 0.9474 - val_loss: 0.1910\nEpoch 13/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9775 - loss: 0.1523 - val_acc: 0.9474 - val_loss: 0.1746\nEpoch 14/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 0.9756 - loss: 0.1263 - val_acc: 0.9474 - val_loss: 0.1637\nEpoch 15/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9793 - loss: 0.1152 - val_acc: 0.9474 - val_loss: 0.1536\nEpoch 16/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9807 - loss: 0.1118 - val_acc: 0.9474 - val_loss: 0.1432\nEpoch 17/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9897 - loss: 0.0782 - val_acc: 0.9474 - val_loss: 0.1402\nEpoch 18/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 0.9978 - loss: 0.0794 - val_acc: 0.9474 - val_loss: 0.1318\nEpoch 19/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 0.9936 - loss: 0.0705 - val_acc: 0.9474 - val_loss: 0.1337\nEpoch 20/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0512 - val_acc: 0.9474 - val_loss: 0.1245\nEpoch 21/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0578 - val_acc: 0.9474 - val_loss: 0.1245\nEpoch 22/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0508 - val_acc: 0.9474 - val_loss: 0.1188\nEpoch 23/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0373 - val_acc: 0.9474 - val_loss: 0.1292\nEpoch 24/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0367 - val_acc: 0.9474 - val_loss: 0.1126\nEpoch 25/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0286 - val_acc: 0.9474 - val_loss: 0.1200\nEpoch 26/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 1.0000 - loss: 0.0286 - val_acc: 0.9474 - val_loss: 0.1144\nEpoch 27/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0229 - val_acc: 0.9474 - val_loss: 0.1204\nEpoch 28/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - acc: 1.0000 - loss: 0.0212 - val_acc: 0.9474 - val_loss: 0.1084\nEpoch 29/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - acc: 1.0000 - loss: 0.0266 - val_acc: 0.9474 - val_loss: 0.1147\nEpoch 30/30\n17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - acc: 1.0000 - loss: 0.0201 - val_acc: 0.9474 - val_loss: 0.1117\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 59ms/step\naverage roc_auc score:  0.934\nstdv roc_auc score:  0.061\nmax roc_auc score:  0.979"
  },
  {
    "objectID": "educ_6190_001/assignments/assignment_1/index.html",
    "href": "educ_6190_001/assignments/assignment_1/index.html",
    "title": "Enhancing Data Quality in Intelligent Tutoring Systems",
    "section": "",
    "text": "Intelligent Tutoring Systems (ITS) have emerged as powerful tools for enhancing learning outcomes across various domains, including reading comprehension. These systems often employ conversational agents to interact with learners, providing personalized feedback and guidance. However, the effectiveness of these agents may depend on various factors, including their language style. This study investigates the impact of the conversational agents’ language style on the quality of written summaries produced by adult participants using AutoTutor ARC, an ITS designed to improve reading comprehension.\nAutoTutor ARC collects extensive log data throughout the learning process, capturing participants’ interactions with the system across multiple lessons. The study design includes pretest assessments (lessons 2 and 3) and posttest assessments (lessons 9 and 10), enabling the comparison of performance before and after the intervention. The conversational agents’ language style is varied between formal, informal, and mixed styles to assess its influence on learning outcomes.\nGiven the inherent complexity of the raw log data, which is characterized by multiple attempts per participant and a diverse range of data formats, a systematic data processing pipeline is crucial. This pipeline aims to prepare the data for meaningful analysis by addressing common issues such as missing values, outliers, and skewed distributions. Furthermore, feature engineering techniques are employed to better capture the relevant variables for subsequent statistical analysis.\nThe present assignment focuses on the importance of numerical data in the context of this study, discussing its background, related work, and techniques. Moreover, it provides considerations and cautions when dealing with numerical data in the realm of machine learning and ITS research. By examining the role of data preparation in the AutoTutor ARC study, this assignment contributes to the broader understanding of how data processing techniques can support the evaluation and improvement of ITS interventions."
  },
  {
    "objectID": "educ_6190_001/assignments/assignment_1/index.html#introduction",
    "href": "educ_6190_001/assignments/assignment_1/index.html#introduction",
    "title": "Enhancing Data Quality in Intelligent Tutoring Systems",
    "section": "",
    "text": "Intelligent Tutoring Systems (ITS) have emerged as powerful tools for enhancing learning outcomes across various domains, including reading comprehension. These systems often employ conversational agents to interact with learners, providing personalized feedback and guidance. However, the effectiveness of these agents may depend on various factors, including their language style. This study investigates the impact of the conversational agents’ language style on the quality of written summaries produced by adult participants using AutoTutor ARC, an ITS designed to improve reading comprehension.\nAutoTutor ARC collects extensive log data throughout the learning process, capturing participants’ interactions with the system across multiple lessons. The study design includes pretest assessments (lessons 2 and 3) and posttest assessments (lessons 9 and 10), enabling the comparison of performance before and after the intervention. The conversational agents’ language style is varied between formal, informal, and mixed styles to assess its influence on learning outcomes.\nGiven the inherent complexity of the raw log data, which is characterized by multiple attempts per participant and a diverse range of data formats, a systematic data processing pipeline is crucial. This pipeline aims to prepare the data for meaningful analysis by addressing common issues such as missing values, outliers, and skewed distributions. Furthermore, feature engineering techniques are employed to better capture the relevant variables for subsequent statistical analysis.\nThe present assignment focuses on the importance of numerical data in the context of this study, discussing its background, related work, and techniques. Moreover, it provides considerations and cautions when dealing with numerical data in the realm of machine learning and ITS research. By examining the role of data preparation in the AutoTutor ARC study, this assignment contributes to the broader understanding of how data processing techniques can support the evaluation and improvement of ITS interventions."
  },
  {
    "objectID": "educ_6190_001/assignments/assignment_1/index.html#background-and-related-work",
    "href": "educ_6190_001/assignments/assignment_1/index.html#background-and-related-work",
    "title": "Enhancing Data Quality in Intelligent Tutoring Systems",
    "section": "Background and Related Work",
    "text": "Background and Related Work\n\nImportance of Numerical Data\nData mining involves preprocessing and transforming data to generate patterns using analysis tools and algorithms. Outlying variables in recorded data can significantly affect the robustness of a model, making the identification of outliers before data analysis essential (Awawdeh et al. 2019). Extreme values can cause problems in statistical analysis by increasing error variance, reducing statistical power, and biasing estimates. Therefore, screening data for extreme scores is crucial (Osborne 2012).\nData cleaning, which includes screening for extreme scores, missing data, and normality, is a critical step in quantitative research to ensure the validity of results. However, this step is often overlooked (Osborne 2012). Transformations are often necessary in data analysis to address issues such as non-normal distribution of errors in linear regression. These transformations can change the scale of variables, alter relationships between variables, and modify the error distributions (Pek, Wong, and Wong 2017).\n\n\nOutlier Detection\nOutliers can be detected using various methods, such as re-weighted least squares (Re-WLS). The bisquare weights method can be used to minimize the effect of outliers in least-squares fitting, where points far from the fitted line will get zero weight and be considered outliers. Outliers can be addressed by either deleting or transforming them, depending on the number of outliers present. Deleting outliers is suitable when there are a limited number of outliers, while transformation is used when there are many (Awawdeh et al. 2019).\nOutlier detection methods have been studied and applied in various fields, including intrusion detection, wireless sensor networks, satellite image analysis, motion segmentation, and weather prediction (Awawdeh et al. 2019).\n\n\nData Transformation\nData transformations are used to address non-normality. However, transformations can change the nature of the effect and should be applied carefully when interpreting effect sizes (Pek, Wong, and Wong 2017). Various transformations include logarithmic, square root, and reciprocal, with logarithmic transformations being particularly useful for compressing heavy-tailed distributions. The Box-Cox transformation is a family of power transforms that includes the log (Zheng and Casari 2018).\nWinsorizing and trimming are techniques used to address data contamination by replacing or removing extreme values. However, these transformations can bias effect size estimates when extreme cases are not outliers. Reverse transformations are not generally recommended because inferential results do not necessarily map back onto the original effect (Pek, Wong, and Wong 2017).\n\n\nFeature Scaling and Normalization\nFeature scaling, or normalization, is used to change the scale of the feature, which may be necessary for models sensitive to the input scale. Common scaling methods include min-max scaling, standardization, and L2 normalization (Zheng and Casari 2018). Min-max scaling squeezes data to be within a specific range, while standardization results in data with a mean of 0 and a variance of 1 (Awawdeh et al. 2019; Zheng and Casari 2018). Feature scaling does not change the shape of the distribution, only the scale (Zheng and Casari 2018).\n\n\nInteraction Features\nInteraction features are created by combining pairs of input features, allowing models to capture interactions between features (Zheng and Casari 2018).\n\n\nFeature Selection\nFeature selection techniques, such as filtering, wrapper, and embedded methods, are used to reduce the computational expense of using many features and to identify the most useful features (Zheng and Casari 2018).\n\n\nConsiderations and Cautions\nWhen dealing with numerical data, it is essential to consider that the magnitude of the data may or may not be important, depending on the situation. The distribution of numerical features matters for some models, and the Central Limit Theorem (CLT) makes the normality of errors less relevant when the sample size is large enough (Pek, Wong, and Wong 2017; Zheng and Casari 2018).\nTransformations should be used to improve effect size interpretation and to address non-normality when sample sizes are small (Pek, Wong, and Wong 2017). Data visualization is critical for understanding data and the effect of transformations (Zheng and Casari 2018). The choice of method depends on whether the goal is statistical prediction or statistical inference (Pek, Wong, and Wong 2017)."
  },
  {
    "objectID": "educ_6190_001/assignments/assignment_1/index.html#methods",
    "href": "educ_6190_001/assignments/assignment_1/index.html#methods",
    "title": "Enhancing Data Quality in Intelligent Tutoring Systems",
    "section": "Methods",
    "text": "Methods\nThe methodological framework for this project can be summarized in the following steps:\n\nData Ingestion and Preliminary Cleaning:\nRaw data is imported from an Excel file hosted on Google Drive. Unique identifiers (e.g., student IDs, lesson IDs) are used to group the data, ensuring that, for each participant, only the earliest non-null response is retained when multiple attempts are recorded.\nFeature Engineering and Labeling:\nA new variable is introduced to distinguish between pretest and posttest conditions, based on the lesson identifier. In addition, specific columns are renamed for clarity (e.g., renaming Q5Duration to WritingTime).\nVisualization and Descriptive Analysis:\nHistograms and summary statistics are generated to examine the distribution of the writing time data, revealing the presence of outliers and a highly skewed distribution.\nOutlier Treatment and Data Transformation:\nOutliers are identified using a three-standard-deviation rule and are replaced with lesson-specific means. A logarithmic transformation is then applied to reduce the skewness of the writing time variable, resulting in a distribution more amenable to parametric analysis.\nFeature Scaling:\nThe writing time variable is normalized using min-max scaling to facilitate further analysis and potential machine learning applications.\nData Quality Assurance and Aggregation:\nDuplicate records and missing values are identified and addressed. Finally, the dataset is aggregated by test condition (pretest vs. posttest) to generate summary statistics that inform subsequent analyses."
  },
  {
    "objectID": "educ_6190_001/assignments/assignment_1/index.html#implementation-details",
    "href": "educ_6190_001/assignments/assignment_1/index.html#implementation-details",
    "title": "Enhancing Data Quality in Intelligent Tutoring Systems",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nData Preprocessing: Handling Multiple Attempts\nThe core preprocessing is encapsulated in the process_log_data function. This function takes a raw DataFrame, groups records by unique identifiers (e.g., student and lesson IDs), and then selects the first non-null response for each repeated measure (such as responses to multiple questions or their corresponding durations).\n\ndef process_log_data(df):\n    \"\"\"\n    Process log data by extracting first attempts and handling missing values\n    by looking at subsequent attempts.\n    \"\"\"\n    # Define identifier and repeated columns\n    base_cols = ['RecordID', 'ClassID', 'UserID', 'LessonID']\n    attempt_cols = ['LessonAttempt', 'TotalTime', 'XML']\n    data_cols = [f'Q{i}Data' for i in range(1, 10)]\n    duration_cols = [f'Q{i}Duration' for i in range(1, 10)]\n\n    # List to store processed rows\n    processed_data = []\n\n    # Group the data by the base columns (unique combination per record)\n    grouped = df.groupby(base_cols)\n\n    for name, group in grouped:\n        # Start with the key identifiers\n        row_dict = dict(zip(base_cols, name))\n\n        # For each data and duration column, pick the first non-null attempt\n        for col in data_cols + duration_cols:\n            row_dict[col] = group[col].iloc[0]  # Take the first attempt\n            if pd.isna(row_dict[col]):\n                # If missing, search subsequent attempts\n                for attempt in range(1, len(group)):\n                    if not pd.isna(group[col].iloc[attempt]):\n                        row_dict[col] = group[col].iloc[attempt]\n                        break\n\n        # Also add attempt-specific columns\n        row_dict['LessonAttempt'] = group['LessonAttempt'].iloc[0]\n        row_dict['TotalTime'] = group['TotalTime'].iloc[0]\n        row_dict['XML'] = group['XML'].iloc[0]\n\n        processed_data.append(row_dict)\n\n    # Convert the list of dictionaries into a DataFrame with a specified column order\n    result_df = pd.DataFrame(processed_data)\n    column_order = base_cols + ['LessonAttempt', 'TotalTime', 'XML'] + data_cols + duration_cols\n\n    return result_df[column_order]\n\nKey aspects of the function include:\n\nGrouping: Data is grouped by RecordID, ClassID, UserID, and LessonID to handle multiple attempts by the same student.\nIteration: For each group, the function selects the first non-null value for each data and duration column, preserving the earliest response.\nOutput: The function returns a cleaned DataFrame with a single row per unique record, with logically ordered columns.\n\n\n\nLoading Data\nIn this segment, the code reads an Excel file containing the log data. This file holds data from multiple lessons that are later used to compare pretest and posttest performance.\n\nimport pandas as pd\n\npd.set_option('display.max_columns', None)\n\nsummary_log_data = pd.read_excel(\n    '/Users/john/Library/CloudStorage/Box-Box/Website/educ_6190_001/assignments/assignment_1/mnt/data/summary_data_-_lesson_2-3-9-10.xlsx',\n    sheet_name='Sheet1'\n)\n\n\n\nApplying the Processing Function and Creating a Test Variable\nThe raw data is processed using our custom function.\n\nprocessed_df = process_log_data(summary_log_data)\nprocessed_df.head()\n\n\n\n\n\n\n\n\nRecordID\nClassID\nUserID\nLessonID\nLessonAttempt\nTotalTime\nXML\nQ1Data\nQ2Data\nQ3Data\nQ4Data\nQ5Data\nQ6Data\nQ7Data\nQ8Data\nQ9Data\nQ1Duration\nQ2Duration\nQ3Duration\nQ4Duration\nQ5Duration\nQ6Duration\nQ7Duration\nQ8Duration\nQ9Duration\n\n\n\n\n0\n1\nformal\nformal5_02\nlesson2\n1\n379.071\nLesson2-Flood.xml\nNext\nArousal8_Pleasant8\n2.0\n3.0\nFloods are one of the most common natural disa...\n4.0\n6.0\n3.0\n3.0\n72.531\n7.484\n19.874\n8.031\n177.328\n6.750\n23.859\n35.843\n15.156\n\n\n1\n2\nformal\nformal5_02\nlesson3\n1\n358.535\nLesson3-Hurricane.xml\nNext\nArousal9_Pleasant5\n1.0\n1.0\nThe two most destructive hurricanes to ever hi...\n6.0\n2.0\n1.0\n6.0\n119.062\n4.593\n4.999\n3.890\n119.484\n3.000\n28.343\n20.140\n40.203\n\n\n2\n3\nformal\nformal5_02\nlesson9\n1\n452.599\nLesson9-Job.xml\nNext\nArousal9_Pleasant5\n1.0\n5.0\nThe United States job market has deteriorated....\n3.0\n4.0\n5.0\n2.0\n71.531\n6.796\n2.687\n3.828\n202.812\n4.609\n97.828\n25.796\n24.296\n\n\n3\n4\nformal\nformal5_02\nlesson10\n1\n395.415\nLesson10-Butterfly.xml\nNext\nArousal5_Pleasant5\n2.0\n3.0\nThe butterfly and the moth have a lot of thing...\n3.0\n1.0\n4.0\n6.0\n73.031\n6.078\n4.078\n3.890\n227.796\n3.718\n22.093\n21.078\n21.031\n\n\n4\n5\nformal\nformal16_02\nlesson2\n1\n537.568\nLesson2-Flood.xml\nNext\nArousal7_Pleasant3\n2.0\n3.0\nFlood is one of the natural disaster that crea...\n3.0\n4.0\n5.0\n4.0\n27.437\n14.343\n19.828\n6.968\n377.781\n7.625\n6.765\n21.375\n14.875\n\n\n\n\n\n\n\nAfter processing the raw data with the custom function, a new column (test) is created to distinguish between pretest (lessons 2 and 3) and posttest (lessons 9 and 10) conditions. The mapping is verified by printing the distribution of the test types and a sample of the processed data.\n\nprocessed_df['test'] = processed_df['LessonID'].map({\n    'lesson2': 'pretest',\n    'lesson3': 'pretest',\n    'lesson9': 'posttest',\n    'lesson10': 'posttest'\n})\n\nprint(\"\\nDistribution of test types:\")\nprint(processed_df['test'].value_counts())\n\nprint(\"\\nSample of processed data with test variable:\")\nprint(processed_df[['LessonID', 'test']].head(10))\n\n\nDistribution of test types:\ntest\nposttest    386\npretest     381\nName: count, dtype: int64\n\nSample of processed data with test variable:\n   LessonID      test\n0   lesson2   pretest\n1   lesson3   pretest\n2   lesson9  posttest\n3  lesson10  posttest\n4   lesson2   pretest\n5   lesson3   pretest\n6   lesson9  posttest\n7  lesson10  posttest\n8   lesson2   pretest\n9   lesson3   pretest\n\n\n\n\nRenaming and Visualizing the Writing Time Variable\nFor clarity, the column Q5Duration is renamed to WritingTime. A histogram is then plotted to visualize the distribution of writing time, with vertical lines indicating the mean and the thresholds defined by three standard deviations.\n\nfrom matplotlib import pyplot as plt\n\n# Rename for clarity\nprocessed_df = processed_df.rename(columns={'Q5Duration': 'WritingTime'})\n\n# Plot histogram with mean and ±3 standard deviations\nplt.figure(figsize=(12, 6))\nplt.hist(processed_df['WritingTime'], bins=30, density=True, alpha=0.7)\nplt.axvline(processed_df['WritingTime'].mean(), color='red', linestyle='dashed', linewidth=1, label='Mean')\nplt.axvline(processed_df['WritingTime'].mean() + 3*processed_df['WritingTime'].std(),\n            color='green', linestyle='dashed', linewidth=1, label='3 SD Above Mean')\nplt.axvline(processed_df['WritingTime'].mean() - 3*processed_df['WritingTime'].std(),\n            color='green', linestyle='dashed', linewidth=1, label='3 SD Below Mean')\nplt.xlabel('Writing Time (seconds)')\nplt.ylabel('Density')\nplt.title('Distribution of Writing Time')\nplt.legend()\nplt.show()\n\n(array([9.57302808e-04, 1.51315605e-03, 8.77012895e-04, 4.01449564e-04,\n        2.09989003e-04, 9.88183543e-05, 4.32330300e-05, 3.08807357e-05,\n        3.70568829e-05, 4.94091772e-05, 1.85284414e-05, 1.23522943e-05,\n        6.17614715e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        6.17614715e-06, 0.00000000e+00, 1.23522943e-05, 0.00000000e+00,\n        6.17614715e-06, 0.00000000e+00, 6.17614715e-06, 0.00000000e+00,\n        6.17614715e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 6.17614715e-06]),\n array([1.26500000e+00, 2.33898967e+02, 4.66532933e+02, 6.99166900e+02,\n        9.31800867e+02, 1.16443483e+03, 1.39706880e+03, 1.62970277e+03,\n        1.86233673e+03, 2.09497070e+03, 2.32760467e+03, 2.56023863e+03,\n        2.79287260e+03, 3.02550657e+03, 3.25814053e+03, 3.49077450e+03,\n        3.72340847e+03, 3.95604243e+03, 4.18867640e+03, 4.42131037e+03,\n        4.65394433e+03, 4.88657830e+03, 5.11921227e+03, 5.35184623e+03,\n        5.58448020e+03, 5.81711417e+03, 6.04974813e+03, 6.28238210e+03,\n        6.51501607e+03, 6.74765003e+03, 6.98028400e+03]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\nText(0.5, 0, 'Writing Time (seconds)')\n\n\nText(0, 0.5, 'Density')\n\n\nText(0.5, 1.0, 'Distribution of Writing Time')\n\n\n\n\n\n\n\n\n\n\nprint(\"\\nWriting Time Summary Statistics:\")\nprint(processed_df['WritingTime'].describe())\nprint(f\"\\nSkewness: {processed_df['WritingTime'].skew():.2f}\")\n\n\nWriting Time Summary Statistics:\ncount     696.000000\nmean      572.174059\nstd       630.892576\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       644.047000\nmax      6980.284000\nName: WritingTime, dtype: float64\n\nSkewness: 4.77\n\n\n\n\nOutlier Replacement and Log Transformation\nTo address skewness and outliers in the WritingTime variable, the code: - Computes acceptable bounds (mean ± 3 standard deviations). - Replaces values outside these bounds with the lesson-specific mean. - Applies a logarithmic transformation (using np.log(x + 1)) to compress the scale of higher values and stabilize variance.\nDescriptive statistics and skewness values are compared across the original, cleaned, and log-transformed data, demonstrating the effectiveness of these preprocessing steps.\n\nimport numpy as np\n\n# Calculate mean, standard deviation, and bounds for outlier detection\nmean = processed_df['WritingTime'].mean()\nstd = processed_df['WritingTime'].std()\nlower_bound = mean - 3 * std\nupper_bound = mean + 3 * std\n\n# Preserve the original writing times\nprocessed_df['WritingTime_original'] = processed_df['WritingTime']\n\n# Replace outliers with the mean writing time of the corresponding lesson\nlesson_means = processed_df.groupby('LessonID')['WritingTime'].transform('mean')\nmask = (processed_df['WritingTime'] &lt; lower_bound) | (processed_df['WritingTime'] &gt; upper_bound)\nprocessed_df.loc[mask, 'WritingTime'] = lesson_means[mask]\n\n# Apply a log transformation to the cleaned writing times\nprocessed_df['WritingTime_log'] = np.log(processed_df['WritingTime'] + 1)\n\n# Display statistics for each transformation stage\nprint(\"Original WritingTime Statistics:\")\nprint(processed_df['WritingTime_original'].describe())\nprint(\"\\nSkewness (original):\", processed_df['WritingTime_original'].skew())\n\nprint(\"\\nCleaned WritingTime Statistics:\")\nprint(processed_df['WritingTime'].describe())\nprint(\"\\nSkewness (cleaned):\", processed_df['WritingTime'].skew())\n\nprint(\"\\nLog-transformed WritingTime Statistics:\")\nprint(processed_df['WritingTime_log'].describe())\nprint(\"\\nSkewness (log-transformed):\", processed_df['WritingTime_log'].skew())\n\nOriginal WritingTime Statistics:\ncount     696.000000\nmean      572.174059\nstd       630.892576\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       644.047000\nmax      6980.284000\nName: WritingTime_original, dtype: float64\n\nSkewness (original): 4.774762070142547\n\nCleaned WritingTime Statistics:\ncount     696.000000\nmean      512.355243\nstd       391.086600\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       626.499750\nmax      2375.579000\nName: WritingTime, dtype: float64\n\nSkewness (cleaned): 2.0766269712768888\n\nLog-transformed WritingTime Statistics:\ncount    696.000000\nmean       5.979808\nstd        0.782562\nmin        0.817575\n25%        5.532290\n50%        6.007293\n75%        6.441735\nmax        7.773417\nName: WritingTime_log, dtype: float64\n\nSkewness (log-transformed): -1.0692825002709074\n\n\n\n\nVisual Comparison of Distribution Transformations\nA three-panel plot compares the original, cleaned, and log-transformed distributions side by side. This visual comparison highlights: - The strong positive skew in the original data. - The reduction of extreme values in the cleaned data. - The near-symmetric distribution achieved after log transformation.\n\nplt.figure(figsize=(15, 5))\n\n# Original data distribution\nplt.subplot(131)\nplt.hist(processed_df['WritingTime_original'], bins=30, alpha=0.7)\nplt.title('Original WritingTime')\nplt.xlabel('Seconds')\nplt.ylabel('Frequency')\n\n# Cleaned data distribution (with outliers replaced)\nplt.subplot(132)\nplt.hist(processed_df['WritingTime'], bins=30, alpha=0.7)\nplt.title('Cleaned WritingTime\\n(Outliers Replaced)')\nplt.xlabel('Seconds')\n\n# Log-transformed distribution\nplt.subplot(133)\nplt.hist(processed_df['WritingTime_log'], bins=30, alpha=0.7)\nplt.title('Log-transformed WritingTime')\nplt.xlabel('Log(Seconds)')\n\nplt.tight_layout()\nplt.show()\n\n(array([155., 245., 142.,  65.,  34.,  16.,   7.,   5.,   6.,   8.,   3.,\n          2.,   1.,   0.,   0.,   0.,   1.,   0.,   2.,   0.,   1.,   0.,\n          1.,   0.,   1.,   0.,   0.,   0.,   0.,   1.]),\n array([1.26500000e+00, 2.33898967e+02, 4.66532933e+02, 6.99166900e+02,\n        9.31800867e+02, 1.16443483e+03, 1.39706880e+03, 1.62970277e+03,\n        1.86233673e+03, 2.09497070e+03, 2.32760467e+03, 2.56023863e+03,\n        2.79287260e+03, 3.02550657e+03, 3.25814053e+03, 3.49077450e+03,\n        3.72340847e+03, 3.95604243e+03, 4.18867640e+03, 4.42131037e+03,\n        4.65394433e+03, 4.88657830e+03, 5.11921227e+03, 5.35184623e+03,\n        5.58448020e+03, 5.81711417e+03, 6.04974813e+03, 6.28238210e+03,\n        6.51501607e+03, 6.74765003e+03, 6.98028400e+03]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\nText(0.5, 1.0, 'Original WritingTime')\n\n\nText(0.5, 0, 'Seconds')\n\n\nText(0, 0.5, 'Frequency')\n\n\n(array([21., 40., 99., 74., 99., 71., 68., 58., 31., 24., 24., 14., 13.,\n        10., 11.,  3.,  6.,  3.,  5.,  1.,  3.,  0.,  2.,  3.,  3.,  1.,\n         1.,  1.,  5.,  2.]),\n array([1.2650000e+00, 8.0408800e+01, 1.5955260e+02, 2.3869640e+02,\n        3.1784020e+02, 3.9698400e+02, 4.7612780e+02, 5.5527160e+02,\n        6.3441540e+02, 7.1355920e+02, 7.9270300e+02, 8.7184680e+02,\n        9.5099060e+02, 1.0301344e+03, 1.1092782e+03, 1.1884220e+03,\n        1.2675658e+03, 1.3467096e+03, 1.4258534e+03, 1.5049972e+03,\n        1.5841410e+03, 1.6632848e+03, 1.7424286e+03, 1.8215724e+03,\n        1.9007162e+03, 1.9798600e+03, 2.0590038e+03, 2.1381476e+03,\n        2.2172914e+03, 2.2964352e+03, 2.3755790e+03]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\nText(0.5, 1.0, 'Cleaned WritingTime\\n(Outliers Replaced)')\n\n\nText(0.5, 0, 'Seconds')\n\n\n(array([ 1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  2.,  3.,  3.,\n         5.,  3.,  1.,  7., 19., 43., 64., 58., 93., 97., 93., 79., 48.,\n        33., 19.,  9., 13.]),\n array([0.81757476, 1.04943618, 1.2812976 , 1.51315902, 1.74502044,\n        1.97688186, 2.20874327, 2.44060469, 2.67246611, 2.90432753,\n        3.13618895, 3.36805037, 3.59991179, 3.83177321, 4.06363463,\n        4.29549605, 4.52735747, 4.75921889, 4.99108031, 5.22294173,\n        5.45480314, 5.68666456, 5.91852598, 6.1503874 , 6.38224882,\n        6.61411024, 6.84597166, 7.07783308, 7.3096945 , 7.54155592,\n        7.77341734]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\nText(0.5, 1.0, 'Log-transformed WritingTime')\n\n\nText(0.5, 0, 'Log(Seconds)')\n\n\n\n\n\n\n\n\n\n\n\nFeature Scaling: Min-Max Normalization\nThe cleaned WritingTime variable is scaled to a [0, 1] range using min-max normalization. Although this step does not alter the distribution’s skewness, it standardizes the data for algorithms that require features on a similar scale.\n\nprocessed_df['WritingTime_scale'] = (\n    (processed_df['WritingTime'] - processed_df['WritingTime'].min()) /\n    (processed_df['WritingTime'].max() - processed_df['WritingTime'].min())\n)\n\nprint(\"Original WritingTime Statistics:\")\nprint(processed_df['WritingTime'].describe())\nprint(\"\\nSkewness (original):\", processed_df['WritingTime'].skew())\n\nprint(\"\\nMin-Max Scaled WritingTime Statistics:\")\nprint(processed_df['WritingTime_scale'].describe())\nprint(\"\\nSkewness (scaled):\", processed_df['WritingTime_scale'].skew())\n\nOriginal WritingTime Statistics:\ncount     696.000000\nmean      512.355243\nstd       391.086600\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       626.499750\nmax      2375.579000\nName: WritingTime, dtype: float64\n\nSkewness (original): 2.0766269712768888\n\nMin-Max Scaled WritingTime Statistics:\ncount    696.000000\nmean       0.215258\nstd        0.164716\nmin        0.000000\n25%        0.105486\n50%        0.170204\n75%        0.263333\nmax        1.000000\nName: WritingTime_scale, dtype: float64\n\nSkewness (scaled): 2.076626971276889\n\n\nA side-by-side histogram confirms that the underlying distribution shape remains unchanged after scaling.\n\nplt.figure(figsize=(12, 5))\n\n# Original distribution\nplt.subplot(121)\nplt.hist(processed_df['WritingTime'], bins=30, alpha=0.7)\nplt.title('Original WritingTime')\nplt.xlabel('Seconds')\nplt.ylabel('Frequency')\n\n# Scaled distribution\nplt.subplot(122)\nplt.hist(processed_df['WritingTime_scale'], bins=30, alpha=0.7)\nplt.title('Min-Max Scaled WritingTime')\nplt.xlabel('Scaled Value (0-1)')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n(array([21., 40., 99., 74., 99., 71., 68., 58., 31., 24., 24., 14., 13.,\n        10., 11.,  3.,  6.,  3.,  5.,  1.,  3.,  0.,  2.,  3.,  3.,  1.,\n         1.,  1.,  5.,  2.]),\n array([1.2650000e+00, 8.0408800e+01, 1.5955260e+02, 2.3869640e+02,\n        3.1784020e+02, 3.9698400e+02, 4.7612780e+02, 5.5527160e+02,\n        6.3441540e+02, 7.1355920e+02, 7.9270300e+02, 8.7184680e+02,\n        9.5099060e+02, 1.0301344e+03, 1.1092782e+03, 1.1884220e+03,\n        1.2675658e+03, 1.3467096e+03, 1.4258534e+03, 1.5049972e+03,\n        1.5841410e+03, 1.6632848e+03, 1.7424286e+03, 1.8215724e+03,\n        1.9007162e+03, 1.9798600e+03, 2.0590038e+03, 2.1381476e+03,\n        2.2172914e+03, 2.2964352e+03, 2.3755790e+03]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\nText(0.5, 1.0, 'Original WritingTime')\n\n\nText(0.5, 0, 'Seconds')\n\n\nText(0, 0.5, 'Frequency')\n\n\n(array([21., 40., 99., 74., 99., 71., 68., 58., 31., 24., 24., 14., 13.,\n        10., 11.,  3.,  6.,  3.,  5.,  1.,  3.,  0.,  2.,  3.,  3.,  1.,\n         1.,  1.,  5.,  2.]),\n array([0.        , 0.03333333, 0.06666667, 0.1       , 0.13333333,\n        0.16666667, 0.2       , 0.23333333, 0.26666667, 0.3       ,\n        0.33333333, 0.36666667, 0.4       , 0.43333333, 0.46666667,\n        0.5       , 0.53333333, 0.56666667, 0.6       , 0.63333333,\n        0.66666667, 0.7       , 0.73333333, 0.76666667, 0.8       ,\n        0.83333333, 0.86666667, 0.9       , 0.93333333, 0.96666667,\n        1.        ]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\nText(0.5, 1.0, 'Min-Max Scaled WritingTime')\n\n\nText(0.5, 0, 'Scaled Value (0-1)')\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\n\n\nData Quality Assurance and Aggregation\nAdditional steps include: - Checking for and confirming the absence of duplicate records.\n\n# Check duplicates\nprint(\"Number of duplicates:\", processed_df.duplicated().sum())\n\nNumber of duplicates: 0\n\n\n\nIdentifying missing values and removing rows with incomplete key columns.\n\n\n# Check missing values\nprint(\"\\nMissing values in relevant columns:\")\nprint(processed_df[['WritingTime', 'WritingTime_log', 'WritingTime_scale', 'test']].isnull().sum())\n# Remove rows with missing values if any exist\nprocessed_df = processed_df.dropna(subset=['WritingTime', 'WritingTime_log', 'WritingTime_scale', 'test'])\n\n\nMissing values in relevant columns:\nWritingTime          71\nWritingTime_log      71\nWritingTime_scale    71\ntest                  0\ndtype: int64\n\n\n\nAggregating summary statistics by test condition (pretest vs. posttest).\n\n\ngrouped_stats = processed_df.groupby('test').agg({\n    'WritingTime_log': ['mean', 'std'],\n    'WritingTime_scale': ['mean', 'std']\n}).round(3)\n\nprint(\"\\nDescriptive Statistics by Test Group:\")\nprint(grouped_stats)\n\n\nDescriptive Statistics by Test Group:\n         WritingTime_log        WritingTime_scale       \n                    mean    std              mean    std\ntest                                                    \nposttest           6.007  0.786             0.224  0.176\npretest            5.953  0.779             0.207  0.153\n\n\n\nSaving the processed dataset as a CSV file for future analysis.\n\n\n# Save the processed DataFrame for future analysis\nprocessed_df.to_csv('assign1_summary_log.csv', index=False)"
  },
  {
    "objectID": "educ_6190_001/assignments/assignment_1/index.html#conclusion",
    "href": "educ_6190_001/assignments/assignment_1/index.html#conclusion",
    "title": "Enhancing Data Quality in Intelligent Tutoring Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThis project implements a comprehensive data processing pipeline designed to prepare raw log data for statistical analysis and machine learning applications. The key steps include:\n\nImporting raw data from an Excel file.\nProcessing multiple attempts per student by grouping and selecting the first non-null response.\nCreating a test label to distinguish between pretest and posttest conditions.\nRenaming and visualizing a key performance metric (WritingTime).\nIdentifying and replacing outliers, followed by a log transformation to reduce skewness.\nNormalizing the data using min-max scaling.\nEnsuring data quality by checking for duplicates and handling missing values.\nAggregating summary statistics by test type and saving the cleaned dataset.\n\nThis systematic approach not only cleans and prepares the data but also enhances its suitability for subsequent statistical tests and modeling. Ultimately, the refined data supports more accurate and meaningful conclusions about the effects of conversational agent language style on learning outcomes within ITS environments."
  }
]