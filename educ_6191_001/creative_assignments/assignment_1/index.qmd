---
title: "Behavior Detection"
description: "A machine learning model to detect off-task behavior"
author:
  - name: John Baker
    affiliations:
      - name: "Penn GSE: University of Pennsylvania Graduate School of Education"
lang: en
date: 09-18-2024
# date-modified: 09-07-2024
format:
  html:
    code-link: false
draft: true
jupyter: python3
---

This is a Quarto website!

To learn more about Quarto websites visit <https://quarto.org/docs/websites>!

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, cohen_kappa_score
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

# Load the dataset
data = pd.read_csv('data/ca1-dataset.csv')

# Prepare the data
data['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable
X = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features
y = data['OffTask']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Calculate the ratio of classes
class_0_count = sum(y_train_resampled == 0)
class_1_count = sum(y_train_resampled == 1)
ratio_of_classes = class_0_count / class_1_count

# Define the model
model = RandomForestClassifier(random_state=42, class_weight='balanced')

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Set up GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, 
                           scoring='f1', cv=5, n_jobs=-1, verbose=2)

# Fit GridSearchCV
grid_search.fit(X_train_resampled, y_train_resampled)

# Best parameters
print("Best parameters found: ", grid_search.best_params_)
```

```{python}
# Train the model on the resampled data
model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
kappa = cohen_kappa_score(y_test, y_pred)
print("Kappa Score:", kappa)
print(classification_report(y_test, y_pred))
```

```{python}
# Train the XGBoost model without the use_label_encoder parameter
xgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
kappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)
print("Kappa Score (XGBoost):", kappa_xgb)
print(classification_report(y_test, y_pred_xgb))
```

```{python}
# Define the Gradient Boosting model
gb_model = GradientBoostingClassifier(
    learning_rate=0.2,
    max_depth=5,
    min_samples_split=10,
    n_estimators=200,
    random_state=42
)

# Fit the model on the resampled training data
gb_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
kappa_gb = cohen_kappa_score(y_test, y_pred_gb)
print("Kappa Score (Gradient Boosting):", kappa_gb)
print(classification_report(y_test, y_pred_gb))
```

```{python}
# Get predicted probabilities
y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]

# Set a new threshold
threshold = 0.3  # Example threshold
y_pred_adjusted_gb = (y_pred_proba_gb >= threshold).astype(int)

# Evaluate the model with the adjusted predictions
kappa_adjusted_gb = cohen_kappa_score(y_test, y_pred_adjusted_gb)
print("Adjusted Kappa Score (Gradient Boosting):", kappa_adjusted_gb)
print(classification_report(y_test, y_pred_adjusted_gb))
```

```{python}
# Experiment with different thresholds
thresholds = np.arange(0.0, 1.0, 0.05)
precisions = []
recalls = []
kappa_scores = []

for threshold in thresholds:
    y_pred_adjusted = (y_pred_proba_gb >= threshold).astype(int)
    
    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) > 0 else 0
    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) > 0 else 0
    
    kappa = cohen_kappa_score(y_test, y_pred_adjusted)
    
    precisions.append(precision)
    recalls.append(recall)
    kappa_scores.append(kappa)

plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, label='Precision', marker='o')
plt.plot(thresholds, recalls, label='Recall', marker='o')
plt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')
plt.title('Precision, Recall, and Kappa Score vs. Threshold')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.xticks(np.arange(0.0, 1.1, 0.1))
plt.legend()
plt.grid()
plt.show()

best_threshold_index = np.argmax(recalls)
best_threshold = thresholds[best_threshold_index]
print(f"Best Threshold for Maximum Recall: {best_threshold:.2f}")
print(f"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}")
print(f"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}")
print(f"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}")
```

```{python}
# Initialize lists to store precision, recall, and F1-score values
f1_scores = []

# Calculate precision, recall, and F1-score for each threshold
for threshold in thresholds:
    y_pred_adjusted = (y_pred_proba_gb >= threshold).astype(int)
    
    # Calculate precision and recall
    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) > 0 else 0
    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) > 0 else 0
    
    # Calculate F1-score
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # Append F1-score to the list
    f1_scores.append(f1_score)

# Plot Precision, Recall, and F1-Score curve
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, label='Precision', marker='o')
plt.plot(thresholds, recalls, label='Recall', marker='o')
plt.plot(thresholds, f1_scores, label='F1 Score', marker='o')
plt.title('Precision, Recall, and F1 Score vs. Threshold')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.xticks(np.arange(0.0, 1.1, 0.1))
plt.legend()
plt.grid()
plt.show()

# Print the best threshold based on maximum F1-score
best_threshold_index = np.argmax(f1_scores)
best_threshold = thresholds[best_threshold_index]
print(f"Best Threshold for Maximum F1-Score: {best_threshold:.2f}")
print(f"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}")
print(f"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}")
print(f"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}")
```

```{python}
# Make predictions using the new threshold
y_pred_final = (gb_model.predict_proba(X_test)[:, 1] >= 0.90).astype(int)

# Evaluate the model with the new predictions
kappa_final = cohen_kappa_score(y_test, y_pred_final)
print("Final Kappa Score with Threshold 0.90:", kappa_final)

# Print the classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_final))
```

```{python}
# Optionally, you can also calculate and print confusion matrix
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred_final)
print("Confusion Matrix:\n", conf_matrix)

# Visualize the confusion matrix (optional)
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Not OffTask (0)', 'OffTask (1)'], 
            yticklabels=['Not OffTask (0)', 'OffTask (1)'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Perform k-fold cross-validation
cv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')  # You can change scoring to 'accuracy', 'precision', etc.

# Print the cross-validation scores
print("Cross-Validation F1 Scores:", cv_scores)
print("Mean F1 Score:", np.mean(cv_scores))
print("Standard Deviation of F1 Scores:", np.std(cv_scores))
```
