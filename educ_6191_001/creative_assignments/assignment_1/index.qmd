---
title: "Analyzing and Optimizing Machine Learning Models for Off-Task Behavior Detection"
description: "A machine learning model to detect off-task behavior"
author:
  - name: John Baker
    affiliations:
      - name: "Penn GSE: University of Pennsylvania Graduate School of Education"
date: 09-18-2024
# date-modified: 
format:
  html:
    code-link: false
draft: false
jupyter: python3
---

## Abstract

Detecting off-task behavior in educational settings is essential for enhancing student engagement and learning outcomes. This study presents a comprehensive analysis of machine learning models designed to classify off-task behavior using a real-world educational dataset. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through a series of experiments involving data resampling, hyperparameter tuning, threshold optimization, and performance evaluation using metrics like Cohen's Kappa score, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts.

## Introduction

In the field of educational data mining, detecting off-task behavior is crucial for understanding student engagement and improving learning outcomes. Off-task behavior refers to any student actions that are not related to the learning objectives, which can hinder the educational process. Traditional methods of identifying off-task behavior are often subjective and resource-intensive. Therefore, developing automated, accurate detection methods using machine learning can significantly benefit educators and learners.

This paper presents an in-depth analysis of machine learning models designed to classify off-task behavior in educational settings. I explore the challenges of working with imbalanced datasets and evaluate the performance of various classifiers, including Random Forest, XGBoost, and Gradient Boosting. Through experiments and analyses, I aim to optimize model performance and provide insights into the complexities of behavior detection in educational contexts.

## Methodology

Our study employed a multi-step approach to develop and evaluate machine learning models:

1. **Data Preparation**: I utilized a dataset containing features related to student behavior, with a binary target variable indicating off-task status (`OffTask`: Y/N).
2. **Model Selection**: I implemented three classifiers: Random Forest, XGBoost, and Gradient Boosting.
3. **Handling Class Imbalance**: To address the imbalanced nature of the dataset, I applied the Synthetic Minority Over-sampling Technique (SMOTE).
4. **Hyperparameter Tuning**: I used `GridSearchCV` to optimize model parameters, focusing on maximizing the F1-score.
5. **Threshold Optimization**: I explored various decision thresholds to balance precision and recall, particularly for the minority class (off-task behavior).
6. **Performance Evaluation**: I assessed model performance using metrics such as Cohen's Kappa score, precision, recall, F1-score, and confusion matrices.
7. **Cross-Validation**: I employed k-fold cross-validation to ensure robust performance estimates across different data subsets.

### Data Preparation

I began by importing necessary libraries and loading the dataset:

```{python}
# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, cohen_kappa_score, confusion_matrix
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('data/ca1-dataset.csv')
```

I then prepared the data by encoding the target variable and selecting relevant features:

```{python}
# Prepare the data
data['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable
X = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features
y = data['OffTask']  # Target variable
```

### Handling Class Imbalance with SMOTE

The dataset exhibited class imbalance, with significantly more instances of "Not OffTask" than "OffTask." I applied SMOTE to the training data to address this issue:

```{python}
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Calculate the ratio of classes
class_0_count = sum(y_train_resampled == 0)
class_1_count = sum(y_train_resampled == 1)
ratio_of_classes = class_0_count / class_1_count
```

### Model Selection and Hyperparameter Tuning

#### Random Forest Classifier

I defined the Random Forest model and set up a parameter grid for hyperparameter tuning:

```{python}
# Define the model
model_rf = RandomForestClassifier(random_state=42, class_weight='balanced')

# Define the parameter grid
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Set up GridSearchCV with corrected parameter names and variables
grid_search_rf = GridSearchCV(estimator=model_rf, param_grid=param_grid_rf,
                              scoring='f1', cv=5, n_jobs=-1, verbose=2)

# Fit GridSearchCV
grid_search_rf.fit(X_train_resampled, y_train_resampled)

# Best parameters
print("Best parameters found for Random Forest: ", grid_search_rf.best_params_)
```

#### XGBoost Classifier

I initialized the XGBoost model, adjusting for class imbalance using `scale_pos_weight`:

```{python}
# Define the XGBoost model
xgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes, use_label_encoder=False)

# Fit the model
xgb_model.fit(X_train_resampled, y_train_resampled)
```

#### Gradient Boosting Classifier

I defined the Gradient Boosting model with specific hyperparameters:

```{python}
# Define the Gradient Boosting model
gb_model = GradientBoostingClassifier(
    learning_rate=0.2,
    max_depth=5,
    min_samples_split=10,
    n_estimators=200,
    random_state=42
)

# Fit the model on the resampled training data
gb_model.fit(X_train_resampled, y_train_resampled)
```

### Performance Evaluation

I evaluated each model using the test set and calculated the Cohen's Kappa score and classification report.

#### Random Forest Evaluation

```{python}
# Make predictions on the test set
y_pred_rf = grid_search_rf.predict(X_test)

# Evaluate the model
kappa_rf = cohen_kappa_score(y_test, y_pred_rf)
print("Kappa Score (Random Forest):", kappa_rf)
print(classification_report(y_test, y_pred_rf))
```

#### XGBoost Evaluation

```{python}
# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
kappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)
print("Kappa Score (XGBoost):", kappa_xgb)
print(classification_report(y_test, y_pred_xgb))
```

#### Gradient Boosting Evaluation

```{python}
# Make predictions on the test set
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
kappa_gb = cohen_kappa_score(y_test, y_pred_gb)
print("Kappa Score (Gradient Boosting):", kappa_gb)
print(classification_report(y_test, y_pred_gb))
```

### Threshold Optimization

To improve the detection of off-task behavior, I experimented with adjusting the decision threshold:

```{python}
# Get predicted probabilities
y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]

# Experiment with different thresholds
thresholds = np.arange(0.0, 1.0, 0.05)
precisions = []
recalls = []
kappa_scores = []

for threshold in thresholds:
    y_pred_adjusted = (y_pred_proba_gb >= threshold).astype(int)
    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) > 0 else 0
    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) > 0 else 0
    kappa = cohen_kappa_score(y_test, y_pred_adjusted)
    precisions.append(precision)
    recalls.append(recall)
    kappa_scores.append(kappa)

# Plot Precision, Recall, and Kappa Score vs. Threshold
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, label='Precision', marker='o')
plt.plot(thresholds, recalls, label='Recall', marker='o')
plt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')
plt.title('Precision, Recall, and Kappa Score vs. Threshold')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.xticks(np.arange(0.0, 1.1, 0.1))
plt.legend()
plt.grid()
plt.show()
```

I determined that a threshold of 0.90 maximized the F1-score for the off-task class.

```{python}
# Apply the optimal threshold
best_threshold = 0.90
y_pred_final = (gb_model.predict_proba(X_test)[:, 1] >= best_threshold).astype(int)

# Evaluate the model with the new predictions
kappa_final = cohen_kappa_score(y_test, y_pred_final)
print("Final Kappa Score with Threshold 0.90:", kappa_final)
print(classification_report(y_test, y_pred_final))
```

### Confusion Matrix and Cross-Validation

I computed the confusion matrix and performed k-fold cross-validation to assess model stability:

```{python}
# Calculate and print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_final)
print("Confusion Matrix:\n", conf_matrix)

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not OffTask (0)', 'OffTask (1)'],
            yticklabels=['Not OffTask (0)', 'OffTask (1)'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Perform k-fold cross-validation
cv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')

# Print the cross-validation scores
print("Cross-Validation F1 Scores:", cv_scores)
print("Mean F1 Score:", np.mean(cv_scores))
print("Standard Deviation of F1 Scores:", np.std(cv_scores))
```

## Results

### Model Performance Comparison

The best hyperparameters found for the **Random Forest Classifier** were:

- **max_depth**: 20
- **min_samples_leaf**: 1
- **min_samples_split**: 2
- **n_estimators**: 50

The **Cohen's Kappa Scores** for the models were:

- **Random Forest**: 0.4018
- **XGBoost**: 0.2966
- **Gradient Boosting**: 0.5513 (after threshold optimization)

### Threshold Optimization Insights

Adjusting the decision threshold significantly impacted the model's performance:

- **At Threshold 0.90**:
  - **Precision (OffTask)**: 0.50
  - **Recall (OffTask)**: 0.67
  - **F1-score (OffTask)**: 0.57
  - **Cohen's Kappa Score**: 0.5513

### Confusion Matrix Analysis

The confusion matrix at the optimal threshold was:

```
Confusion Matrix:
 [[143   4]
 [  2   4]]
```

- **True Positives**: 4
- **False Positives**: 4
- **True Negatives**: 143
- **False Negatives**: 2

### Cross-Validation Results

- **Cross-Validation F1 Scores**: [0.4, 0.0, 0.2857, 0.5, 0.5455]
- **Mean F1 Score**: 0.299
- **Standard Deviation**: 0.201

## Discussion

### Challenges with Class Imbalance

The imbalanced dataset posed significant challenges:

- **Difficulty in Learning Minority Class Patterns**: The scarcity of OffTask instances made it hard for models to generalize.
- **Overfitting Risk**: Without proper handling, models could overfit to the majority class.

### Effectiveness of SMOTE

Applying SMOTE helped in:

- **Balancing the Dataset**: Synthetic samples improved the representation of the minority class.
- **Improving Recall**: The model became better at identifying OffTask instances.

However, reliance on synthetic data might not capture the complexity of actual off-task behavior.

### Threshold Optimization Trade-offs

- **Improved Detection**: A higher threshold increased the precision for the OffTask class.
- **False Positives and Negatives**: Adjusting the threshold affected the balance between missing actual OffTask instances and incorrectly flagging Not OffTask instances.

### Model Selection Insights

- **Gradient Boosting Superiority**: Its ability to focus on misclassified instances led to better performance.
- **Random Forest and XGBoost Limitations**: These models were less effective, possibly due to their parameter sensitivity and handling of imbalanced data.

### Cross-Validation Variability

The significant standard deviation in cross-validation scores suggests:

- **Model Instability**: Performance varied across different data splits.
- **Need for Robustness**: Further techniques are required to ensure consistent performance.

## Conclusion

This study highlights the complexities involved in detecting off-task behavior using machine learning. Key findings include:

- **Gradient Boosting Effectiveness**: With proper tuning and threshold adjustment, it outperformed other models.
- **Importance of Handling Class Imbalance**: Techniques like SMOTE are crucial but have limitations.
- **Threshold Optimization**: Essential for improving minority class detection but requires careful trade-off consideration.

### Future Work

- **Advanced Imbalance Handling**: Explore cost-sensitive learning and ensemble methods.
- **Feature Engineering**: Incorporate more behavioral indicators to improve model accuracy.
- **Real-world Implementation**: Test models in live educational settings for practical validation.
- **Ethical Considerations**: Ensure models are used responsibly, respecting privacy and fairness.

# Submission Guidelines

All required explanations are included in this document. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed.