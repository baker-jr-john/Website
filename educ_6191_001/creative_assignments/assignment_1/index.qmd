---
title: "Behavior Detection"
description: "A machine learning model to detect off-task behavior"
author:
  - name: John Baker
    affiliations:
      - name: "Penn GSE: University of Pennsylvania Graduate School of Education"
lang: en
date: 09-18-2024
# date-modified: 09-07-2024
format:
  html:
    code-link: false
draft: true
jupyter: python3
---



```{python}
# Import libraries
import pandas as pd                                                                                                           # <1>
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, cohen_kappa_score
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve
```
1. Importing Libraries
  * pandas: Used for data manipulation.
  * train_test_split: Used to split the dataset into training and testing sets.
  * GridSearchCV: Helps in finding the optimal hyperparameters for the model.
  * RandomForestClassifier: A decision-tree-based classifier that combines many trees to improve performance.
  * SMOTE: Synthetic Minority Over-sampling Technique, used to handle class imbalance in the training data.
  * classification_report: Used to generate a detailed report on model performance metrics.
```{python}
# Load the dataset
data = pd.read_csv('data/ca1-dataset.csv')                                                                                    # <2>
```
2. Loading the Dataset
  * Loads the dataset from the CSV file.
```{python}
# Prepare the data
data['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable                                             # <3>
X = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features
y = data['OffTask']  # Target variable
```
3. Preparing the Data
  * The target variable OffTask is encoded, converting 'N' (No) to 0 and 'Y' (Yes) to 1.
  * X is set as the feature set by dropping irrelevant columns ('Unique-id', 'namea', 'OffTask').
  * y is the target variable, which is the encoded OffTask column.
```{python}
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)                                     # <4>
```
4. Splitting Data
  * The dataset is split into training (80%) and testing sets (20%).
```{python}
# Apply SMOTE to the training data
smote = SMOTE(random_state=42)                                                                                                # <5>
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
```
5. Handling Imbalanced Data with SMOTE
  * SMOTE is applied to balance the classes in the training data. It creates synthetic samples for the minority class (OffTask == 1).
  * X_train_resampled and y_train_resampled contain the balanced training data.
```{python}
# Calculate the ratio of classes
class_0_count = sum(y_train_resampled == 0)                                                                                   # <6>
class_1_count = sum(y_train_resampled == 1)
ratio_of_classes = class_0_count / class_1_count
```
6. Class Distribution Check
  * Counts the number of instances in each class after resampling, to calculate the class ratio (0 = not off-task, 1 = off-task).
```{python}
# Define the model
model = RandomForestClassifier(random_state=42, class_weight='balanced')                                                      # <7>
```
7. Defining the Model
  * A RandomForest model is instantiated. The class_weight='balanced' argument is used to adjust weights inversely proportional to class frequencies.
```{python}
# Define the parameter grid
param_grid = {                                                                                                                # <8>
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
```
8. Setting Up Hyperparameter Tuning (GridSearchCV)
  * Defines the parameter grid for GridSearchCV to find the optimal combination of hyperparameters:
    * n_estimators: Number of trees.
    * max_depth: Maximum depth of trees.
    * min_samples_split: Minimum samples required to split a node.
    * min_samples_leaf: Minimum number of samples required in a leaf node.
```{python}
# Set up GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,                                                            # <9>
                           scoring='f1', cv=5, n_jobs=-1, verbose=2)
```
9. GridSearchCV for Hyperparameter Tuning
  * GridSearchCV is initialized with:
    * model: The RandomForest classifier.
    * param_grid: The defined hyperparameter grid.
    * scoring='f1': Uses the F1 score as the evaluation metric.
    * cv=5: Performs 5-fold cross-validation.
    * n_jobs=-1: Utilizes all available CPU cores.
    * verbose=2: Provides detailed output.
```{python}
# Fit GridSearchCV
grid_search.fit(X_train_resampled, y_train_resampled)                                                                         # <10>
```
10. Fitting the Model
  * Fits the GridSearchCV on the resampled training data to find the best hyperparameters.
```{python}
# Best parameters
print("Best parameters found: ", grid_search.best_params_)
```

```{python}
# Train the model on the resampled data
model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
kappa = cohen_kappa_score(y_test, y_pred)
print("Kappa Score:", kappa)
print(classification_report(y_test, y_pred))
```

```{python}
# Train the XGBoost model without the use_label_encoder parameter
xgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
kappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)
print("Kappa Score (XGBoost):", kappa_xgb)
print(classification_report(y_test, y_pred_xgb))
```

```{python}
# Define the Gradient Boosting model
gb_model = GradientBoostingClassifier(
    learning_rate=0.2,
    max_depth=5,
    min_samples_split=10,
    n_estimators=200,
    random_state=42
)

# Fit the model on the resampled training data
gb_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
kappa_gb = cohen_kappa_score(y_test, y_pred_gb)
print("Kappa Score (Gradient Boosting):", kappa_gb)
print(classification_report(y_test, y_pred_gb))
```

```{python}
# Get predicted probabilities
y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]

# Set a new threshold
threshold = 0.3  # Example threshold
y_pred_adjusted_gb = (y_pred_proba_gb >= threshold).astype(int)

# Evaluate the model with the adjusted predictions
kappa_adjusted_gb = cohen_kappa_score(y_test, y_pred_adjusted_gb)
print("Adjusted Kappa Score (Gradient Boosting):", kappa_adjusted_gb)
print(classification_report(y_test, y_pred_adjusted_gb))
```

```{python}
# Experiment with different thresholds
thresholds = np.arange(0.0, 1.0, 0.05)
precisions = []
recalls = []
kappa_scores = []

for threshold in thresholds:
    y_pred_adjusted = (y_pred_proba_gb >= threshold).astype(int)
    
    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) > 0 else 0
    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) > 0 else 0
    
    kappa = cohen_kappa_score(y_test, y_pred_adjusted)
    
    precisions.append(precision)
    recalls.append(recall)
    kappa_scores.append(kappa)

plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, label='Precision', marker='o')
plt.plot(thresholds, recalls, label='Recall', marker='o')
plt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')
plt.title('Precision, Recall, and Kappa Score vs. Threshold')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.xticks(np.arange(0.0, 1.1, 0.1))
plt.legend()
plt.grid()
plt.show()

best_threshold_index = np.argmax(recalls)
best_threshold = thresholds[best_threshold_index]
print(f"Best Threshold for Maximum Recall: {best_threshold:.2f}")
print(f"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}")
print(f"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}")
print(f"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}")
```

```{python}
# Initialize lists to store precision, recall, and F1-score values
f1_scores = []

# Calculate precision, recall, and F1-score for each threshold
for threshold in thresholds:
    y_pred_adjusted = (y_pred_proba_gb >= threshold).astype(int)
    
    # Calculate precision and recall
    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) > 0 else 0
    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) > 0 else 0
    
    # Calculate F1-score
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # Append F1-score to the list
    f1_scores.append(f1_score)

# Plot Precision, Recall, and F1-Score curve
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, label='Precision', marker='o')
plt.plot(thresholds, recalls, label='Recall', marker='o')
plt.plot(thresholds, f1_scores, label='F1 Score', marker='o')
plt.title('Precision, Recall, and F1 Score vs. Threshold')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.xticks(np.arange(0.0, 1.1, 0.1))
plt.legend()
plt.grid()
plt.show()

# Print the best threshold based on maximum F1-score
best_threshold_index = np.argmax(f1_scores)
best_threshold = thresholds[best_threshold_index]
print(f"Best Threshold for Maximum F1-Score: {best_threshold:.2f}")
print(f"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}")
print(f"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}")
print(f"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}")
```

```{python}
# Make predictions using the new threshold
y_pred_final = (gb_model.predict_proba(X_test)[:, 1] >= 0.90).astype(int)

# Evaluate the model with the new predictions
kappa_final = cohen_kappa_score(y_test, y_pred_final)
print("Final Kappa Score with Threshold 0.90:", kappa_final)

# Print the classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_final))
```

```{python}
# Optionally, you can also calculate and print confusion matrix
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred_final)
print("Confusion Matrix:\n", conf_matrix)

# Visualize the confusion matrix (optional)
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Not OffTask (0)', 'OffTask (1)'], 
            yticklabels=['Not OffTask (0)', 'OffTask (1)'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Perform k-fold cross-validation
cv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')  # You can change scoring to 'accuracy', 'precision', etc.

# Print the cross-validation scores
print("Cross-Validation F1 Scores:", cv_scores)
print("Mean F1 Score:", np.mean(cv_scores))
print("Standard Deviation of F1 Scores:", np.std(cv_scores))
```
