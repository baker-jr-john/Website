---
title: "Building an Enhanced Behavior Detector: A Machine Learning Approach"
description: "Developing an improved behavior classifier using feature engineering and ensemble methods."
date: 2024-10-02
author: 
  - name: John Baker
    email: jbaker1@upenn.edu
    affiliation:
      - name: "Penn GSE: University of Pennsylvania Graduate School of Education"
abstract: |
  This project aims to develop an improved behavior classifier by engineering new features from a detailed student interaction dataset (`ca2-dataset.csv`) and integrating them with existing features from `ca1-dataset.csv`. Utilizing ensemble machine learning techniques and rigorous cross-validation strategies, we demonstrate enhanced predictive performance in detecting off-task behavior among students interacting with educational software.
keywords:
  - behavior detection
  - feature engineering
  - machine learning
  - ensemble methods
  - educational data mining
bibliography: bibliography/bibliography.bib
nocite: |
  @*
format:
  html:
    code-link: false
draft: true
jupyter: python3
ipynb-shell-interactivity: all
execute: 
  freeze: false
---

## Introduction

Understanding student behavior within educational software environments is crucial for providing timely interventions and enhancing learning outcomes. Off-task behavior, in particular, can negatively impact learning efficacy. Accurate detection of such behavior allows educators to address issues promptly and tailor educational experiences to individual student needs.

This project builds upon previous work by engineering new features derived from detailed logs of student interactions. By integrating these features with existing ones and applying advanced machine learning techniques, we aim to develop an improved behavior detector that can more accurately identify off-task behaviors.

## Literature Review

Educational Data Mining (EDM) has emerged as a significant field, leveraging student data to enhance learning outcomes. Recent research has focused on developing algorithms and metrics to address algorithmic bias in education and other related fields [@cohausz2024fairness]. Analyzing student data can provide valuable insights into factors influencing academic performance, including social connections [@siemens2012learning].

A particularly relevant area within EDM for this study is detecting student misuse of educational systems. Baker and Siemens [@siemens2012learning] explored how data mining techniques can identify instances where students "game the system" in constraint-based tutors. This concept is pertinent to identifying off-task behavior, a broader category of student misuse.

Off-task behavior encompasses actions where students deviate from their intended engagement with educational software, including disengagement, inappropriate tool use, or attempts to circumvent learning activities. "Gaming the system" [@baker2009state] can be understood as a specific manifestation of off-task behavior in which students exploit system mechanics to achieve desired outcomes without genuine engagement.

Other relevant methodologies and ethical considerations include:

1. The use of "text replays" to gain a deeper understanding of student behavior [@sao2012improving; @slater2020iterative], which could potentially be adapted for analyzing off-task behavior patterns.

2. Addressing fairness and bias in machine learning models used in educational contexts [@cohausz2024fairness; @baker2022algorithmic], ensuring that models for detecting off-task behavior are equitable and do not unfairly disadvantage certain student groups.

## Methodology

### Data Preparation

I began by importing essential libraries for data manipulation and machine learning, loading the datasets (`ca1` and `ca2`) from CSV files.

```{python}
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV
from sklearn.metrics import roc_auc_score, cohen_kappa_score
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE

# Load datasets
ca1 = pd.read_csv('data/ca1-dataset.csv')
ca2 = pd.read_csv('data/ca2-dataset.csv')
```

- **`ca1-dataset.csv`**: Contains existing features related to student interactions.
- **`ca2-dataset.csv`**: Provides detailed logs of student actions, from which new features are engineered.

Both datasets were imported into Pandas DataFrames for manipulation and analysis.

#### Dataset Overview

**`ca1-dataset.csv`:**

- **Entries:** 763 rows
- **Columns:** 27
- **Data Types:** Numeric and categorical
- **Missing Values:** None

**`ca2-dataset.csv`:**

- **Entries:** 1,745 rows
- **Columns:** 34
- **Data Types:** Numeric and categorical
- **Missing Values:** None

### Feature Engineering

From `ca2-dataset.csv`, we engineered several user-specific features to capture behavioral patterns:

- **Action Frequency:** Total number of actions per user.
- **Average Time Between Actions:** Mean time interval between consecutive actions.
- **Maximum Action Duration:** Longest time interval between actions.
- **Action Diversity:** Number of unique actions performed.
- **Idle Time Metrics:** Total idle time (periods of inactivity exceeding 60 seconds) and the ratio of idle to active time.

```{python}
# Action Frequency, Avg Time Between Actions, Max Action Duration, Action Diversity, Idle/Active Ratio
action_freq = ca2.groupby('Unique-id')['Row'].count().reset_index()
action_freq.columns = ['Unique-id', 'Action_Frequency']

ca2['time'] = pd.to_datetime(ca2['time'], errors='coerce')
ca2 = ca2.sort_values(by=['Unique-id', 'time'])
ca2['Time_Diff'] = ca2.groupby('Unique-id')['time'].diff().dt.total_seconds()
avg_time_diff = ca2.groupby('Unique-id')['Time_Diff'].mean().reset_index()
avg_time_diff.columns = ['Unique-id', 'Avg_Time_Between_Actions']

max_time_diff = ca2.groupby('Unique-id')['Time_Diff'].max().reset_index()
max_time_diff.columns = ['Unique-id', 'Max_Action_Duration']

action_diversity = ca2.groupby('Unique-id')['prod'].nunique().reset_index()
action_diversity.columns = ['Unique-id', 'Action_Diversity']

ca2['Idle_Time'] = ca2['Time_Diff'].apply(lambda x: x if x > 60 else 0)
total_idle_time = ca2.groupby('Unique-id')['Idle_Time'].sum().reset_index()
total_active_time = ca2.groupby('Unique-id')['Time_Diff'].sum().reset_index()
total_active_time.columns = ['Unique-id', 'Total_Active_Time']
idle_active_ratio = total_idle_time.merge(total_active_time, on='Unique-id')
idle_active_ratio['Idle_Active_Ratio'] = idle_active_ratio['Idle_Time'] / idle_active_ratio['Total_Active_Time']
```

These features aim to quantify user engagement and detect patterns indicative of off-task behavior.

### Data Merging and Cleaning

The new features were merged with `ca1-dataset.csv` based on the `Unique-id` key. Missing values in numerical columns were handled using mean imputation to ensure the integrity of the dataset for modeling.

```{python}
# Merging the new features into the original ca1-dataset.csv
ca1_enhanced = ca1.merge(action_freq, on='Unique-id', how='left')
ca1_enhanced = ca1_enhanced.merge(avg_time_diff, on='Unique-id', how='left')
ca1_enhanced = ca1_enhanced.merge(max_time_diff, on='Unique-id', how='left')
ca1_enhanced = ca1_enhanced.merge(action_diversity, on='Unique-id', how='left')
ca1_enhanced = ca1_enhanced.merge(idle_active_ratio[['Unique-id', 'Idle_Active_Ratio']], on='Unique-id', how='left')

# Handling missing values using mean imputation
numeric_cols = ca1_enhanced.select_dtypes(include=['number']).columns
ca1_enhanced[numeric_cols] = ca1_enhanced[numeric_cols].fillna(ca1_enhanced[numeric_cols].mean())
```

### Model Development

We developed two primary models:

- **Model 1:** Utilizes only the original features from `ca1-dataset.csv`.
- **Model 2:** Combines original features with the newly engineered features.

#### Model 1: Original Features

A Random Forest Classifier was trained using the original features to serve as a baseline model. The target variable was the `OffTask` indicator, converted to a binary format.

```{python}
# Model Development using RandomForestClassifier
original_features = ['Avgright', 'Avgbug', 'Avghelp', 'Avgchoice', 'Avgstring', 'Avgnumber', 'Avgpoint', 'Avgpchange', 'Avgtime', 'AvgtimeSDnormed', 'Avgtimelast3SDnormed', 'Avgtimelast5SDnormed', 'Avgnotright', 'Avghowmanywrong-up', 'Avghelppct-up', 'Avgwrongpct-up', 'Avgtimeperact-up', 'AvgPrev3Count-up', 'AvgPrev5Count-up', 'Avgrecent8help', 'Avg recent5wrong', 'Avgmanywrong-up', 'AvgasymptoteA-up', 'AvgasymptoteB-up']

# Separate features and target variable ('OffTask')
X_original = ca1_enhanced[original_features]
y = ca1_enhanced['OffTask'].apply(lambda x: 1 if x == 'Y' else 0)

# Split the dataset into train and test sets
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original, y, test_size=0.2, random_state=42)

# Build the RandomForest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_orig, y_train_orig)

# Predict and evaluate the model
y_pred_orig = rf.predict(X_test_orig)
auc_orig = roc_auc_score(y_test_orig, y_pred_orig)
kappa_orig = cohen_kappa_score(y_test_orig, y_pred_orig)
print(f"AUC: {auc_orig}, Kappa: {kappa_orig}")
```

#### Model 2: Combined Features

The second model incorporated both original and new features. We performed hyperparameter tuning using GridSearchCV to optimize the Random Forest Classifier.

```{python}
#| output: false

# Combined Features
new_features = ['Action_Frequency', 'Avg_Time_Between_Actions', 'Max_Action_Duration', 'Action_Diversity', 'Idle_Active_Ratio']
X_combined = ca1_enhanced[original_features + new_features]
X_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(X_combined, y, test_size=0.2, random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initialize the RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='roc_auc')

# Fit GridSearchCV
grid_search.fit(X_train_comb, y_train_comb)

# Get the best parameters
best_params = grid_search.best_params_
```
```{python}
print(f'Best parameters found: {best_params}')
```

Based on the output, the best model configuration according to the grid search is a `RandomForestClassifier` with:

- **No bootstrapping** (`bootstrap=False`): Each tree is built on the entire dataset instead of a random subset.
- **Maximum depth of 10** (`max_depth=10`): Limits the number of levels in each decision tree to prevent overfitting.
- **Minimum of 2 samples per leaf node** (`min_samples_leaf=2`): Each leaf node must have at least 2 samples.
- **Minimum of 2 samples required to split an internal node** (`min_samples_split=2`): Controls the tree's complexity.
- **100 decision trees** (`n_estimators=100`): The number of trees in the forest.

```{python}
# Train the model with the best parameters
best_rf = RandomForestClassifier(**best_params, random_state=42)
best_rf.fit(X_train_comb, y_train_comb)
y_pred_comb = best_rf.predict(X_test_comb)

# Evaluate the model
auc_comb = roc_auc_score(y_test_comb, y_pred_comb)
kappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)
print(f'AUC for Combined Features with Best Params: {auc_comb}')
print(f'Kappa for Combined Features with Best Params: {kappa_comb}')
```

### Addressing Class Imbalance with SMOTE and Ensemble Modeling

To address class imbalance, Synthetic Minority Over-sampling Technique (SMOTE) was applied. An ensemble model comprising a Random Forest, Logistic Regression, and Support Vector Classifier was built using a soft voting strategy.

```{python}
# Separate features and target variable
X = ca1_enhanced[original_features + new_features]
y = ca1_enhanced['OffTask'].apply(lambda x: 1 if x == 'Y' else 0)

# Split the dataset into train and test sets before SMOTE
X_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Check original class distribution
print(f'Original training set class distribution: {Counter(y_train_comb)}')

# Apply SMOTE to balance the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)

# Check class distribution after SMOTE
print(f'Resampled training set class distribution: {Counter(y_train_resampled)}')

# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and test data
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test_comb)

# Initialize individual models with appropriate parameters
rf = RandomForestClassifier(**best_params, random_state=42)
lr = LogisticRegression(random_state=42, max_iter=1000)
svc = SVC(probability=True, random_state=42)

# Create an ensemble model
ensemble = VotingClassifier(
    estimators=[('rf', rf), ('lr', lr), ('svc', svc)],
    voting='soft'
)

# Train the ensemble model
ensemble.fit(X_train_scaled, y_train_resampled)

# Predict on the scaled test data
y_pred_comb = ensemble.predict(X_test_scaled)

# Evaluate the ensemble model
auc_comb = roc_auc_score(y_test_comb, y_pred_comb)
kappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)
print(f'AUC for Combined Features with Ensemble: {auc_comb}')
print(f'Kappa for Combined Features with Ensemble: {kappa_comb}')
```

### Cross-Validation Strategy

To ensure the model's generalizability, we employed GroupKFold cross-validation based on `Unique-id`. This approach prevents data from the same student appearing in both training and testing sets, thereby providing a more robust evaluation.

```{python}
# Cross-validation using GroupKFold with the ensemble model
gkf = GroupKFold(n_splits=5)
groups = ca1_enhanced['Unique-id']

auc_scores_comb = []
kappa_scores_comb = []

for train_idx, test_idx in gkf.split(X_combined, y, groups=groups):
    X_train_comb, X_test_comb = X_combined.iloc[train_idx], X_combined.iloc[test_idx]
    y_train_comb, y_test_comb = y.iloc[train_idx], y.iloc[test_idx]
    
    # Apply SMOTE to each fold
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)
    
    ensemble.fit(X_train_resampled, y_train_resampled)
    y_pred_comb = ensemble.predict(X_test_comb)
    auc_scores_comb.append(roc_auc_score(y_test_comb, y_pred_comb))
    kappa_scores_comb.append(cohen_kappa_score(y_test_comb, y_pred_comb))

# Averaged cross-validation results for combined features with ensemble
avg_auc_comb = sum(auc_scores_comb) / len(auc_scores_comb)
avg_kappa_comb = sum(kappa_scores_comb) / len(kappa_scores_comb)
print(f'Average AUC for Combined Features with Ensemble: {avg_auc_comb}')
print(f'Average Kappa for Combined Features with Ensemble: {avg_kappa_comb}')
```

### Feature Selection

Recursive Feature Elimination (RFE) was utilized to identify the top 10 most significant features. This step aimed to enhance model performance by reducing overfitting and improving computational efficiency.

```{python}
# Perform Recursive Feature Elimination (RFE)
rfe = RFE(estimator=rf, n_features_to_select=10, step=1)
rfe.fit(X_combined, y)

# Get the selected features
selected_features = X_combined.columns[rfe.support_]

# Use only the selected features for training and testing
X_combined_selected = X_combined[selected_features]

# Apply SMOTE to balance the dataset
X_resampled, y_resampled = smote.fit_resample(X_combined_selected, y)

# Split the resampled data
X_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Train the ensemble model with selected features
ensemble.fit(X_train_comb, y_train_comb)
y_pred_comb = ensemble.predict(X_test_comb)

# Evaluate the ensemble model with selected features
auc_comb = roc_auc_score(y_test_comb, y_pred_comb)
kappa_comb = cohen_kappa_score(y_test_comb, y_pred_comb)
print(f'AUC for Combined Features with Ensemble and RFE: {auc_comb}')
print(f'Kappa for Combined Features with Ensemble and RFE: {kappa_comb}')
```

#### Cross-Validation with Selected Features

```{python}
# Cross-validation using GroupKFold with the ensemble model and selected features
auc_scores_comb = []
kappa_scores_comb = []

for train_idx, test_idx in gkf.split(X_combined_selected, y, groups=groups):
    X_train_comb, X_test_comb = X_combined_selected.iloc[train_idx], X_combined_selected.iloc[test_idx]
    y_train_comb, y_test_comb = y.iloc[train_idx], y.iloc[test_idx]
    
    # Apply SMOTE to each fold
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_comb, y_train_comb)
    
    ensemble.fit(X_train_resampled, y_train_resampled)
    y_pred_comb = ensemble.predict(X_test_comb)
    auc_scores_comb.append(roc_auc_score(y_test_comb, y_pred_comb))
    kappa_scores_comb.append(cohen_kappa_score(y_test_comb, y_pred_comb))

# Averaged cross-validation results for combined features with ensemble and RFE
avg_auc_comb = sum(auc_scores_comb) / len(auc_scores_comb)
avg_kappa_comb = sum(kappa_scores_comb) / len(kappa_scores_comb)
print(f'Average AUC for Combined Features with Ensemble and RFE: {avg_auc_comb}')
print(f'Average Kappa for Combined Features with Ensemble and RFE: {avg_kappa_comb}')
```

## Results

### Model Performance Comparison

**Model 1 (Original Features):**

- **AUC Score:** 0.583
- **Cohen's Kappa:** 0.278

**Interpretation:** The baseline model using only the original features yielded an AUC score of 0.583, which is slightly better than random guessing (AUC of 0.5). The Cohen's Kappa score of approximately 0.278 indicates a fair agreement between the predicted and actual classifications but suggests room for improvement.

**Model 2 (Combined Features with Best Parameters):**

- **AUC Score:** 0.75
- **Cohen's Kappa:** 0.658

**Interpretation:** Incorporating the newly engineered features and optimizing the hyperparameters significantly improved the model's performance. The AUC increased to 0.75, indicating better discriminative ability, and the Cohen's Kappa score rose to approximately 0.658, reflecting substantial agreement.

**Model 2 with Ensemble and SMOTE:**

- **AUC Score:** 0.803
- **Cohen's Kappa:** 0.388

**Interpretation:** Using SMOTE to address class imbalance and employing an ensemble model further enhanced the AUC to approximately 0.803. However, the Cohen's Kappa score decreased to around 0.388. This discrepancy suggests that while the model's ability to rank predictions improved, the overall agreement between predicted and actual classes did not improve proportionally.

**Model 2 with Ensemble, SMOTE, and RFE:**

- **AUC Score:** 0.942
- **Cohen's Kappa:** 0.884

**Interpretation:** Applying Recursive Feature Elimination (RFE) to select the top 10 features resulted in a significant boost in both AUC and Cohen's Kappa scores. An AUC of approximately 0.942 indicates excellent model performance, and a Cohen's Kappa of about 0.884 suggests almost perfect agreement.

**Cross-Validation Results (With RFE):**

- **Average AUC:** 0.730
- **Average Cohen's Kappa:** 0.240

**Interpretation:** The cross-validation results using GroupKFold showed an average AUC of 0.730 and an average Cohen's Kappa of 0.240. While these scores are lower than the test set results, they provide a more realistic estimate of model performance on unseen data.

## Discussion

The progressive enhancements in model performance demonstrate the effectiveness of feature engineering and model optimization techniques:

1. **Feature Engineering:** The introduction of new features derived from `ca2-dataset.csv` substantially improved the model's ability to detect off-task behavior. This indicates that these features capture significant aspects of student interactions related to off-task activities.

2. **Hyperparameter Tuning:** Optimizing the Random Forest parameters led to better model performance, highlighting the importance of tailoring the model to the data characteristics.

3. **Addressing Class Imbalance:** Applying SMOTE balanced the training data, which is crucial when dealing with imbalanced classes. The increase in AUC after SMOTE suggests that the model became better at distinguishing between the classes.

4. **Ensemble Modeling:** Combining different algorithms (Random Forest, Logistic Regression, and SVC) in an ensemble improved the robustness of the predictions. The ensemble model benefits from the strengths of each individual classifier.

5. **Feature Selection with RFE:** Reducing the feature set to the most significant 10 features using RFE not only simplified the model but also enhanced performance. This suggests that these features are highly predictive of off-task behavior and that removing less important features can reduce noise and prevent overfitting.

6. **Cross-Validation Insights:** The cross-validation results, while lower than the test set scores, are critical for assessing how the model might perform on new, unseen data. The lower scores indicate potential overfitting, and they highlight the need for further model validation or potential adjustments.

**Potential Reasons for Discrepancies:**

- The significant drop in Cohen's Kappa during cross-validation suggests variability in model performance across different folds. This could be due to the inherent differences in student behaviors or the limited size of the dataset.
- The high performance on the test set after RFE may indicate overfitting to that specific split. Cross-validation provides a more generalized performance metric.

**Implications for Educational Interventions:**

- The top features identified can help educators understand which behaviors are most indicative of off-task activities.
- Real-time monitoring systems can be developed using these key features to alert educators when a student may need intervention.

## Limitations

Despite the promising results obtained from the enhanced behavior detection model, several limitations must be acknowledged to contextualize the findings and guide future research.

**Dataset Size and Diversity**

The datasets utilized in this study, `ca1-dataset.csv` and `ca2-dataset.csv`, contain 763 and 1,745 entries, respectively. While these datasets provided a valuable foundation for feature engineering and model development, their relatively small size may limit the generalizability of the results. A limited dataset might not capture the full spectrum of student behaviors and interactions, potentially leading to models that are tailored to the specific characteristics of the sampled population. This constraint can affect the model's performance when applied to different student groups or educational settings.

**Potential Overfitting**

The significant improvement in model performance, particularly after applying Recursive Feature Elimination (RFE), raises concerns about potential overfitting. Overfitting occurs when a model learns patterns specific to the training data, including noise and outliers, which do not generalize well to unseen data. The discrepancy between the high test set performance (AUC of 0.942 and Cohen's Kappa of 0.884) and the lower cross-validation scores (average AUC of 0.730 and average Cohen's Kappa of 0.240) suggests that the model may not perform as effectively on new data. This overfitting can be attributed to several factors:

- **Feature Selection Bias:** RFE might have selected features that are highly predictive in the training set but not necessarily in other datasets.
- **Class Imbalance:** Despite using SMOTE to balance the classes, the original imbalance and the synthetic nature of the oversampled data may still influence the model's learning process.
- **Complexity of the Ensemble Model:** Combining multiple algorithms can increase the risk of overfitting if not properly regularized or if the individual models are themselves overfitted.

**Need for External Validation**

The models developed have not been validated on external datasets or in real-world educational environments. External validation is crucial for assessing the robustness and generalizability of the model across different contexts. Without testing the model on independent data, it's challenging to determine whether the performance improvements are due to the model's predictive capabilities or artifacts of the specific datasets used.

**Implications of Limitations**

These limitations imply that while the model shows high performance on the provided datasets, its applicability to broader educational settings remains uncertain. The potential overfitting and lack of external validation mean that educators and stakeholders should exercise caution when considering the deployment of this model in real-world applications.

## Future Work to Address Limitations

To mitigate these limitations and enhance the model's utility, the following steps are recommended:

- **Data Expansion:** Collecting more extensive and diverse datasets will help capture a wider range of student behaviors, reducing the risk of overfitting and improving generalizability.
- **External Validation:** Testing the model on independent datasets from different educational software or institutions will provide insights into its real-world applicability.
- **Regularization Techniques:** Incorporating regularization methods and exploring simpler models may prevent overfitting by limiting model complexity.
- **Model Interpretability:** Investigating the contribution of each feature to the model's predictions can help identify any overreliance on specific features that may not generalize well.
- **Alternative Algorithms:** Experimenting with other machine learning algorithms, such as gradient boosting machines or neural networks, could offer better performance and generalization.
- **Cross-Validation Strategies:** Utilizing more robust cross-validation techniques, such as nested cross-validation, can provide a more accurate assessment of model performance.

By addressing these areas, future research can develop a more robust behavior detection model that maintains high performance while generalizing effectively across different student populations and educational contexts.

## Conclusion

This study successfully developed an enhanced behavior detector by:

- Engineering new features from detailed student interaction data.
- Combining these features with existing ones to improve model performance.
- Utilizing ensemble machine learning techniques and addressing class imbalance.
- Applying feature selection methods to identify the most significant predictors.

The enhanced model demonstrates a high ability to detect off-task behavior, which is crucial for timely educational interventions. Future work could involve:

- Expanding the dataset to include more students for better generalization.
- Testing the model in a real-world educational setting.
- Exploring additional features or algorithms to further improve performance.

### Submission Guidelines

This document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed.