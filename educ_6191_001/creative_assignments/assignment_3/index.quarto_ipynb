{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "pagetitle: \"John Baker â€“ Learning Analytics\"\n",
        "title: \"Knowledge Structure Mapping: a Comprehensive Report\"\n",
        "description: \"An in-depth exploration of knowledge structure mapping using Factor Analysis, K-Means clustering, and PCA to uncover latent skills in an eight-item test dataset\"\n",
        "date: 2024-11-20\n",
        "date-modified: 2024-12-02\n",
        "author: \n",
        "  - name: John Baker\n",
        "    email: jbaker1@upenn.edu\n",
        "    affiliation:\n",
        "      - name: \"Penn GSE: University of Pennsylvania Graduate School of Education\"\n",
        "        url: https://www.gse.upenn.edu/\n",
        "abstract: |\n",
        "  This study aims to identify the underlying knowledge structure of an eight-item test dataset by applying Factor Analysis, K-Means clustering, and Principal Component Analysis (PCA). Factor Analysis was utilized to uncover latent skills, with results cross-validated using K-Means clustering (simulating Barnes's Q-Matrix method) and PCA. Models with two to four components were compared to determine the optimal skill representation. The findings indicate that a three-component Factor Analysis model best captures the relationships among test items, effectively identifying three distinct skills. The final Q-matrix balances complexity and interpretability, providing a robust mapping of items to latent skills and enhancing the understanding of the dataset's knowledge structure.\n",
        "keywords:\n",
        "  - knowledge structure mapping\n",
        "  - factor analysis\n",
        "  - q-matrix\n",
        "  - latent skills\n",
        "  - pca\n",
        "bibliography: bibliography/bibliography.bib\n",
        "nocite: |\n",
        "  @*\n",
        "image: images/image_fx_.png\n",
        "format:\n",
        "  html:\n",
        "    code-link: false\n",
        "draft: false\n",
        "jupyter: python3\n",
        "ipynb-shell-interactivity: all\n",
        "execute: \n",
        "  freeze: true\n",
        "  echo: fenced\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Knowledge structure mapping** is a powerful tool that allows educators to uncover hidden connections between what students know and what they are tested on. By revealing the relationships between test items and the underlying skills they measure, **knowledge structure mapping** provides crucial insights for developing targeted educational interventions and improving student outcomes. This understanding is essential for creating effective assessments, personalizing instruction, and ensuring that all students have the opportunity to succeed.\n",
        "\n",
        "However, identifying the optimal representation of latent skills within educational data is a complex challenge. Traditional methods often rely on assumptions that may not generalize across diverse contexts or assessment types. To address this issue, researchers have developed a range of data-driven approaches that aim to uncover skill structures in a more flexible and robust manner.\n",
        "\n",
        "This study presents a comprehensive methodology for identifying the latent skill structure underlying an eight-item test dataset. By leveraging the complementary strengths of **Factor Analysis**, **K-Means Clustering**, and **Principal Component Analysis (PCA)**, I aim to derive a robust and interpretable model of the key skills assessed by the test items. My approach involves iteratively testing models with varying numbers of components to determine the optimal balance between model complexity and explanatory power.\n",
        "\n",
        "The resulting three-skill model offers a clear and actionable framework for understanding student performance on the test items. The model's interpretability and strong empirical foundation make it a valuable tool for informing assessment design, instructional planning, and student support initiatives. By aligning educational practices with the identified skill structure, educators can more effectively foster student learning and achievement.\n",
        "\n",
        "Moreover, this study contributes to the broader field of educational data mining and learning analytics by demonstrating the value of a multi-method, data-driven approach to **knowledge structure mapping**. The methodology presented here can serve as a template for future research aimed at uncovering the hidden skills and competencies that underlie student performance across a wide range of educational contexts and assessment types.\n",
        "\n",
        "In the following sections, I provide an overview of relevant background literature, describe my methodological approach in detail, present the key findings of my analysis, and discuss the implications of my work for educational practice and future research.\n",
        "\n",
        "## Background and Related Work\n",
        "\n",
        "**Knowledge structure mapping** is a fundamental area of research in educational data mining and learning analytics, focusing on uncovering the latent skills and relationships that underlie student performance on educational assessments [@de2008educational]. By providing insights into the hidden structure of educational data, **knowledge structure mapping** enables researchers and educators to develop more effective assessments, instructional interventions, and student support systems.\n",
        "\n",
        "### Methods for Knowledge Structure Mapping\n",
        "\n",
        "Researchers have developed a range of methods to map knowledge structures, each offering unique advantages and limitations. **Factor Analysis**, a widely used statistical technique, identifies latent skills by analyzing patterns of correlations among test items [@beavers2019practical]. This data-driven approach uncovers hidden skill structures without requiring prior knowledge of the relationships.\n",
        "\n",
        "In contrast, Barnes's Q-matrix method [@barnes2005q] takes a different approach by using a binary matrix to represent item-skill associations. The Q-matrix provides a visual tool for understanding which skills are assessed by each item, making it valuable for cognitive modeling and educational data mining. By explicitly encoding the relationships between items and skills, the Q-matrix enables researchers to develop more interpretable and actionable models of student knowledge.\n",
        "\n",
        "**K-Means Clustering** offers another perspective by grouping items based on response patterns, allowing researchers to infer underlying skills from the emergent clusters [@kargupta2001distributed]. This unsupervised learning technique enables exploratory analysis of skill structures when predefined skill mappings are unavailable. By identifying groups of items that elicit similar student responses, **K-Means Clustering** can reveal hidden commonalities that may correspond to latent skills.\n",
        "\n",
        "**Principal Component Analysis (PCA** is another powerful tool for uncovering latent structures in educational data [@chen2018knowedu]. By identifying the principal components that explain the maximum variance in the data, **PCA** can help researchers identify the key dimensions or skills that underlie student performance. While **PCA** is not specifically designed for **knowledge structure mapping**, it can provide valuable insights into the overall structure of the data and inform the interpretation of other methods.\n",
        "\n",
        "### Applications in Educational Data Mining\n",
        "\n",
        "The methods described above have been widely applied in educational data mining and learning analytics to support a range of tasks and objectives. One key application area is the development of intelligent tutoring systems and adaptive learning environments [@cukurova2022learning]. By incorporating **knowledge structure mapping** techniques, these systems can dynamically assess student skills and provide personalized feedback and recommendations based on individual needs.\n",
        "\n",
        "**Knowledge structure mapping** also plays a crucial role in assessment design and evaluation. By uncovering the latent skills assessed by test items, researchers can develop more valid and reliable assessments that effectively measure student knowledge. This information can also be used to identify areas where assessments may be over- or under-emphasizing certain skills, enabling educators to make informed decisions about assessment design and revision.\n",
        "\n",
        "### Limitations and Challenges\n",
        "\n",
        "Despite the significant advances in **knowledge structure mapping**, there are still important limitations and challenges to address. One key issue is the need for more flexible and robust methods that can handle the complexity and diversity of educational data [@gordon2003learning]. Many existing methods rely on strong assumptions about the structure of the data or the nature of the skills being assessed, which may not hold across different contexts or domains.\n",
        "\n",
        "Another important challenge is the need for more interpretable and actionable models that can inform educational practice [@chen2018knowedu]. While **knowledge structure mapping** can provide valuable insights into the hidden structure of educational data, translating these insights into concrete recommendations for educators and learners remains a significant challenge.\n",
        "\n",
        "To address these limitations, researchers are exploring new approaches that combine multiple methods and data sources to develop more comprehensive and robust models of student knowledge [@cukurova2022learning]. There is also growing interest in developing more transparent and explainable models that can provide clear guidance to educators and learners [@gordon2003learning].\n",
        "\n",
        "In the present study, I aim to contribute to this ongoing research effort by presenting a comprehensive methodology for **knowledge structure mapping** that leverages the strengths of multiple methods to uncover the latent skills underlying an eight-item test dataset. By comparing models with varying levels of complexity and interpretability, I seek to identify the optimal balance between model fit and practical utility. My approach demonstrates the value of a multi-method, data-driven approach to **knowledge structure mapping** and provides a template for future research in this area.\n",
        "\n",
        "## Methods Used\n",
        "\n",
        "### Overview\n",
        "\n",
        "To uncover the latent skills underlying the eight-item test dataset, I employed a multi-method approach that combines **Factor Analysis**, **K-Means Clustering**, and **PCA**. Each method offers unique strengths and limitations, and by leveraging their complementary perspectives, I aimed to develop a more robust and comprehensive understanding of the knowledge structure underlying the data.\n",
        "\n",
        "**Factor Analysis** served as the primary method for identifying latent skills, as it is specifically designed to uncover hidden constructs that explain the patterns of correlations among observed variables [@beavers2019practical]. **K-Means Clustering** provided a complementary perspective by grouping items based on their response patterns, allowing me to explore potential skill clusters without imposing strong assumptions about the number or nature of the underlying skills [@kargupta2001distributed]. Finally, **PCA** was used as a validation technique to assess the stability and robustness of the latent skill structure identified by the other methods [@chen2018knowedu].\n",
        "\n",
        "By comparing the results of these three methods and exploring models with varying levels of complexity, I sought to identify the optimal balance between model fit and interpretability. My goal was to develop a parsimonious and actionable representation of the latent skills that could inform assessment design, instructional planning, and student support initiatives.\n",
        "\n",
        "### Factor Analysis\n",
        "\n",
        "**Factor Analysis** was selected as the primary method for identifying latent skills due to its ability to uncover hidden constructs that explain the patterns of correlations among test items [@beavers2019practical].\n",
        "\n",
        "To determine the optimal number of factors to retain, I used a combination of statistical criteria and substantive considerations. Specifically, I examined the scree plot of eigenvalues, the percentage of variance explained by each factor, and the interpretability of the resulting factor solutions [@beavers2019practical]. I also compared models with varying numbers of factors (ranging from two to four) to assess their relative fit and interpretability.\n",
        "\n",
        "While **Factor Analysis** is a powerful tool for uncovering latent constructs, it is important to acknowledge its assumptions and limitations. **Factor Analysis** assumes that the observed variables are continuous and normally distributed, which may not hold for binary or ordinal data (such as the correct or incorrect responses in the present dataset). However, research has shown that **Factor Analysis** can still provide useful insights when applied to binary data, particularly when the sample size is large and the factor loadings are strong [@watkins2018exploratory].\n",
        "\n",
        "### K-Means Clustering\n",
        "\n",
        "**K-Means Clustering** was used as a complementary method to explore potential skill clusters based on item response patterns [@kargupta2001distributed]. Unlike **Factor Analysis**, **K-Means Clustering** does not impose strong assumptions about the structure of the data or the nature of the underlying constructs. Instead, it aims to partition the data into a specified number of clusters based on the similarity of their response patterns.\n",
        "\n",
        "To apply **K-Means Clustering**, I first transformed the data to represent each item as a vector of binary responses across all students. I then used the elbow method to determine the optimal number of clusters, which involves plotting the within-cluster sum of squares (WCSS) against the number of clusters and identifying the \"elbow\" point where the rate of decrease in WCSS begins to level off [@kargupta2001distributed]. Based on this analysis, I selected a three-cluster solution as the most parsimonious and interpretable representation of the data.\n",
        "\n",
        "While **K-Means Clustering** can provide valuable insights into the structure of the data, it, too, has limitations. **K-Means Clustering** assumes that the clusters are spherical and of equal size, which may not hold in practice [@gordon2003learning]. Additionally, the resulting clusters are sensitive to the initial placement of the cluster centroids, which can lead to different solutions across multiple runs of the algorithm [@kargupta2001distributed]. To mitigate these issues, I used multiple random initializations and selected the solution with the lowest WCSS.\n",
        "\n",
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "**PCA** was employed as a validation technique to assess the stability and robustness of the latent skill structure identified by **Factor Analysis** and **K-Means Clustering** [@chen2018knowedu]. **PCA** is a dimensionality reduction technique that aims to identify the principal components that explain the maximum amount of variance in the data.\n",
        "\n",
        "To apply **PCA**, the data was initially standardized to ensure all items were on a consistent scale. The scree plot of eigenvalues was then examined to identify the optimal number of components for the analysis. PCA was subsequently conducted on the standardized item response data to evaluate the stability and robustness of the underlying skill structure [@chen2018knowedu].\n",
        "\n",
        "**PCA** provides a useful complement to **Factor Analysis** and **K-Means Clustering**, as it does not impose strong assumptions about the structure of the data or the nature of the underlying constructs. Instead, it identifies the key dimensions of variation in the data, which can be used to validate the stability and robustness of the latent skill structure identified by the other methods [@chen2018knowedu].\n",
        "\n",
        "However, it is important to recognize that **PCA** is a purely data-driven technique and does not necessarily identify constructs that are substantively meaningful or interpretable [@gordon2003learning]. Additionally, **PCA** assumes that the relationships among the observed variables are linear, which may not hold in practice [@chen2018knowedu]. Despite these limitations, **PCA** can still provide valuable insights into the overall structure of the data and inform the interpretation of the latent skill structure.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "#### Loading the Data\n",
        "\n",
        "The first step in my analysis was to load and preprocess the eight-item test dataset. The dataset consisted of binary responses (correct or incorrect) from 1,920 students on eight test items. I used the `pandas` library in Python to load the data into a data frame and perform initial data exploration\n",
        "\n",
        "```{{python}}\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data/8items.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data.head()\n",
        "```"
      ],
      "id": "f9260198"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-prep\n",
        "#| tbl-cap: First few rows of the dataset\n",
        "#| echo: false\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data/8items.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data.head()"
      ],
      "id": "tbl-prep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To prepare the data for analysis, I examined the structure of the data frame and checked for missing values.\n"
      ],
      "id": "8fe4b4af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check the dimensions of the dataset\n",
        "print(f\"Dataset dimensions: {data.shape}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(data.isnull().sum())"
      ],
      "id": "c5b06262",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Factor Analysis\n",
        "\n",
        "#### Preparing Data for Factor Analysis\n",
        "\n",
        "I extracted the item response data, excluding any non-item columns such as student identifiers. This step ensured that my analyses focused solely on the patterns of student responses across the eight test items.\n"
      ],
      "id": "df87c452"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract item data (excluding the 'student' column if present)\n",
        "item_data = data.drop(columns=['student'], errors='ignore')"
      ],
      "id": "0b10c2c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Determining the Number of Factors Using Scree Plot\n",
        "\n",
        "To determine the optimal number of factors to retain, I examined a scree plot of eigenvalues.\n"
      ],
      "id": "3bdfcc48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary modules\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from factor_analyzer import FactorAnalyzer\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "item_data_scaled = scaler.fit_transform(item_data)\n",
        "\n",
        "# Perform factor analysis with maximum factors\n",
        "fa_model = FactorAnalyzer(rotation=None)\n",
        "fa_model.fit(item_data_scaled)"
      ],
      "id": "de2b173b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get eigenvalues and variance explained\n",
        "ev, v = fa_model.get_eigenvalues()\n",
        "variance = fa_model.get_factor_variance()\n",
        "\n",
        "# Extract variance explained and cumulative variance\n",
        "variance_explained = variance[1]\n",
        "cumulative_variance_explained = variance[2]\n",
        "\n",
        "# Total variance explained by the factors\n",
        "total_variance_explained = cumulative_variance_explained[-1]\n",
        "print(f\"Total Variance Explained by Factors: {total_variance_explained}\")"
      ],
      "id": "33d3b3d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{{python}}\n",
        "# Plot the scree plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(ev) + 1), ev, 'o-', color='blue')\n",
        "_ = plt.title('Scree Plot for Factor Analysis')\n",
        "_ = plt.xlabel('Factor Number')\n",
        "_ = plt.ylabel('Eigenvalue')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```"
      ],
      "id": "0d712ddc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-scree1\n",
        "#| fig-cap: Scree Plot for Factor Analysis\n",
        "#| echo: false\n",
        "\n",
        "# Plot the scree plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(ev) + 1), ev, 'o-', color='blue')\n",
        "_ = plt.title('Scree Plot for Factor Analysis')\n",
        "_ = plt.xlabel('Factor Number')\n",
        "_ = plt.ylabel('Eigenvalue')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "fig-scree1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scree plot shows a clear \"elbow\" after the second factor, where the eigenvalues drop sharply initially and then level off. This \"elbow\" suggests that the first two factors capture most of the meaningful variance, with subsequent factors contributing relatively little.\n",
        "\n",
        "#### Performing Factor Analysis\n",
        "\n",
        "I applied **Factor Analysis** with three components to identify latent skills in the dataset.\n",
        "\n",
        "```{{python}}\n",
        "# Retrieve the factor loadings\n",
        "factor_loadings = fa_model.loadings_\n",
        "\n",
        "# Dynamically determine the number of factors extracted\n",
        "n_factors_extracted = factor_loadings.shape[1]\n",
        "\n",
        "# Create a data frame for the factor loadings\n",
        "factor_loadings_df = pd.DataFrame(\n",
        "    factor_loadings,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_factors_extracted)]\n",
        ")\n",
        "\n",
        "# Display the factor loadings\n",
        "factor_loadings_df\n",
        "```"
      ],
      "id": "29e768a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-factor\n",
        "#| tbl-cap: Factor Loadings\n",
        "#| echo: false\n",
        "\n",
        "# Retrieve the factor loadings\n",
        "factor_loadings = fa_model.loadings_\n",
        "\n",
        "# Dynamically determine the number of factors extracted\n",
        "n_factors_extracted = factor_loadings.shape[1]\n",
        "\n",
        "# Create a data frame for the factor loadings\n",
        "factor_loadings_df = pd.DataFrame(\n",
        "    factor_loadings,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_factors_extracted)]\n",
        ")\n",
        "\n",
        "# Display the factor loadings\n",
        "factor_loadings_df"
      ],
      "id": "tbl-factor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **Factor Analysis** revealed three distinct latent skills underlying the eight test items. The first skill was characterized by high loadings on **Items 3**, **5**, **7**, and **8**. The second skill was defined by high loadings on **Items 2**, **4**, and **6**. The third skill was primarily associated with **Item 1**.\n",
        "\n",
        "### K-Means Clustering\n",
        "\n",
        "#### Transposing Item Data\n",
        "\n",
        "I transposed the item data to cluster items based on their response patterns.\n"
      ],
      "id": "3b9aac2c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import K-Means module\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Transpose the item data to have items as rows and students as columns\n",
        "item_data_transposed = item_data_scaled.T\n",
        "\n",
        "# Specify the number of clusters (skills)\n",
        "n_clusters = 3\n",
        "\n",
        "# Initialize the K-Means model\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "# Fit the model to the transposed item data\n",
        "kmeans.fit(item_data_transposed)"
      ],
      "id": "2e2cf709",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Retrieve the cluster labels for each item\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Create a data frame to display the item-cluster mapping\n",
        "kmeans_q_matrix_df = pd.DataFrame({\n",
        "    'Item': item_data.columns,\n",
        "    'Mapped_Skill': [f'Skill_{label+1}' for label in cluster_labels]\n",
        "})"
      ],
      "id": "e4b79ac6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Determining the Number of Clusters Using Elbow Method\n",
        "\n",
        "To determine the optimal number of clusters, I used the elbow method, which involved plotting the within-cluster sum of squares (WCSS) against the number of clusters and identifying the \"elbow\" point where the rate of decrease in WCSS began to level off.\n"
      ],
      "id": "3fc47081"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary module\n",
        "import numpy as np\n",
        "\n",
        "# Calculate WCSS for different number of clusters\n",
        "wcss = []\n",
        "for i in range(1, 7):\n",
        "    kmeans_elbow = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans_elbow.fit(item_data_transposed)\n",
        "    wcss.append(kmeans_elbow.inertia_)"
      ],
      "id": "6c706f6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{{python}}\n",
        "# Plot the elbow graph\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 7), wcss, 'o-', color='red')\n",
        "_ = plt.title('Elbow Method for K-Means Clustering')\n",
        "_ = plt.xlabel('Number of Clusters')\n",
        "_ = plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```"
      ],
      "id": "d2a8697f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-elbow\n",
        "#| fig-cap: Elbow Method for K-Means Clustering\n",
        "#| echo: false\n",
        "\n",
        "# Plot the elbow graph\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 7), wcss, 'o-', color='red')\n",
        "_ = plt.title('Elbow Method for K-Means Clustering')\n",
        "_ = plt.xlabel('Number of Clusters')\n",
        "_ = plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "fig-elbow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the elbow plot, I decided a three-cluster solution is the most parsimonious and interpretable representation of the data.\n",
        "\n",
        "#### Applying K-Means Clustering\n",
        "\n",
        "I applied **K-Means Clustering** to the transformed item response data to explore potential skill clusters based on the similarity of item response patterns [@kargupta2001distributed].\n",
        "\n",
        "```{{python}}\n",
        "# Get unique clusters (skills) from the kmeans_q_matrix_df\n",
        "unique_skills = kmeans_q_matrix_df['Mapped_Skill'].unique()\n",
        "n_clusters = len(unique_skills)\n",
        "\n",
        "# Create a binary Q-matrix based on the kmeans clustering results\n",
        "# Create an empty matrix of zeros\n",
        "binary_matrix = np.zeros((len(kmeans_q_matrix_df), n_clusters), dtype=int)\n",
        "\n",
        "# Iterate through the rows of kmeans_q_matrix_df and fill in the appropriate cluster assignment\n",
        "for index, row in kmeans_q_matrix_df.iterrows():\n",
        "    skill_index = int(row['Mapped_Skill'].split('_')[1]) - 1  # Extract the skill number and convert to zero-indexed\n",
        "    binary_matrix[index, skill_index] = 1\n",
        "\n",
        "# Create a DataFrame for the binary Q-matrix\n",
        "q_matrix_kmeans_binary_df = pd.DataFrame(\n",
        "    binary_matrix,\n",
        "    index=kmeans_q_matrix_df['Item'],\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_clusters)]\n",
        ")\n",
        "\n",
        "# Reset index\n",
        "q_matrix_kmeans_binary_df.reset_index(inplace=True)\n",
        "q_matrix_kmeans_binary_df.rename(columns={'index': 'Item'}, inplace=True)\n",
        "\n",
        "# Display the Q-matrix\n",
        "q_matrix_kmeans_binary_df\n",
        "```"
      ],
      "id": "ee816bfc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-kmeansqmatrix\n",
        "#| tbl-cap: Item-Cluster Mapping\n",
        "#| echo: false\n",
        "\n",
        "# Get unique clusters (skills) from the kmeans_q_matrix_df\n",
        "unique_skills = kmeans_q_matrix_df['Mapped_Skill'].unique()\n",
        "n_clusters = len(unique_skills)\n",
        "\n",
        "# Create a binary Q-matrix based on the kmeans clustering results\n",
        "# Create an empty matrix of zeros\n",
        "binary_matrix = np.zeros((len(kmeans_q_matrix_df), n_clusters), dtype=int)\n",
        "\n",
        "# Iterate through the rows of kmeans_q_matrix_df and fill in the appropriate cluster assignment\n",
        "for index, row in kmeans_q_matrix_df.iterrows():\n",
        "    skill_index = int(row['Mapped_Skill'].split('_')[1]) - 1  # Extract the skill number and convert to zero-indexed\n",
        "    binary_matrix[index, skill_index] = 1\n",
        "\n",
        "# Create a DataFrame for the binary Q-matrix\n",
        "q_matrix_kmeans_binary_df = pd.DataFrame(\n",
        "    binary_matrix,\n",
        "    index=kmeans_q_matrix_df['Item'],\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_clusters)]\n",
        ")\n",
        "\n",
        "# Reset index\n",
        "q_matrix_kmeans_binary_df.reset_index(inplace=True)\n",
        "q_matrix_kmeans_binary_df.rename(columns={'index': 'Item'}, inplace=True)\n",
        "\n",
        "# Display the Q-matrix\n",
        "q_matrix_kmeans_binary_df"
      ],
      "id": "tbl-kmeansqmatrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting clusters closely aligned with the latent skills identified by the **Factor Analysis**, providing convergent evidence for the three-skill structure underlying the test items.\n",
        "\n",
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "#### Determining the Number of Components Using Scree Plot\n",
        "\n",
        "I generated another scree plot to help determine the optimal number of components.\n"
      ],
      "id": "896ede78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary module\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA to get all components\n",
        "pca = PCA()\n",
        "pca.fit(item_data_scaled)"
      ],
      "id": "8441fb80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{{python}}\n",
        "# Calculate explained variance\n",
        "explained_variance = pca.explained_variance_\n",
        "\n",
        "# Plot the scree plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'o-', color='green')\n",
        "_ = plt.title('Scree Plot for PCA')\n",
        "_ = plt.xlabel('Principal Component Number')\n",
        "_ = plt.ylabel('Eigenvalue')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```"
      ],
      "id": "07d04707"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-scree2\n",
        "#| fig-cap: Scree Plot for PCA\n",
        "#| echo: false\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_variance = pca.explained_variance_\n",
        "\n",
        "# Plot the scree plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'o-', color='green')\n",
        "_ = plt.title('Scree Plot for PCA')\n",
        "_ = plt.xlabel('Principal Component Number')\n",
        "_ = plt.ylabel('Eigenvalue')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "fig-scree2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the scree plot used in **Factor Analysis**, this plot indicates that the majority of significant variance is explained by the first two factors, while the remaining factors contribute comparatively little additional information.\n",
        "\n",
        "#### Performing PCA\n",
        "\n",
        "I conducted **PCA** on the standardized item response data to assess the stability and robustness of the latent skill structure identified by **Factor Analysis** and **K-Means Clustering** [@chen2018knowedu].\n"
      ],
      "id": "c41d2cf3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize the PCA model with three components\n",
        "pca_model = PCA(n_components=3)\n",
        "\n",
        "# Fit the PCA model to the item data\n",
        "pca_model.fit(item_data_scaled)"
      ],
      "id": "c3a396bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{{python}}\n",
        "# Retrieve the PCA loadings\n",
        "pca_loadings = pca_model.components_.T\n",
        "\n",
        "# Create a data frame for the PCA loadings\n",
        "pca_loadings_df = pd.DataFrame(\n",
        "    pca_loadings,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(3)]\n",
        ")\n",
        "\n",
        "# Display the PCA loadings\n",
        "pca_loadings_df\n",
        "```"
      ],
      "id": "cddb8f5d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-pca\n",
        "#| tbl-cap: PCA Loadings\n",
        "#| echo: false\n",
        "\n",
        "# Retrieve the PCA loadings\n",
        "pca_loadings = pca_model.components_.T\n",
        "\n",
        "# Create a data frame for the PCA loadings\n",
        "pca_loadings_df = pd.DataFrame(\n",
        "    pca_loadings,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(3)]\n",
        ")\n",
        "\n",
        "# Display the PCA loadings\n",
        "pca_loadings_df"
      ],
      "id": "tbl-pca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **PCA** results largely confirmed the three-skill structure identified by the other methods.\n",
        "\n",
        "## Results\n",
        "\n",
        "### Mapping Items to Skills Using PCA\n",
        "\n",
        "To understand the item-skill relationships further, I created a Q-matrix based on the **PCA** loadings. Each item is assigned to the skill (principal component) with which it has the highest loading.\n",
        "\n",
        "```{{python}}\n",
        "# Convert the PCA loadings to a Q-matrix format (binary)\n",
        "# Set a threshold to determine if the item is associated with a skill\n",
        "threshold = 0.2\n",
        "\n",
        "# Create a binary Q-matrix based on the loadings and the threshold\n",
        "q_matrix_binary = (np.abs(pca_loadings_df) > threshold).astype(int)\n",
        "\n",
        "# Display the Q-matrix\n",
        "q_matrix_binary.index.name = 'Item'\n",
        "q_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n",
        "\n",
        "# Reset the index to display it like a table\n",
        "q_matrix_binary_df = q_matrix_binary.reset_index()\n",
        "\n",
        "# Display the Q-matrix\n",
        "q_matrix_binary_df\n",
        "```"
      ],
      "id": "eaaa2e53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-pcaqmarix\n",
        "#| tbl-cap: PCA Q-Matrix\n",
        "#| echo: false\n",
        "\n",
        "# Convert the PCA loadings to a Q-matrix format (binary)\n",
        "# Set a threshold to determine if the item is associated with a skill\n",
        "threshold = 0.2\n",
        "\n",
        "# Create a binary Q-matrix based on the loadings and the threshold\n",
        "q_matrix_binary = (np.abs(pca_loadings_df) > threshold).astype(int)\n",
        "\n",
        "# Display the Q-matrix\n",
        "q_matrix_binary.index.name = 'Item'\n",
        "q_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n",
        "\n",
        "# Reset the index to display it like a table\n",
        "q_matrix_binary_df = q_matrix_binary.reset_index()\n",
        "\n",
        "# Display the Q-matrix\n",
        "q_matrix_binary_df"
      ],
      "id": "tbl-pcaqmarix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison Across Methods\n",
        "\n",
        "The mappings obtained from **Factor Analysis**, **K-Means Clustering**, and **PCA** show considerable agreement, suggesting the presence of three distinct latent skills assessed by the test items.\n",
        "\n",
        "### Testing Alternative Factor Analysis Models\n",
        "\n",
        "I tested **Factor Analysis** models with two-, three-, and four-factor models to determine the optimal number of latent skills.\n",
        "\n",
        "#### Factor Analysis with Four Components\n"
      ],
      "id": "9588f859"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Performing Factor Analysis with four components to explore the potential presence of additional latent skills\n",
        "n_factors_extended = 4\n",
        "fa_model_extended = FactorAnalyzer(n_factors=n_factors_extended, rotation=None)\n",
        "fa_model_extended.fit(item_data_scaled)"
      ],
      "id": "61ff445b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{{python}}\n",
        "# Get the factor loadings for the 4-component model\n",
        "factor_loadings_extended = fa_model_extended.loadings_\n",
        "\n",
        "# Create a data frame to visualize the factor loadings for the four-component model\n",
        "factor_loadings_extended_df = pd.DataFrame(\n",
        "    factor_loadings_extended,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_factors_extended)]\n",
        ")\n",
        "\n",
        "# Reset the index to properly align the \"Item\" column with the factor loadings\n",
        "factor_loadings_extended_df.reset_index(inplace=True)\n",
        "factor_loadings_extended_df.rename(columns={'index': 'Item'}, inplace=True)\n",
        "\n",
        "# Display the extended factor loadings\n",
        "factor_loadings_extended_df\n",
        "```"
      ],
      "id": "8c4cc6ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-factorloadingsextended\n",
        "#| tbl-cap: Factor Analysis with Four Components\n",
        "#| echo: false\n",
        "\n",
        "# Get the factor loadings for the 4-component model\n",
        "factor_loadings_extended = fa_model_extended.loadings_\n",
        "\n",
        "# Create a data frame to visualize the factor loadings for the four-component model\n",
        "factor_loadings_extended_df = pd.DataFrame(\n",
        "    factor_loadings_extended,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_factors_extended)]\n",
        ")\n",
        "\n",
        "# Reset the index to properly align the \"Item\" column with the factor loadings\n",
        "factor_loadings_extended_df.reset_index(inplace=True)\n",
        "factor_loadings_extended_df.rename(columns={'index': 'Item'}, inplace=True)\n",
        "\n",
        "# Display the extended factor loadings\n",
        "factor_loadings_extended_df"
      ],
      "id": "tbl-factorloadingsextended",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations from the Four-Component Model**:\n",
        "\n",
        "- **Complexity and Overfitting**: The four-component model introduces additional complexity without significant gains in explained variance. Some items load significantly on multiple factors, making interpretation challenging.\n",
        "- **Item Loadings**:\n",
        "  - **Item2** and **Item4** have substantial loadings on both **Skill_2** and **Skill_4**, indicating overlapping skills.\n",
        "  - **Item5** loads highly on both **Skill_1** and **Skill_3**, suggesting it may be measuring a combination of skills.\n",
        "- **Interpretability**: The overlapping loadings reduce the model's interpretability, making it less practical for educational applications.\n",
        "\n",
        "#### Factor Analysis with Two Components\n"
      ],
      "id": "889e0130"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Performing Factor Analysis with two components to explore if a simpler model might explain the relationships\n",
        "n_factors_simpler = 2\n",
        "fa_model_simpler = FactorAnalyzer(n_factors=n_factors_simpler, rotation=None)\n",
        "fa_model_simpler.fit(item_data_scaled)"
      ],
      "id": "71d77910",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{{python}}\n",
        "# Get the factor loadings for the two-component model\n",
        "factor_loadings_simpler = fa_model_simpler.loadings_\n",
        "\n",
        "# Create a data frame to visualize the factor loadings for the two-component model\n",
        "factor_loadings_simpler_df = pd.DataFrame(\n",
        "    factor_loadings_simpler,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_factors_simpler)]\n",
        ")\n",
        "\n",
        "# Reset the index and rename it to align with the desired table format\n",
        "factor_loadings_simpler_df.reset_index(inplace=True)\n",
        "factor_loadings_simpler_df.rename(columns={'index': 'Item'}, inplace=True)\n",
        "\n",
        "factor_loadings_simpler_df\n",
        "```"
      ],
      "id": "7ba02756"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-factorloadingssimpler\n",
        "#| tbl-cap: Factor Analysis with Two Components\n",
        "#| echo: false\n",
        "\n",
        "# Get the factor loadings for the two-component model\n",
        "factor_loadings_simpler = fa_model_simpler.loadings_\n",
        "\n",
        "# Create a data frame to visualize the factor loadings for the two-component model\n",
        "factor_loadings_simpler_df = pd.DataFrame(\n",
        "    factor_loadings_simpler,\n",
        "    index=item_data.columns,\n",
        "    columns=[f'Skill_{i+1}' for i in range(n_factors_simpler)]\n",
        ")\n",
        "\n",
        "# Reset the index and rename it to align with the desired table format\n",
        "factor_loadings_simpler_df.reset_index(inplace=True)\n",
        "factor_loadings_simpler_df.rename(columns={'index': 'Item'}, inplace=True)\n",
        "\n",
        "factor_loadings_simpler_df"
      ],
      "id": "tbl-factorloadingssimpler",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Simplicity vs. Variance Explained**: The two-component model is simpler but explains less variance compared to the three-component model.\n",
        "- **Item Loadings**:\n",
        "  - **Item3**, **Item5**, **Item7**, and **Item8** load highly on **Skill_1**.\n",
        "  - **Item2**, **Item4**, and **Item6** load on **Skill_2**.\n",
        "  - **Item1** has very low loadings on both factors, suggesting it may not be well-represented in this model.\n",
        "- **Loss of Detail**: The two-component model may be too simplistic, failing to capture nuances in the data, particularly the unique contribution of **Item1**.\n",
        "\n",
        "### Visualizations\n",
        "\n",
        "#### Diagrams\n",
        "\n",
        "I created a couple [Mermaid](https://mermaid.js.org/) diagrams to gain further insight.\n",
        "\n",
        "```{{mermaid}}\n",
        "graph TB\n",
        "    subgraph PCA\n",
        "        PCA_S1[Skill_1] --- PCA_I3[Item 3]\n",
        "        PCA_S1 --- PCA_I7[Item 7]\n",
        "        PCA_S1 --- PCA_I8[Item 8]\n",
        "        \n",
        "        PCA_S2[Skill_2] --- PCA_I2[Item 2]\n",
        "        PCA_S2 --- PCA_I4[Item 4]\n",
        "        PCA_S2 --- PCA_I6[Item 6]\n",
        "        \n",
        "        PCA_S3[Skill_3] --- PCA_I1[Item 1]\n",
        "        PCA_S3 --- PCA_I5[Item 5]\n",
        "    end\n",
        "    \n",
        "    subgraph KMeans\n",
        "        KM_S1[Skill_1] --- KM_I3[Item 3]\n",
        "        KM_S1 --- KM_I5[Item 5]\n",
        "        KM_S1 --- KM_I7[Item 7]\n",
        "        KM_S1 --- KM_I8[Item 8]\n",
        "        \n",
        "        KM_S2[Skill_2] --- KM_I2[Item 2]\n",
        "        KM_S2 --- KM_I4[Item 4]\n",
        "        KM_S2 --- KM_I6[Item 6]\n",
        "        \n",
        "        KM_S3[Skill_3] --- KM_I1[Item 1]\n",
        "    end\n",
        "    \n",
        "    subgraph Factor_Analysis\n",
        "        FA_S1[Skill_1] --- FA_I3[Item 3]\n",
        "        FA_S1 --- FA_I5[Item 5]\n",
        "        FA_S1 --- FA_I7[Item 7]\n",
        "        FA_S1 --- FA_I8[Item 8]\n",
        "        \n",
        "        FA_S2[Skill_2] --- FA_I2[Item 2]\n",
        "        FA_S2 --- FA_I4[Item 4]\n",
        "        FA_S2 --- FA_I6[Item 6]\n",
        "        \n",
        "        FA_S3[Skill_3] --- FA_I1[Item 1]\n",
        "    end\n",
        "\n",
        "    style PCA fill:#f9f,stroke:#333,stroke-width:2px\n",
        "    style KMeans fill:#bbf,stroke:#333,stroke-width:2px\n",
        "    style Factor_Analysis fill:#bfb,stroke:#333,stroke-width:2px\n",
        "```\n",
        "\n",
        "```{mermaid}\n",
        "%%| label: fig-mermaid1\n",
        "%%| fig-cap: \"Comparison Across the Three Methods\"\n",
        "%%| echo: false\n",
        "\n",
        "graph TB\n",
        "    subgraph PCA\n",
        "        PCA_S1[Skill_1] --- PCA_I3[Item 3]\n",
        "        PCA_S1 --- PCA_I7[Item 7]\n",
        "        PCA_S1 --- PCA_I8[Item 8]\n",
        "        \n",
        "        PCA_S2[Skill_2] --- PCA_I2[Item 2]\n",
        "        PCA_S2 --- PCA_I4[Item 4]\n",
        "        PCA_S2 --- PCA_I6[Item 6]\n",
        "        \n",
        "        PCA_S3[Skill_3] --- PCA_I1[Item 1]\n",
        "        PCA_S3 --- PCA_I5[Item 5]\n",
        "    end\n",
        "    \n",
        "    subgraph KMeans\n",
        "        KM_S1[Skill_1] --- KM_I3[Item 3]\n",
        "        KM_S1 --- KM_I5[Item 5]\n",
        "        KM_S1 --- KM_I7[Item 7]\n",
        "        KM_S1 --- KM_I8[Item 8]\n",
        "        \n",
        "        KM_S2[Skill_2] --- KM_I2[Item 2]\n",
        "        KM_S2 --- KM_I4[Item 4]\n",
        "        KM_S2 --- KM_I6[Item 6]\n",
        "        \n",
        "        KM_S3[Skill_3] --- KM_I1[Item 1]\n",
        "    end\n",
        "    \n",
        "    subgraph Factor_Analysis\n",
        "        FA_S1[Skill_1] --- FA_I3[Item 3]\n",
        "        FA_S1 --- FA_I5[Item 5]\n",
        "        FA_S1 --- FA_I7[Item 7]\n",
        "        FA_S1 --- FA_I8[Item 8]\n",
        "        \n",
        "        FA_S2[Skill_2] --- FA_I2[Item 2]\n",
        "        FA_S2 --- FA_I4[Item 4]\n",
        "        FA_S2 --- FA_I6[Item 6]\n",
        "        \n",
        "        FA_S3[Skill_3] --- FA_I1[Item 1]\n",
        "    end\n",
        "\n",
        "    style PCA fill:#f9f,stroke:#333,stroke-width:2px\n",
        "    style KMeans fill:#bbf,stroke:#333,stroke-width:2px\n",
        "    style Factor_Analysis fill:#bfb,stroke:#333,stroke-width:2px\n",
        "```\n",
        "\n",
        "\n",
        "##### Method Comparison (Factor Analysis, K-Means, PCA)\n",
        "- **Key Observations**:\n",
        "  1. **Consistency Across Methods**: Many items (e.g., **Item 3** and **Item 7**) align similarly across **Factor Analysis**, **K-Means**, and **PCA**, reinforcing the robustness of these mappings.\n",
        "  2. **Item Overlap**: The clustering of items (e.g., **Items 3**, **7**, and **8** under **Skill_1**) consistently suggests a strong latent skill grouping.\n",
        "  3. **Discrepancies**: While most items map consistently, some differences (e.g., **Item 5** under **Factor Analysis** vs. **PCA**) suggest subtle differences in how these methods interpret data structures.\n",
        "  4. **Skill 3 Representation**: This skill emerges consistently across methods but captures fewer items, which might indicate a niche or less represented skill.\n",
        "\n",
        "The visual comparison highlights overlaps and outliers more effectively than numerical tables, making it easier to identify items that contribute ambiguously to multiple skills or are method-dependent.\n",
        "\n",
        "```{{mermaid}}\n",
        "graph TB\n",
        "    subgraph Four_Component_Model\n",
        "        FC_S1[Skill_1] --- FC_I3[Item 3]\n",
        "        FC_S1 --- FC_I7[Item 7]\n",
        "        FC_S1 --- FC_I8[Item 8]\n",
        "        FC_S1 -.-> FC_I5[Item 5]\n",
        "        \n",
        "        FC_S2[Skill_2] --- FC_I6[Item 6]\n",
        "        FC_S2 -.-> FC_I2[Item 2]\n",
        "        FC_S2 -.-> FC_I4[Item 4]\n",
        "        \n",
        "        FC_S3[Skill_3] --- FC_I1[Item 1]\n",
        "        FC_S3 --- FC_I5\n",
        "        \n",
        "        FC_S4[Skill_4] -.-> FC_I2\n",
        "        FC_S4 -.-> FC_I4\n",
        "    end\n",
        "    \n",
        "    subgraph Three_Component_Model\n",
        "        TH_S1[Skill_1] --- TH_I3[Item 3]\n",
        "        TH_S1 --- TH_I7[Item 7]\n",
        "        TH_S1 --- TH_I8[Item 8]\n",
        "        TH_S1 --- TH_I5[Item 5]\n",
        "        \n",
        "        TH_S2[Skill_2] --- TH_I2[Item 2]\n",
        "        TH_S2 --- TH_I4[Item 4]\n",
        "        TH_S2 --- TH_I6[Item 6]\n",
        "        \n",
        "        TH_S3[Skill_3] --- TH_I1[Item 1]\n",
        "        TH_S3 -.-> TH_I5\n",
        "    end\n",
        "    \n",
        "    subgraph Two_Component_Model\n",
        "        TC_S1[Skill_1] --- TC_I3[Item 3]\n",
        "        TC_S1 --- TC_I5[Item 5]\n",
        "        TC_S1 --- TC_I7[Item 7]\n",
        "        TC_S1 --- TC_I8[Item 8]\n",
        "        \n",
        "        TC_S2[Skill_2] --- TC_I2[Item 2]\n",
        "        TC_S2 --- TC_I4[Item 4]\n",
        "        TC_S2 --- TC_I6[Item 6]\n",
        "        \n",
        "        TC_I1[Item 1<br/>Weak Loadings] -..- TC_S1\n",
        "        TC_I1 -..- TC_S2\n",
        "    end\n",
        "\n",
        "    style Two_Component_Model fill:#bfb,stroke:#333,stroke-width:2px\n",
        "    style Three_Component_Model fill:#bbf,stroke:#333,stroke-width:2px\n",
        "    style Four_Component_Model fill:#f9f,stroke:#333,stroke-width:2px\n",
        "```\n",
        "\n",
        "```{mermaid}\n",
        "%%| label: fig-mermaid2\n",
        "%%| fig-cap: \"Comparison Across the Three Models\"\n",
        "%%| echo: false\n",
        "\n",
        "graph TB\n",
        "    subgraph Four_Component_Model\n",
        "        FC_S1[Skill_1] --- FC_I3[Item 3]\n",
        "        FC_S1 --- FC_I7[Item 7]\n",
        "        FC_S1 --- FC_I8[Item 8]\n",
        "        FC_S1 -.-> FC_I5[Item 5]\n",
        "        \n",
        "        FC_S2[Skill_2] --- FC_I6[Item 6]\n",
        "        FC_S2 -.-> FC_I2[Item 2]\n",
        "        FC_S2 -.-> FC_I4[Item 4]\n",
        "        \n",
        "        FC_S3[Skill_3] --- FC_I1[Item 1]\n",
        "        FC_S3 --- FC_I5\n",
        "        \n",
        "        FC_S4[Skill_4] -.-> FC_I2\n",
        "        FC_S4 -.-> FC_I4\n",
        "    end\n",
        "    \n",
        "    subgraph Three_Component_Model\n",
        "        TH_S1[Skill_1] --- TH_I3[Item 3]\n",
        "        TH_S1 --- TH_I7[Item 7]\n",
        "        TH_S1 --- TH_I8[Item 8]\n",
        "        TH_S1 --- TH_I5[Item 5]\n",
        "        \n",
        "        TH_S2[Skill_2] --- TH_I2[Item 2]\n",
        "        TH_S2 --- TH_I4[Item 4]\n",
        "        TH_S2 --- TH_I6[Item 6]\n",
        "        \n",
        "        TH_S3[Skill_3] --- TH_I1[Item 1]\n",
        "        TH_S3 -.-> TH_I5\n",
        "    end\n",
        "    \n",
        "    subgraph Two_Component_Model\n",
        "        TC_S1[Skill_1] --- TC_I3[Item 3]\n",
        "        TC_S1 --- TC_I5[Item 5]\n",
        "        TC_S1 --- TC_I7[Item 7]\n",
        "        TC_S1 --- TC_I8[Item 8]\n",
        "        \n",
        "        TC_S2[Skill_2] --- TC_I2[Item 2]\n",
        "        TC_S2 --- TC_I4[Item 4]\n",
        "        TC_S2 --- TC_I6[Item 6]\n",
        "        \n",
        "        TC_I1[Item 1<br/>Weak Loadings] -..- TC_S1\n",
        "        TC_I1 -..- TC_S2\n",
        "    end\n",
        "\n",
        "    style Two_Component_Model fill:#bfb,stroke:#333,stroke-width:2px\n",
        "    style Three_Component_Model fill:#bbf,stroke:#333,stroke-width:2px\n",
        "    style Four_Component_Model fill:#f9f,stroke:#333,stroke-width:2px\n",
        "```\n",
        "\n",
        "\n",
        "##### Model Comparison (Two-, Three-, and Four-Component Models)\n",
        "- **Key Observations**:\n",
        "  1. **Two-Component Model**: Simpler but lacks granularity, as evident in fewer distinct mappings and the merging of certain skills.\n",
        "  2. **Three-Component Model**: Balanced in complexity and interpretability, with clear item-skill relationships (e.g., **Items 3**, **7**, and **8** consistently linked to **Skill 1**).\n",
        "  3. **Four-Component Model**: Overcomplicates relationships with multiple cross-loadings (e.g., **Item 5** linked to both **Skill 1** and **Skill 3**), making the model harder to interpret.\n",
        "  4. **Weak Loadings (Item 1)**: Visualizing weak loadings in the two-component model underscores its limited ability to represent all test items adequately.\n",
        "\n",
        "The diagrams provide a clear visual distinction between the interpretability trade-offs of different models. For instance, they highlight how additional components in the four-component model lead to more overlap, supporting the conclusion that the three-component model is optimal.\n",
        "\n",
        "##### Broader Insights:\n",
        "- **Support for Prior Work**: The diagrams reinforce the findings that a three-component model is the most interpretable and aligns well across methods.\n",
        "- **New Learnings**:\n",
        "  - **Item-Specific Trends**: Items like **Item 5** show variability across methods and models, suggesting they may assess complex or multiple skills.\n",
        "  - **Skill Coverage**: Skills identified in **PCA** seem broader, potentially capturing more nuanced relationships, while **K-Means** provides a stricter clustering.\n",
        "  - **Cross-Method Validation**: The diagrams visually validate the multi-method approach, showing where methods agree or diverge.\n",
        "\n",
        "#### Heatmap of Factor Loadings (Three Components)\n",
        "\n",
        "Using a heatmap, I visualized the factor loadings from the three-component **Factor Analysis** model.\n",
        "\n",
        "```{{python}}\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a heatmap to visualize item-skill relationships from Factor Analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(factor_loadings_df, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='black', cbar=True)\n",
        "_ = plt.title('Item-Skill Relationships (Factor Analysis with Three Components)')\n",
        "plt.show()\n",
        "```"
      ],
      "id": "fed1a537"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-heatmap\n",
        "#| fig-cap: Factor Analysis with Three Components\n",
        "#| echo: false\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a heatmap to visualize item-skill relationships from Factor Analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(factor_loadings_df, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='black', cbar=True)\n",
        "_ = plt.title('Item-Skill Relationships (Factor Analysis with Three Components)')\n",
        "plt.show()"
      ],
      "id": "fig-heatmap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Key Observations:\n",
        "1. **Dominant Item-Skill Relationships**:\n",
        "   - **Item 1** strongly loads on **Skill 3** (0.99), indicating that it is almost exclusively associated with this latent skill.\n",
        "   - **Item 3**, **Item 7**, and **Item 8** have high loadings on **Skill 1** (0.81, 0.78, and 0.78, respectively), showing that they are closely related to this skill.\n",
        "   - **Item 6** is strongly associated with **Skill 2** (1.00), suggesting it is a clear indicator of this skill.\n",
        "\n",
        "2. **Cross-Skill Contributions**:\n",
        "   - **Item 5** has moderate loadings on both **Skill 1** (0.48) and **Skill 3** (0.33), indicating that it measures a mix of these skills.\n",
        "   - **Item 2** has a moderate loading on **Skill 2** (0.30), with negligible contributions to other skills, suggesting it is moderately representative of this skill but not a strong indicator.\n",
        "\n",
        "3. **Weak Loadings**:\n",
        "   - **Item 4** shows relatively weak loadings across all skills, with the highest on **Skill 2** (0.33). This suggests that it may not align well with any single skill or may be ambiguously measuring multiple skills.\n",
        "   - Similarly, **Item 2** and **Item 5** exhibit weak or mixed relationships across skills, warranting further investigation.\n",
        "\n",
        "4. **Distinct Skills**:\n",
        "   - **Skill 1**: Clearly defined by **Item 3**, **Item 7**, and **Item 8**.\n",
        "   - **Skill 2**: Dominated by **Item 6**, with some contributions from **Item 2** and **Item 4**.\n",
        "   - **Skill 3**: Clearly represented by **Item 1**, with partial contributions from **Item 5**.\n",
        "\n",
        "##### Insights:\n",
        "- **Item-Skill Assignment**: The heatmap visually confirms the appropriateness of assigning items to the skills based on their dominant factor loadings.\n",
        "- **Complex or Ambiguous Items**: Items like **Item 5** and **Item 4** exhibit weaker or mixed relationships, suggesting potential challenges in their interpretation or measurement of a specific skill.\n",
        "- **Skill Coverage**: Each skill appears to have at least one strongly associated item, ensuring that all skills are represented in the model.\n",
        "\n",
        "#### Bar Charts for Individual Items\n",
        "\n",
        "I generated bar charts to illustrate the factor loadings of each item across the three skills.\n",
        "\n",
        "```{{python}}\n",
        "# Create bar charts for each item to show its relationship across skills\n",
        "num_items = len(factor_loadings_df.index)\n",
        "fig, axes = plt.subplots(num_items, 1, figsize=(9, num_items * 2))\n",
        "\n",
        "for i, item in enumerate(factor_loadings_df.index):\n",
        "    axes[i].bar(factor_loadings_df.columns, factor_loadings_df.loc[item], color='skyblue')\n",
        "    _ = axes[i].set_title(f'Relationship of {item} with Skills')\n",
        "    _ = axes[i].set_ylabel('Loading Value')\n",
        "    _ = axes[i].set_ylim(-1, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "```"
      ],
      "id": "08694499"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-barcharts\n",
        "#| fig-cap: Bar Charts for Individual Items\n",
        "#| echo: false\n",
        "\n",
        "# Create bar charts for each item to show its relationship across skills\n",
        "num_items = len(factor_loadings_df.index)\n",
        "fig, axes = plt.subplots(num_items, 1, figsize=(9, num_items * 2))\n",
        "\n",
        "for i, item in enumerate(factor_loadings_df.index):\n",
        "    axes[i].bar(factor_loadings_df.columns, factor_loadings_df.loc[item], color='skyblue')\n",
        "    _ = axes[i].set_title(f'Relationship of {item} with Skills')\n",
        "    _ = axes[i].set_ylabel('Loading Value')\n",
        "    _ = axes[i].set_ylim(-1, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "id": "fig-barcharts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Key Insights:\n",
        "1. **Dominant Item-Skill Relationships**:\n",
        "   - **Item 1**: Almost exclusively associated with **Skill 3**, with a very high loading value (~0.99). It does not meaningfully load on **Skill 1** or **Skill 2**.\n",
        "   - **Item 3**, **Item 7**, and **Item 8**: Strongly associated with **Skill 1**, with high positive loadings (~0.81 and ~0.78). These items clearly represent this latent skill.\n",
        "   - **Item 6**: Solely aligned with **Skill 2** (loading ~1.00), making it the clearest representative of this skill.\n",
        "\n",
        "2. **Mixed and Moderate Relationships**:\n",
        "   - **Item 5**: Shows moderate loadings on both **Skill 1** (~0.48) and **Skill 3** (~0.33), indicating that it may measure a combination of these skills.\n",
        "   - **Item 2**: Moderately aligned with **Skill 2** (~0.30) but has negligible loadings on the other skills, making it a less prominent representative of any single skill.\n",
        "\n",
        "3. **Ambiguous or Weak Relationships**:\n",
        "   - **Item 4**: Has low to moderate loadings across the board, with the highest (~0.33) on **Skill 2**. This indicates that the item may be ambiguous or weakly related to the latent skills in this model.\n",
        "   - **Item 2**: Although moderately associated with **Skill 2**, its low loadings suggest it does not strongly differentiate itself in measuring this skill.\n",
        "\n",
        "4. **Distinct Skills**:\n",
        "   - **Skill 1**: Clearly defined by **Item 3**, **Item 7**, and **Item 8**.\n",
        "   - **Skill 2**: Primarily represented by **Item 6**, with minor contributions from **Item 2** and **Item 4**.\n",
        "   - **Skill 3**: Dominated by **Item 1**, with partial contributions from **Item 5**.\n",
        "\n",
        "##### Further Insight:\n",
        "1. **Support for Factor Analysis Findings**:\n",
        "   - The charts confirm that the three-component model successfully captures distinct latent skills, with most items showing strong associations with a single skill.\n",
        "   - The visualization highlights items that load cleanly on one skill (e.g., **Item 6** for **Skill 2**, **Item 1** for **Skill 3**).\n",
        "\n",
        "2. **Ambiguous Items**:\n",
        "   - Items like **Item 4** and **Item 5** demonstrate weaker or mixed relationships, indicating potential issues with their design or alignment with specific skills.\n",
        "   - These items may require revision or could indicate the need for further exploration of an additional component.\n",
        "\n",
        "3. **Strength of Representation**:\n",
        "   - Certain skills (e.g., **Skill 1** and **Skill 3**) have multiple items with high loadings, providing strong representation.\n",
        "   - **Skill 2** is highly dependent on a single dominant item (**Item 6**), which could make it more vulnerable to measurement error.\n",
        "   \n",
        "### Creating the Final Q-Matrix\n",
        "\n",
        "Based on the consistency of results across methods, I developed a final Q-matrix that maps each item to its primary associated skill based on the three-factor model. Table 8 presents the final Q-matrix, which shows a clear and interpretable mapping of items to skills.\n",
        "\n",
        "```{{python}}\n",
        "# Creating the final Q-matrix based on the visualization and analysis findings\n",
        "# Assigning each item to the skill with the highest loading from the Factor Analysis with three components\n",
        "final_q_matrix = factor_loadings_df.idxmax(axis=1)\n",
        "\n",
        "# Create a data frame to visualize the final Q-matrix, showing the mapping between items and skills\n",
        "final_q_matrix_df = pd.DataFrame({'Item': item_data.columns, 'Mapped_Skill': final_q_matrix.values})\n",
        "\n",
        "# Set a threshold to determine the significant loading\n",
        "threshold = 0.2\n",
        "\n",
        "# Create a binary Q-matrix based on the factor loadings and the threshold\n",
        "q_matrix_binary = (np.abs(factor_loadings_df) > threshold).astype(int)\n",
        "\n",
        "# Rename index and columns for better readability in the Q-matrix\n",
        "q_matrix_binary.index.name = 'Item'\n",
        "q_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n",
        "\n",
        "# Reset the index to present it as a table\n",
        "q_matrix_binary_df = q_matrix_binary.reset_index()\n",
        "\n",
        "# Display the final Q-matrix\n",
        "q_matrix_binary_df\n",
        "```"
      ],
      "id": "08b0878d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-finalqmarix\n",
        "#| tbl-cap: Final Q-Matrix\n",
        "#| echo: false\n",
        "\n",
        "# Creating the final Q-matrix based on the visualization and analysis findings\n",
        "# Assigning each item to the skill with the highest loading from the Factor Analysis with three components\n",
        "final_q_matrix = factor_loadings_df.idxmax(axis=1)\n",
        "\n",
        "# Create a data frame to visualize the final Q-matrix, showing the mapping between items and skills\n",
        "final_q_matrix_df = pd.DataFrame({'Item': item_data.columns, 'Mapped_Skill': final_q_matrix.values})\n",
        "\n",
        "# Set a threshold to determine the significant loading\n",
        "threshold = 0.2\n",
        "\n",
        "# Create a binary Q-matrix based on the factor loadings and the threshold\n",
        "q_matrix_binary = (np.abs(factor_loadings_df) > threshold).astype(int)\n",
        "\n",
        "# Rename index and columns for better readability in the Q-matrix\n",
        "q_matrix_binary.index.name = 'Item'\n",
        "q_matrix_binary.columns = [f'Skill_{i+1}' for i in range(q_matrix_binary.shape[1])]\n",
        "\n",
        "# Reset the index to present it as a table\n",
        "q_matrix_binary_df = q_matrix_binary.reset_index()\n",
        "\n",
        "# Display the final Q-matrix\n",
        "q_matrix_binary_df"
      ],
      "id": "tbl-finalqmarix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I also developed another diagram to support the final Q-Matrix.\n",
        "\n",
        "```{{mermaid}}\n",
        "graph LR\n",
        "    subgraph Final_Q_Matrix_Mappings\n",
        "        S1[Skill_1] --- I3[Item 3]\n",
        "        S1 --- I5[Item 5]\n",
        "        S1 --- I7[Item 7]\n",
        "        S1 --- I8[Item 8]\n",
        "        \n",
        "        S2[Skill_2] --- I2[Item 2]\n",
        "        S2 --- I4[Item 4]\n",
        "        S2 --- I6[Item 6]\n",
        "        \n",
        "        S3[Skill_3] --- I1[Item 1]\n",
        "    end\n",
        "    \n",
        "    style Final_Q_Matrix_Mappings fill:#bfb,stroke:#333,stroke-width:2px\n",
        "```\n",
        "\n",
        "```{mermaid}\n",
        "%%| label: fig-mermaid3\n",
        "%%| fig-cap: \"Final Q-Matirx\"\n",
        "%%| echo: false\n",
        "\n",
        "graph LR\n",
        "    subgraph Final_Q_Matrix_Mappings\n",
        "        S1[Skill_1] --- I3[Item 3]\n",
        "        S1 --- I5[Item 5]\n",
        "        S1 --- I7[Item 7]\n",
        "        S1 --- I8[Item 8]\n",
        "        \n",
        "        S2[Skill_2] --- I2[Item 2]\n",
        "        S2 --- I4[Item 4]\n",
        "        S2 --- I6[Item 6]\n",
        "        \n",
        "        S3[Skill_3] --- I1[Item 1]\n",
        "    end\n",
        "    \n",
        "    style Final_Q_Matrix_Mappings fill:#bfb,stroke:#333,stroke-width:2px\n",
        "```\n",
        "\n",
        "\n",
        "### Key Strengths of the Final Q-Matrix and Diagram\n",
        "1. **Clear Mapping**:\n",
        "   - Each item is assigned to the skill with the highest loading, ensuring that the relationships are driven by the statistical analysis.\n",
        "   - The diagram visually highlights these relationships, making it easy to understand and communicate the structure.\n",
        "\n",
        "2. **Skill Representation**:\n",
        "   - **Skill 1**: Represented by four items (**Item 3**, **Item 5**, **Item 7**, and **Item 8**), providing robust coverage and reliability for assessing this skill.\n",
        "   - **Skill 2**: Supported by three items (**Item 2**, **Item 4**, and **Item 6**), with **Item 6** being the strongest indicator.\n",
        "   - **Skill 3**: Represented by **Item 1**, a highly specific item exclusively aligned with this skill.\n",
        "\n",
        "3. **Alignment with Analyses**:\n",
        "   - The Q-matrix directly reflects the findings from the **Factor Analysis** heatmap and bar charts, ensuring consistency and validation of the mappings.\n",
        "\n",
        "4. **Balanced Complexity**:\n",
        "   - By selecting three components, the Q-matrix strikes a balance between interpretability and detail, avoiding the over-complexity of a four-component model while capturing nuances missed in a two-component model.\n",
        "\n",
        "### Observations and Recommendations\n",
        "1. **Strength of Item Representation**:\n",
        "   - **Skill 3** relies on a single item (**Item 1**). While **Item 1** has a strong loading, additional items (e.g., **Item 5**) may be needed to ensure the skill is robustly assessed.\n",
        "   - **Skill 2** shows moderate contributions from **Item 2** and **Item 4**, which might require review to ensure their alignment with this skill.\n",
        "\n",
        "2. **Ambiguous Items**:\n",
        "   - **Item 5** has a mixed loading (moderate on **Skill 1** and **Skill 3**), but its assignment to **Skill 1** aligns well with the overall structure.\n",
        "   - **Item 4** has weaker loadings but is still included under **Skill 2**, reflecting its statistical alignment while acknowledging its relative ambiguity.\n",
        "\n",
        "### Model Evaluation Metrics\n",
        "\n",
        "#### Calculating Proportion of Variance Explained ($R^2$)\n",
        "\n",
        "**For Factor Analysis**:\n"
      ],
      "id": "e7c640f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the communalities\n",
        "communalities = fa_model.get_communalities()\n",
        "\n",
        "# Total variance explained\n",
        "total_variance_explained = np.sum(communalities)\n",
        "\n",
        "# Total variance (number of variables)\n",
        "total_variance = item_data_scaled.shape[1]\n",
        "\n",
        "# Proportion of variance explained\n",
        "r_squared_fa = total_variance_explained / total_variance\n",
        "\n",
        "print(f\"Factor Analysis R^2: {r_squared_fa:.2f}\")"
      ],
      "id": "8e90f2ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- The $R^2$ value of **0.56** indicates that the three-factor model explains **56%** of the total variance in the data.\n",
        "- **Implications:**\n",
        "  - A proportion of variance explained greater than 50% is generally considered acceptable in exploratory **Factor Analysis**, especially with psychological or educational data where constructs are often complex.\n",
        "  - However, it also suggests that **44%** of the variance is not explained by the model, which may be due to measurement error, unique variance of items, or additional latent factors not captured by the model.\n",
        "\n",
        "**For PCA**:\n"
      ],
      "id": "0d01fc74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate cumulative variance explained\n",
        "cumulative_variance = np.cumsum(pca_model.explained_variance_ratio_)\n",
        "print(f\"PCA cumulative variance explained by first 3 components: {cumulative_variance[2]:.2f}\")"
      ],
      "id": "f3ccd5e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- The first three principal components explain **66%** of the total variance in the data.\n",
        "- **Implications:**\n",
        "  - This indicates a slightly better variance explanation than the **Factor Analysis** model.\n",
        "  - **PCA** aims to capture the maximum variance with the fewest components, so a higher cumulative variance explained is desirable.\n",
        "  - However, **PCA** components may not be as interpretable as factors from **Factor Analysis**, since **PCA** components are linear combinations that maximize variance without considering underlying latent constructs.\n",
        "\n",
        "**Comparison:**\n",
        "\n",
        "- The **PCA** model explains more variance (66%) compared to the **Factor Analysis** model (56%).\n",
        "- This difference may be due to the methodological differences between **PCA** and **Factor Analysis**:\n",
        "  - **PCA** focuses on capturing variance and is sensitive to the scale of the data.\n",
        "  - **Factor Analysis** models the underlying latent constructs and accounts for measurement error.\n",
        "\n",
        "**Considerations:**\n",
        "\n",
        "- **Adequacy of Variance Explained:**\n",
        "  - In social sciences, cumulative variance explained between 50% and 75% is generally acceptable.\n",
        "  - Both models fall within this range, but there is room for improvement.\n",
        "- **Unexplained Variance:**\n",
        "  - The unexplained variance suggests that additional factors or components might exist, or that some items do not fit well within the identified latent skills.\n",
        "\n",
        "#### Calculating Cohen's Kappa Coefficient\n",
        "\n",
        "I also examined the consistency of item assignments across methods using Cohen's kappa coefficient [@cohen1960coefficient].\n"
      ],
      "id": "78eedae8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# Map skill labels to numeric codes for Factor Analysis\n",
        "fa_skill_codes = final_q_matrix_df['Mapped_Skill'].map({'Skill_1': 0, 'Skill_2': 1, 'Skill_3': 2}).values\n",
        "\n",
        "# K-Means cluster labels\n",
        "kmeans_labels = kmeans.labels_\n",
        "\n",
        "# Compute confusion matrix\n",
        "confusion = confusion_matrix(fa_skill_codes, kmeans_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)\n",
        "\n",
        "# Align clusters with skills using the Hungarian algorithm\n",
        "row_ind, col_ind = linear_sum_assignment(-confusion)\n",
        "mapping = dict(zip(col_ind, row_ind))\n",
        "\n",
        "# Map K-Means labels to Factor Analysis skill codes\n",
        "kmeans_labels_mapped = np.array([mapping[label] for label in kmeans_labels])\n",
        "\n",
        "# Compute Cohen's kappa\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(fa_skill_codes, kmeans_labels_mapped)\n",
        "print(f\"Cohen's kappa after alignment: {kappa:.2f}\")"
      ],
      "id": "676f9628",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- **Confusion Matrix:**\n",
        "  - The confusion matrix shows perfect agreement between the methods after alignment:\n",
        "    - All items assigned to Skill 1 in **Factor Analysis** are also assigned to the corresponding cluster in **K-Means**.\n",
        "    - The same applies to Skills 2 and 3.\n",
        "- **Cohen's Kappa Value:**\n",
        "  - A Kappa value of **1.00** indicates **perfect agreement** between the two methods after alignment.\n",
        "- **Implications:**\n",
        "  - This high level of agreement suggests that both methods are consistently identifying the same underlying item-skill structures.\n",
        "  - It provides strong validation for the robustness of your item-skill mappings.\n",
        "\n",
        "**Considerations:**\n",
        "\n",
        "- **Alignment Step:**\n",
        "  - The necessity of aligning clusters to skills underscores that cluster labels are arbitrary.\n",
        "  - It's important to perform this alignment to make meaningful comparisons.\n",
        "\n",
        "- **Cohen's Kappa Interpretation:**\n",
        "  - Kappa values range from -1 to 1, where:\n",
        "    - **< 0**: Less than chance agreement.\n",
        "    - **0â€“0.20**: Slight agreement.\n",
        "    - **0.21â€“0.40**: Fair agreement.\n",
        "    - **0.41â€“0.60**: Moderate agreement.\n",
        "    - **0.61â€“0.80**: Substantial agreement.\n",
        "    - **0.81â€“1.00**: Almost perfect agreement.\n",
        "  - A value of **1.00** confirms that the two methods are in complete concordance post-alignment.\n",
        "\n",
        "### Overall Evaluation\n",
        "\n",
        "**Strengths:**\n",
        "\n",
        "- **Converging Evidence:**\n",
        "  - The high Cohen's Kappa value indicates that different analytical methods converge on the same item-skill mappings, enhancing confidence in the results.\n",
        "- **Variance Explained:**\n",
        "  - Both **Factor Analysis** and **PCA** explain a substantial portion of the variance, supporting the validity of the three-component model.\n",
        "- **Methodological Rigor:**\n",
        "  - My approach of using multiple methods and comparing them through quantitative metrics strengthens the robustness of the findings.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "- **Variance Not Explained:**\n",
        "  - Approximately 34% to 44% of the variance remains unexplained, which could be due to:\n",
        "    - Measurement error.\n",
        "    - Additional latent skills not captured by the model.\n",
        "    - Unique variances of items.\n",
        "- **Assumptions of Methods:**\n",
        "  - **Factor Analysis** and **PCA** assumptions may not be fully met with binary data, which could affect the variance explained.\n",
        "\n",
        "#### Verifying Item-Skill Mappings\n",
        "\n",
        "```{{python}}\n",
        "final_q_matrix_df\n",
        "```"
      ],
      "id": "ac2750ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-verify1\n",
        "#| tbl-cap: Factor Analysis Mappings\n",
        "#| echo: false\n",
        "\n",
        "final_q_matrix_df"
      ],
      "id": "tbl-verify1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- **Item Assignments**: Each item is assigned to the skill with which it has the highest factor loading from the final Q-matrix.\n",
        "- **Skill Representation**:\n",
        "  - **Skill_1**: Items 3, 5, 7, 8\n",
        "  - **Skill_2**: Items 2, 4, 6\n",
        "  - **Skill_3**: Item 1\n",
        "\n",
        "**Significance:**\n",
        "\n",
        "- **Consistent Mapping**: The assignments reflect the conclusions drawn from my **Factor Analysis**.\n",
        "- **Foundation for Comparison**: These mappings serve as the reference point for comparing with the **K-Means Clustering** results.\n",
        "\n",
        "```{{python}}\n",
        "kmeans_q_matrix_df\n",
        "```"
      ],
      "id": "e1ac8639"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-verify2\n",
        "#| tbl-cap: K-Means Clustering Mappings (before alignment)\n",
        "#| echo: false\n",
        "\n",
        "kmeans_q_matrix_df"
      ],
      "id": "tbl-verify2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- **Cluster Assignments**: Items are assigned to clusters labeled as **Skill_1**, **Skill_2**, or **Skill_3**, based on the **K-Means Clustering** algorithm.\n",
        "- **Arbitrary Labels**: The cluster labels (e.g., **Skill_1**, **Skill_2**) are assigned by the algorithm and do not necessarily correspond to the skills identified in **Factor Analysis**.\n",
        "\n",
        "**Significance:**\n",
        "\n",
        "- **Initial Comparison**: At first glance, the mappings appear similar to the **Factor Analysis** mappings, but due to arbitrary labeling, a direct comparison isn't meaningful yet.\n",
        "- **Need for Alignment**: To accurately compare the item-skill assignments, cluster labels must be aligned with the skills from **Factor Analysis**.\n"
      ],
      "id": "07fb5d97"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Map clusters to skills after alignment\n",
        "kmeans_skill_names_aligned = ['Skill_' + str(mapping[label] + 1) for label in kmeans_labels]\n",
        "kmeans_q_matrix_df_aligned = kmeans_q_matrix_df.copy()\n",
        "kmeans_q_matrix_df_aligned['Mapped_Skill'] = kmeans_skill_names_aligned"
      ],
      "id": "cf613834",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Process:**\n",
        "\n",
        "- **Alignment Using the Hungarian Algorithm**:\n",
        "  - Since cluster labels are arbitrary, I used the **Hungarian algorithm** (also known as the **linear sum assignment method**) to find the optimal one-to-one mapping between clusters and skills.\n",
        "  - This algorithm minimizes the total disagreement between the two sets of labels.\n",
        "\n",
        "- **Mapping Clusters to Skills**:\n",
        "  - I created a mapping dictionary (`mapping`) that aligns each cluster label with the corresponding skill from **Factor Analysis**.\n",
        "  - This ensures that clusters are correctly interpreted in the context of the identified skills.\n",
        "\n",
        "```{{python}}\n",
        "kmeans_q_matrix_df_aligned\n",
        "```"
      ],
      "id": "61ab515d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-verify3\n",
        "#| tbl-cap: K-Means Clustering Mappings (after alignment)\n",
        "#| echo: false\n",
        "\n",
        "kmeans_q_matrix_df_aligned"
      ],
      "id": "tbl-verify3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- **Aligned Assignments**: After alignment, the cluster labels now correspond to the same skills as in the **Factor Analysis** mappings.\n",
        "- **Perfect Agreement**: The item-skill assignments from **K-Means Clustering** match exactly with those from **Factor Analysis**.\n",
        "\n",
        "**Significance:**\n",
        "\n",
        "- **Validation of Consistency**: The perfect match indicates strong agreement between the two methods.\n",
        "- **Robustness of Findings**: The consistency across methods reinforces the reliability of the item-skill mappings.\n",
        "\n",
        "## Discussion\n",
        "\n",
        "### Overview of Model Comparison and Selection\n",
        "\n",
        "#### Model Complexity and Interpretability\n",
        "\n",
        "After comparing models with two, three, and four components, the **three-component Factor Analysis model** emerged as the most suitable representation of the latent skills in the dataset.\n",
        "\n",
        "**Two-Component Model**\n",
        "\n",
        "- **Simplicity:** The two-component model is the simplest, reducing the latent skills to two factors.\n",
        "- **Interpretability:**\n",
        "  - Some items showed weak loadings or ambiguous associations.\n",
        "  - **Item 1**, for example, had very low loadings on both factors, suggesting it doesn't fit well within this model.\n",
        "- **Implications:**\n",
        "  - The model may be too simplistic, failing to capture important nuances in the data.\n",
        "  - It potentially merges distinct skills into broader categories, which could obscure meaningful distinctions.\n",
        "\n",
        "**Three-Component Model**\n",
        "\n",
        "- **Balance:** Offers a middle ground between simplicity and complexity.\n",
        "- **Interpretability:**\n",
        "  - Provides clear and distinct latent skills.\n",
        "  - Most items load strongly on a single factor, enhancing interpretability.\n",
        "- **Findings:**\n",
        "  - The model captures the nuances in the data without unnecessary complexity.\n",
        "  - **Item 5** shows moderate loadings on two skills, indicating split influences but remains interpretable.\n",
        "\n",
        "**Four-Component Model**\n",
        "\n",
        "- **Complexity:** Introduces additional complexity with a fourth factor.\n",
        "- **Interpretability:**\n",
        "  - Overlapping loadings make the model harder to interpret.\n",
        "  - Some items load significantly on multiple factors, causing ambiguity.\n",
        "- **Implications:**\n",
        "  - The added complexity doesn't substantially increase explained variance.\n",
        "  - May overfit the data, capturing noise rather than meaningful structure.\n",
        "\n",
        "**Trade-Offs:**\n",
        "\n",
        "- The **two-component model** may underfit, missing key distinctions between skills.\n",
        "- The **four-component model** may overfit, adding unnecessary complexity without practical benefits.\n",
        "\n",
        "**Optimal Complexity:**\n",
        "\n",
        "- The **three-component model** strikes a balance, capturing essential structures while maintaining interpretability.\n",
        "\n",
        "#### Variance Explained and Model Fit\n",
        "\n",
        "**Factor Analysis Variance Explained**\n",
        "\n",
        "- **Two-Component Model:**\n",
        "  - Lower proportion of variance explained (less than 56%).\n",
        "  - Indicates insufficient capture of the data's variability.\n",
        "- **Three-Component Model:**\n",
        "  - Explains approximately **56%** of the total variance.\n",
        "  - Represents a reasonable fit for exploratory purposes.\n",
        "- **Four-Component Model:**\n",
        "  - Slight increase in variance explained.\n",
        "  - Not significant enough to justify added complexity.\n",
        "\n",
        "**PCA Variance Explained**\n",
        "\n",
        "- **Three-Component Model:**\n",
        "  - Cumulative variance explained is **66%**.\n",
        "  - Indicates a substantial capture of data variability.\n",
        "- **Comparison:**\n",
        "  - **PCA** generally explains more variance than **Factor Analysis** in your findings.\n",
        "  - However, **PCA** components may not be as interpretable in terms of latent skills.\n",
        "\n",
        "**Thresholds:** In social sciences, explaining around 50-75% variance is acceptable.\n",
        "\n",
        "**Diminishing Returns:** The variance explained by adding a fourth component doesn't justify the increased complexity.\n",
        "\n",
        "**Model Fit:** The three-component model provides an acceptable fit with reasonable simplicity.\n",
        "\n",
        "#### Consistency Across Methods\n",
        "\n",
        "**Agreement Among Methods**\n",
        "\n",
        "- **Three-Component Model:**\n",
        "  - High consistency in item-skill mappings across **Factor Analysis**, **K-Means Clustering**, and **PCA**.\n",
        "  - **Cohen's Kappa Coefficient** of **1.00** after alignment indicates perfect agreement.\n",
        "- **Two- and Four-Component Models:**\n",
        "  - Less consistent across methods.\n",
        "  - Ambiguities in item assignments due to overlapping loadings.\n",
        "\n",
        "**Reinforcement:**\n",
        "\n",
        "- Different methods converging on the same solution supports the robustness of the three-component model.\n",
        "\n",
        "**Practical Implications:**\n",
        "\n",
        "- A consistent model is more reliable for educational applications, such as test design and interpretation.\n",
        "\n",
        "#### Model Evaluation Metrics\n",
        "\n",
        "**Proportion of Variance Explained ($R^2$)**\n",
        "\n",
        "- **Factor Analysis:**\n",
        "  - **Three-Component Model ($R^2$):** Approximately **0.56**.\n",
        "  - Indicates that 56% of the variance is captured by the model.\n",
        "- **PCA:**\n",
        "  - **Three-Component Model Cumulative Variance:** **66%**.\n",
        "  - Suggests a better variance capture, but **PCA** components may be less interpretable.\n",
        "\n",
        "**Cohen's Kappa Coefficient**\n",
        "\n",
        "- **Value:** **1.00** after alignment.\n",
        "- **Interpretation:**\n",
        "  - Indicates perfect agreement between item-skill mappings from **Factor Analysis** and **K-Means Clustering**.\n",
        "- **Significance:**\n",
        "  - Validates the consistency and reliability of the three-component model.\n",
        "\n",
        "**Balance of Metrics:**\n",
        "\n",
        "- The three-component model provides a good balance between variance explained and interpretability.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "- Acknowledge that a portion of variance remains unexplained.\n",
        "- Suggests potential areas for further investigation or alternative modeling approaches.\n",
        "\n",
        "#### Final Model Selection\n",
        "\n",
        "**Reasons for Selecting the Three-Component Model**\n",
        "\n",
        "- **Optimal Balance:**\n",
        "  - Captures essential structures without overcomplicating the model.\n",
        "- **High Interpretability:**\n",
        "  - Clear item-skill relationships make it practical for educational use.\n",
        "- **Strong Validation:**\n",
        "  - Consistent findings across multiple methods reinforce its selection.\n",
        "- **Model Performance:**\n",
        "  - Satisfactory variance explained and perfect agreement in item assignments.\n",
        "\n",
        "**Implications for the Q-Matrix**\n",
        "\n",
        "- **Robust Mapping:**\n",
        "  - The final Q-matrix derived from the three-component model provides a reliable item-skill mapping.\n",
        "- **Educational Utility:**\n",
        "  - Enhances interpretability of test results.\n",
        "  - Aids in identifying areas for instructional focus and intervention.\n",
        "\n",
        "### Justification for the Final Q-Matrix\n",
        "\n",
        "#### Derivation from Multiple Methods\n",
        "\n",
        "**Integration of Analytical Findings**\n",
        "\n",
        "- **Factor Analysis**: The Final Q-Matrix is primarily based on the results of the three-component **Factor Analysis**, where each item is assigned to the skill with the highest factor loading.\n",
        "\n",
        "- **K-Means Clustering** and **PCA**: The item-skill mappings derived from these methods align closely with the **Factor Analysis** results, reinforcing the assignments in the Final Q-Matrix.\n",
        "\n",
        "  - **Consistency in Item Groupings**: Items that cluster together in **K-Means** and load on the same principal components in **PCA** correspond to the same skills identified in **Factor Analysis**.\n",
        "\n",
        "- **Converging Evidence**: The consistent findings across multiple methods provide strong evidence that the item-skill assignments in the Final Q-Matrix accurately reflect the underlying knowledge structure.\n",
        "\n",
        "- **Robustness**: Using different analytical techniques reduces the likelihood that the results are artifacts of a specific method, increasing confidence in the Q-Matrix.\n",
        "\n",
        "#### Support from Model Evaluation Metrics\n",
        "\n",
        "**Variance Explained**\n",
        "\n",
        "- **Factor Analysis ($R^2$)**: The three-component model explains approximately **56%** of the total variance.\n",
        "\n",
        "- **PCA Variance**: The first three principal components account for **66%** of the variance.\n",
        "\n",
        "**Cohen's Kappa Coefficient**\n",
        "\n",
        "- **Value of 1.00**: Indicates perfect agreement between the item-skill mappings from **Factor Analysis** and **K-Means Clustering** after alignment.\n",
        "\n",
        "- **Adequate Model Fit**: The proportion of variance explained suggests that the model captures a substantial amount of the data's variability, which is acceptable in exploratory analyses.\n",
        "\n",
        "- **Validation of Mappings**: The perfect Cohen's Kappa score confirms that different methods agree on the item-skill assignments, supporting the validity of the Final Q-Matrix.\n",
        "\n",
        "#### Balance of Complexity and Interpretability\n",
        "\n",
        "**Model Selection**\n",
        "\n",
        "- **Three-Component Model**: Chosen for providing the best balance between capturing sufficient detail and maintaining simplicity.\n",
        "\n",
        "- **Avoiding Overfitting**: The four-component model introduced complexity without significant gains in variance explained, making it less interpretable.\n",
        "\n",
        "- **Preventing Oversimplification**: The two-component model failed to capture important nuances, with some items not fitting well.\n",
        "\n",
        "- **Practical Interpretability**: The three-component model allows for clear and distinct item-skill relationships, making the Q-Matrix practical for educational purposes.\n",
        "\n",
        "#### Consistency Across Analytical Methods\n",
        "\n",
        "**Alignment of Results**\n",
        "\n",
        "- **Factor Analysis, K-Means Clustering, and PCA** all indicate similar item-skill groupings.\n",
        "\n",
        "- **Mermaid Diagrams and Heatmaps**: Provide visual confirmation of the consistent item-skill relationships across methods.\n",
        "\n",
        "- **Cross-Method Validation**: Consistency across methods strengthens the argument that the Final Q-Matrix accurately represents the latent skills.\n",
        "\n",
        "- **Reinforcement of Findings**: Visual tools help illustrate the robustness of the mappings, making the justification more compelling.\n",
        "\n",
        "#### Educational Relevance and Practicality\n",
        "\n",
        "**Actionable Insights**: The Q-Matrix provides educators with clear information about which items assess which skills, facilitating targeted instruction and remediation.\n",
        "\n",
        "**Test Design Improvement**: Understanding item-skill relationships helps in refining assessments to better measure the intended skills.\n",
        "\n",
        "By understanding the relationships between items and skills, test designers can create assessments that more effectively target specific skills, ensuring a balanced coverage of the identified latent skills. The item-skill mappings can also help identify potentially redundant or less informative items, allowing for more efficient and focused assessments.\n",
        "\n",
        "Moreover, educators can leverage the findings to diagnose student strengths and weaknesses at the skill level. The identification of specific skills associated with each item enables targeted remediation or enrichment activities, focusing on the areas where students may need additional support. This information can also guide the development of instructional materials and resources, ensuring that students have ample opportunities to practice and master the identified skills.\n",
        "\n",
        "### Limitations and Future Work\n",
        "\n",
        "Despite the insights provided by this study, there are limitations to consider.\n",
        "\n",
        "**Acknowledging Split Influences**\n",
        "\n",
        "- **Item 5**: Exhibits moderate loadings on both **Skill 1** and **Skill 3**.\n",
        "\n",
        "**Justification**:\n",
        "\n",
        "- **Assignment Based on Dominant Loading**: Despite the split influence, **Item 5** is assigned to **Skill 1** due to its higher loading, aligning with the overall structure.\n",
        "\n",
        "- **Consideration for Revision**: Recognizing the split influence allows for potential item revision to enhance its alignment with a single skill.\n",
        "\n",
        "**Ensuring Skill Representation**\n",
        "\n",
        "- **Skill 3**: Currently represented by a single item (**Item 1**).\n",
        "\n",
        "**Justification**:\n",
        "\n",
        "- **Recognition of Limitations**: Acknowledging that **Skill 3** relies on a single item highlights an area for potential expansion in future assessments.\n",
        "\n",
        "- **Maintaining Integrity**: Despite the limited representation, the strong loading of **Item 1** on **Skill 3** justifies its inclusion in the Q-Matrix.\n",
        "\n",
        "- **Binary Data Consideration**:\n",
        "  - The use of **Factor Analysis** and **PCA** on binary data may not fully meet the assumptions of these methods. Future research could explore the application of **Item Response Theory (IRT)** models specifically designed for analyzing binary response data [@van2015handbook].\n",
        "\n",
        "- **Sample Size and Generalizability**:\n",
        "  - The small sample size of eight items limits the generalizability of the findings. Replicating the study with a larger set of items and a more diverse student population would help validate the identified skill structure and its applicability to different educational contexts.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This study significantly contributes to the field of educational assessment and learning analytics by demonstrating the effectiveness of a comprehensive, multi-method approach to uncovering latent skill structures in an eight-item test dataset. By leveraging the complementary strengths of **Factor Analysis**, **K-Means Clustering**, and **Principal Component Analysis (PCA)**, I identified a robust and interpretable three-skill model that best represents the underlying knowledge structure.\n",
        "\n",
        "**Key findings of this study include:**\n",
        "\n",
        "1. **Identification of Three Distinct Latent Skills:** These skills capture the essential relationships among the test items, providing a clearer understanding of the knowledge assessed.\n",
        "\n",
        "2. **Development of a Final Q-Matrix:** The Q-matrix offers a precise and empirically derived mapping of items to skills, consistent across multiple analytical methods, enhancing the reliability of skill assessment.\n",
        "\n",
        "3. **Validation of Item-Skill Relationships:** Cross-validation using multiple methods supports the interpretability of the identified skill structure, confirming the robustness of the findings.\n",
        "\n",
        "The practical significance of this work lies in its potential to inform and enhance educational assessment and instructional practices. By providing a more precise understanding of the skills assessed by individual test items, this study enables educators and test designers to:\n",
        "\n",
        "- **Develop Targeted Assessments:** Create more focused and efficient assessments that effectively measure specific skills.\n",
        "- **Identify Student Needs:** Pinpoint areas where students may require additional support or remediation based on their performance on skill-related items.\n",
        "- **Design Aligned Instructional Interventions:** Develop instructional resources that align with the identified skill structure, promoting more personalized and adaptive learning experiences.\n",
        "\n",
        "Moreover, the multi-method approach presented in this study serves as a valuable template for future research in educational data mining and learning analytics. Researchers can build upon this methodology to investigate knowledge structures underlying different types of assessments, learning materials, and educational contexts.\n",
        "\n",
        "**Future research should address this study's limitations and explore new avenues for extending its findings. Specific opportunities include:**\n",
        "\n",
        "1. **Applying Item Response Theory (IRT) Models:** Utilize IRT models, which are specifically designed to analyze binary response data, to validate and refine the identified skill structure.\n",
        "2. **Expanding the Dataset:** Replicate the study with larger and more diverse datasets, including assessments with a greater number of items and student populations from various educational backgrounds, to enhance generalizability.\n",
        "3. **Exploring Generalizability Across Contexts:** Investigate the applicability of the identified skill structure across different domains, grade levels, and assessment formats.\n",
        "4. **Integrating with Adaptive Learning Systems:** Explore the integration of the derived Q-matrix with adaptive learning systems and intelligent tutoring platforms to enable real-time, skill-based feedback and personalized learning paths.\n",
        "\n",
        "By addressing these challenges and opportunities, future research can further advance our understanding of **knowledge structure mapping** and its applications in educational settings, ultimately contributing to the development of more effective and equitable learning experiences for all students.\n",
        "\n",
        "### Submission Guidelines\n",
        "\n",
        "This document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed."
      ],
      "id": "2c5d309b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Library/Frameworks/Python.framework/Versions/3.12/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}