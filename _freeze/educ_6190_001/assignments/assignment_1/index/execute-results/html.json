{
  "hash": "d3446db5fbc078eeb9a5bd4f7247064a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\npagetitle: \"John Baker – Learning Analytics\"\ntitle: \"Enhancing Data Quality in Intelligent Tutoring Systems\"\ndescription: \"A Pipeline for Analyzing Conversational Agent Language Style and Its Impact on Written Summaries\"\ndate: 2025-02-11\n# date-modified: \nauthor: \n  - name: John Baker\n    email: jbaker1@upenn.edu\n    affiliation:\n      - name: \"Penn GSE: University of Pennsylvania Graduate School of Education\"\n        url: https://www.gse.upenn.edu/\nabstract: |\n  This project presents a comprehensive data processing pipeline designed to enhance the quality of raw log data from an Intelligent Tutoring System (ITS) and to evaluate the impact of conversational agent language styles on the quality of written summaries. Using AutoTutor ARC, adult participants engaged in pretest and posttest lessons where the language style of the conversational agents was varied among formal, informal, and mixed modalities. The raw data, characterized by multiple attempts per participant, missing values, and skewed distributions, necessitated a systematic approach to data cleaning and transformation. Our pipeline groups data by unique identifiers, retains the first non-null responses, detects and replaces outliers, applies a logarithmic transformation to reduce skewness, and normalizes key features via min-max scaling. The resulting data not only supports robust statistical analyses but also provides clear insights into how conversational agent language style influences learning outcomes. This work underscores the importance of rigorous data preparation in ITS research and offers a replicable framework for future studies examining the efficacy of adaptive educational technologies.\nkeywords:\n  - Intelligent Tutoring Systems\n  - Conversational Agents\n  - Educational Data Analysis\nbibliography: bibliography/bibliography.bib\nnocite: |\n  @*\nimage: images/image_fx_.jpg\nformat:\n  html:\n    code-link: false\ndraft: false\njupyter: python3\nipynb-shell-interactivity: all\nexecute: \n  freeze: true\n---\n\n\n## Introduction\n  \nIntelligent Tutoring Systems (ITS) have emerged as powerful tools for enhancing learning outcomes across various domains, including reading comprehension. These systems often employ conversational agents to interact with learners, providing personalized feedback and guidance. However, the effectiveness of these agents may depend on various factors, including their language style. This study investigates the impact of the conversational agents' language style on the quality of written summaries produced by adult participants using AutoTutor ARC, an ITS designed to improve reading comprehension.\n\nAutoTutor ARC collects extensive log data throughout the learning process, capturing participants' interactions with the system across multiple lessons. The study design includes pretest assessments (lessons 2 and 3) and posttest assessments (lessons 9 and 10), enabling the comparison of performance before and after the intervention. The conversational agents' language style is varied between formal, informal, and mixed styles to assess its influence on learning outcomes.\n\nGiven the inherent complexity of the raw log data, which is characterized by multiple attempts per participant and a diverse range of data formats, a systematic data processing pipeline is crucial. This pipeline aims to prepare the data for meaningful analysis by addressing common issues such as missing values, outliers, and skewed distributions. Furthermore, feature engineering techniques are employed to better capture the relevant variables for subsequent statistical analysis.\n\nThe present assignment focuses on the importance of numerical data in the context of this study, discussing its background, related work, and techniques. Moreover, it provides considerations and cautions when dealing with numerical data in the realm of machine learning and ITS research. By examining the role of data preparation in the AutoTutor ARC study, this assignment contributes to the broader understanding of how data processing techniques can support the evaluation and improvement of ITS interventions.\n\n## Background and Related Work\n\n### Importance of Numerical Data\n\nData mining involves preprocessing and transforming data to generate patterns using analysis tools and algorithms. Outlying variables in recorded data can significantly affect the robustness of a model, making the identification of outliers before data analysis essential [@awawdeh2019application]. Extreme values can cause problems in statistical analysis by increasing error variance, reducing statistical power, and biasing estimates. Therefore, screening data for extreme scores is crucial [@osborne2012best].\n\nData cleaning, which includes screening for extreme scores, missing data, and normality, is a critical step in quantitative research to ensure the validity of results. However, this step is often overlooked [@osborne2012best]. Transformations are often necessary in data analysis to address issues such as non-normal distribution of errors in linear regression. These transformations can change the scale of variables, alter relationships between variables, and modify the error distributions [@pek2017data].\n\n### Outlier Detection\n\nOutliers can be detected using various methods, such as re-weighted least squares (Re-WLS). The bisquare weights method can be used to minimize the effect of outliers in least-squares fitting, where points far from the fitted line will get zero weight and be considered outliers. Outliers can be addressed by either deleting or transforming them, depending on the number of outliers present. Deleting outliers is suitable when there are a limited number of outliers, while transformation is used when there are many [@awawdeh2019application].\n\nOutlier detection methods have been studied and applied in various fields, including intrusion detection, wireless sensor networks, satellite image analysis, motion segmentation, and weather prediction [@awawdeh2019application].\n\n### Data Transformation\n\nData transformations are used to address non-normality. However, transformations can change the nature of the effect and should be applied carefully when interpreting effect sizes [@pek2017data]. Various transformations include logarithmic, square root, and reciprocal, with logarithmic transformations being particularly useful for compressing heavy-tailed distributions. The Box-Cox transformation is a family of power transforms that includes the log  [@zheng2018feature].\n\nWinsorizing and trimming are techniques used to address data contamination by replacing or removing extreme values. However, these transformations can bias effect size estimates when extreme cases are not outliers. Reverse transformations are not generally recommended because inferential results do not necessarily map back onto the original effect [@pek2017data].\n\n### Feature Scaling and Normalization\n\nFeature scaling, or normalization, is used to change the scale of the feature, which may be necessary for models sensitive to the input scale. Common scaling methods include min-max scaling, standardization, and L2 normalization [@zheng2018feature]. Min-max scaling squeezes data to be within a specific range, while standardization results in data with a mean of 0 and a variance of 1 [@awawdeh2019application; @zheng2018feature]. Feature scaling does not change the shape of the distribution, only the scale [@zheng2018feature].\n\n### Interaction Features\n\nInteraction features are created by combining pairs of input features, allowing models to capture interactions between features [@zheng2018feature].\n\n### Feature Selection\n\nFeature selection techniques, such as filtering, wrapper, and embedded methods, are used to reduce the computational expense of using many features and to identify the most useful features [@zheng2018feature].\n\n### Considerations and Cautions\n\nWhen dealing with numerical data, it is essential to consider that the magnitude of the data may or may not be important, depending on the situation. The distribution of numerical features matters for some models, and the Central Limit Theorem (CLT) makes the normality of errors less relevant when the sample size is large enough [@pek2017data; @zheng2018feature].\n\nTransformations should be used to improve effect size interpretation and to address non-normality when sample sizes are small [@pek2017data]. Data visualization is critical for understanding data and the effect of transformations [@zheng2018feature]. The choice of method depends on whether the goal is statistical prediction or statistical inference [@pek2017data].\n\n## Methods\n\nThe methodological framework for this project can be summarized in the following steps:\n\n1. **Data Ingestion and Preliminary Cleaning:**  \n   Raw data is imported from an Excel file hosted on Google Drive. Unique identifiers (e.g., student IDs, lesson IDs) are used to group the data, ensuring that, for each participant, only the earliest non-null response is retained when multiple attempts are recorded.\n\n2. **Feature Engineering and Labeling:**  \n   A new variable is introduced to distinguish between pretest and posttest conditions, based on the lesson identifier. In addition, specific columns are renamed for clarity (e.g., renaming `Q5Duration` to `WritingTime`).\n\n3. **Visualization and Descriptive Analysis:**  \n   Histograms and summary statistics are generated to examine the distribution of the writing time data, revealing the presence of outliers and a highly skewed distribution.\n\n4. **Outlier Treatment and Data Transformation:**  \n   Outliers are identified using a three-standard-deviation rule and are replaced with lesson-specific means. A logarithmic transformation is then applied to reduce the skewness of the writing time variable, resulting in a distribution more amenable to parametric analysis.\n\n5. **Feature Scaling:**  \n   The writing time variable is normalized using min-max scaling to facilitate further analysis and potential machine learning applications.\n\n6. **Data Quality Assurance and Aggregation:**  \n   Duplicate records and missing values are identified and addressed. Finally, the dataset is aggregated by test condition (pretest vs. posttest) to generate summary statistics that inform subsequent analyses.\n\n## Implementation Details\n\n### Data Preprocessing: Handling Multiple Attempts\n\nThe core preprocessing is encapsulated in the `process_log_data` function. This function takes a raw DataFrame, groups records by unique identifiers (e.g., student and lesson IDs), and then selects the first non-null response for each repeated measure (such as responses to multiple questions or their corresponding durations).\n\n::: {#d6d27ba4 .cell execution_count=1}\n``` {.python .cell-code}\ndef process_log_data(df):\n    \"\"\"\n    Process log data by extracting first attempts and handling missing values\n    by looking at subsequent attempts.\n    \"\"\"\n    # Define identifier and repeated columns\n    base_cols = ['RecordID', 'ClassID', 'UserID', 'LessonID']\n    attempt_cols = ['LessonAttempt', 'TotalTime', 'XML']\n    data_cols = [f'Q{i}Data' for i in range(1, 10)]\n    duration_cols = [f'Q{i}Duration' for i in range(1, 10)]\n\n    # List to store processed rows\n    processed_data = []\n\n    # Group the data by the base columns (unique combination per record)\n    grouped = df.groupby(base_cols)\n\n    for name, group in grouped:\n        # Start with the key identifiers\n        row_dict = dict(zip(base_cols, name))\n\n        # For each data and duration column, pick the first non-null attempt\n        for col in data_cols + duration_cols:\n            row_dict[col] = group[col].iloc[0]  # Take the first attempt\n            if pd.isna(row_dict[col]):\n                # If missing, search subsequent attempts\n                for attempt in range(1, len(group)):\n                    if not pd.isna(group[col].iloc[attempt]):\n                        row_dict[col] = group[col].iloc[attempt]\n                        break\n\n        # Also add attempt-specific columns\n        row_dict['LessonAttempt'] = group['LessonAttempt'].iloc[0]\n        row_dict['TotalTime'] = group['TotalTime'].iloc[0]\n        row_dict['XML'] = group['XML'].iloc[0]\n\n        processed_data.append(row_dict)\n\n    # Convert the list of dictionaries into a DataFrame with a specified column order\n    result_df = pd.DataFrame(processed_data)\n    column_order = base_cols + ['LessonAttempt', 'TotalTime', 'XML'] + data_cols + duration_cols\n\n    return result_df[column_order]\n```\n:::\n\n\n*Key aspects of the function include:*\n- **Grouping:** Data is grouped by `RecordID`, `ClassID`, `UserID`, and `LessonID` to handle multiple attempts by the same student.\n- **Iteration:** For each group, the function selects the first non-null value for each data and duration column, preserving the earliest response.\n- **Output:** The function returns a cleaned DataFrame with a single row per unique record, with logically ordered columns.\n\n### Loading Data\nIn this segment, the code reads an Excel file containing the log data. This file holds data from multiple lessons that are later used to compare pretest and posttest performance.\n\n::: {#3f6a96e8 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\npd.set_option('display.max_columns', None)\n\nsummary_log_data = pd.read_excel(\n    '/Users/john/Library/CloudStorage/Box-Box/Website/educ_6190_001/assignments/assignment_1/mnt/data/summary_data_-_lesson_2-3-9-10.xlsx',\n    sheet_name='Sheet1'\n)\n```\n:::\n\n\n### Applying the Processing Function and Creating a Test Variable\n\nThe raw data is processed using our custom function.\n\n::: {#ddfc100d .cell execution_count=3}\n``` {.python .cell-code}\nprocessed_df = process_log_data(summary_log_data)\nprocessed_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RecordID</th>\n      <th>ClassID</th>\n      <th>UserID</th>\n      <th>LessonID</th>\n      <th>LessonAttempt</th>\n      <th>TotalTime</th>\n      <th>XML</th>\n      <th>Q1Data</th>\n      <th>Q2Data</th>\n      <th>Q3Data</th>\n      <th>Q4Data</th>\n      <th>Q5Data</th>\n      <th>Q6Data</th>\n      <th>Q7Data</th>\n      <th>Q8Data</th>\n      <th>Q9Data</th>\n      <th>Q1Duration</th>\n      <th>Q2Duration</th>\n      <th>Q3Duration</th>\n      <th>Q4Duration</th>\n      <th>Q5Duration</th>\n      <th>Q6Duration</th>\n      <th>Q7Duration</th>\n      <th>Q8Duration</th>\n      <th>Q9Duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>formal</td>\n      <td>formal5_02</td>\n      <td>lesson2</td>\n      <td>1</td>\n      <td>379.071</td>\n      <td>Lesson2-Flood.xml</td>\n      <td>Next</td>\n      <td>Arousal8_Pleasant8</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>Floods are one of the most common natural disa...</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>72.531</td>\n      <td>7.484</td>\n      <td>19.874</td>\n      <td>8.031</td>\n      <td>177.328</td>\n      <td>6.750</td>\n      <td>23.859</td>\n      <td>35.843</td>\n      <td>15.156</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>formal</td>\n      <td>formal5_02</td>\n      <td>lesson3</td>\n      <td>1</td>\n      <td>358.535</td>\n      <td>Lesson3-Hurricane.xml</td>\n      <td>Next</td>\n      <td>Arousal9_Pleasant5</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>The two most destructive hurricanes to ever hi...</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>119.062</td>\n      <td>4.593</td>\n      <td>4.999</td>\n      <td>3.890</td>\n      <td>119.484</td>\n      <td>3.000</td>\n      <td>28.343</td>\n      <td>20.140</td>\n      <td>40.203</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>formal</td>\n      <td>formal5_02</td>\n      <td>lesson9</td>\n      <td>1</td>\n      <td>452.599</td>\n      <td>Lesson9-Job.xml</td>\n      <td>Next</td>\n      <td>Arousal9_Pleasant5</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>The United States job market has deteriorated....</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>71.531</td>\n      <td>6.796</td>\n      <td>2.687</td>\n      <td>3.828</td>\n      <td>202.812</td>\n      <td>4.609</td>\n      <td>97.828</td>\n      <td>25.796</td>\n      <td>24.296</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>formal</td>\n      <td>formal5_02</td>\n      <td>lesson10</td>\n      <td>1</td>\n      <td>395.415</td>\n      <td>Lesson10-Butterfly.xml</td>\n      <td>Next</td>\n      <td>Arousal5_Pleasant5</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>The butterfly and the moth have a lot of thing...</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>73.031</td>\n      <td>6.078</td>\n      <td>4.078</td>\n      <td>3.890</td>\n      <td>227.796</td>\n      <td>3.718</td>\n      <td>22.093</td>\n      <td>21.078</td>\n      <td>21.031</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>formal</td>\n      <td>formal16_02</td>\n      <td>lesson2</td>\n      <td>1</td>\n      <td>537.568</td>\n      <td>Lesson2-Flood.xml</td>\n      <td>Next</td>\n      <td>Arousal7_Pleasant3</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>Flood is one of the natural disaster that crea...</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>27.437</td>\n      <td>14.343</td>\n      <td>19.828</td>\n      <td>6.968</td>\n      <td>377.781</td>\n      <td>7.625</td>\n      <td>6.765</td>\n      <td>21.375</td>\n      <td>14.875</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAfter processing the raw data with the custom function, a new column (`test`) is created to distinguish between pretest (lessons 2 and 3) and posttest (lessons 9 and 10) conditions. The mapping is verified by printing the distribution of the test types and a sample of the processed data.\n\n::: {#6120b288 .cell execution_count=4}\n``` {.python .cell-code}\nprocessed_df['test'] = processed_df['LessonID'].map({\n    'lesson2': 'pretest',\n    'lesson3': 'pretest',\n    'lesson9': 'posttest',\n    'lesson10': 'posttest'\n})\n\nprint(\"\\nDistribution of test types:\")\nprint(processed_df['test'].value_counts())\n\nprint(\"\\nSample of processed data with test variable:\")\nprint(processed_df[['LessonID', 'test']].head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDistribution of test types:\ntest\nposttest    386\npretest     381\nName: count, dtype: int64\n\nSample of processed data with test variable:\n   LessonID      test\n0   lesson2   pretest\n1   lesson3   pretest\n2   lesson9  posttest\n3  lesson10  posttest\n4   lesson2   pretest\n5   lesson3   pretest\n6   lesson9  posttest\n7  lesson10  posttest\n8   lesson2   pretest\n9   lesson3   pretest\n```\n:::\n:::\n\n\n### Renaming and Visualizing the Writing Time Variable\n\nFor clarity, the column `Q5Duration` is renamed to `WritingTime`. A histogram is then plotted to visualize the distribution of writing time, with vertical lines indicating the mean and the thresholds defined by three standard deviations.\n\n::: {#a266e7ea .cell execution_count=5}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\n\n# Rename for clarity\nprocessed_df = processed_df.rename(columns={'Q5Duration': 'WritingTime'})\n\n# Plot histogram with mean and ±3 standard deviations\nplt.figure(figsize=(12, 6))\nplt.hist(processed_df['WritingTime'], bins=30, density=True, alpha=0.7)\nplt.axvline(processed_df['WritingTime'].mean(), color='red', linestyle='dashed', linewidth=1, label='Mean')\nplt.axvline(processed_df['WritingTime'].mean() + 3*processed_df['WritingTime'].std(),\n            color='green', linestyle='dashed', linewidth=1, label='3 SD Above Mean')\nplt.axvline(processed_df['WritingTime'].mean() - 3*processed_df['WritingTime'].std(),\n            color='green', linestyle='dashed', linewidth=1, label='3 SD Below Mean')\nplt.xlabel('Writing Time (seconds)')\nplt.ylabel('Density')\nplt.title('Distribution of Writing Time')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n(array([9.57302808e-04, 1.51315605e-03, 8.77012895e-04, 4.01449564e-04,\n        2.09989003e-04, 9.88183543e-05, 4.32330300e-05, 3.08807357e-05,\n        3.70568829e-05, 4.94091772e-05, 1.85284414e-05, 1.23522943e-05,\n        6.17614715e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        6.17614715e-06, 0.00000000e+00, 1.23522943e-05, 0.00000000e+00,\n        6.17614715e-06, 0.00000000e+00, 6.17614715e-06, 0.00000000e+00,\n        6.17614715e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 6.17614715e-06]),\n array([1.26500000e+00, 2.33898967e+02, 4.66532933e+02, 6.99166900e+02,\n        9.31800867e+02, 1.16443483e+03, 1.39706880e+03, 1.62970277e+03,\n        1.86233673e+03, 2.09497070e+03, 2.32760467e+03, 2.56023863e+03,\n        2.79287260e+03, 3.02550657e+03, 3.25814053e+03, 3.49077450e+03,\n        3.72340847e+03, 3.95604243e+03, 4.18867640e+03, 4.42131037e+03,\n        4.65394433e+03, 4.88657830e+03, 5.11921227e+03, 5.35184623e+03,\n        5.58448020e+03, 5.81711417e+03, 6.04974813e+03, 6.28238210e+03,\n        6.51501607e+03, 6.74765003e+03, 6.98028400e+03]),\n <BarContainer object of 30 artists>)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 0, 'Writing Time (seconds)')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0, 0.5, 'Density')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'Distribution of Writing Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-5.png){width=986 height=523}\n:::\n:::\n\n\n::: {#76dcb157 .cell execution_count=6}\n``` {.python .cell-code}\nprint(\"\\nWriting Time Summary Statistics:\")\nprint(processed_df['WritingTime'].describe())\nprint(f\"\\nSkewness: {processed_df['WritingTime'].skew():.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nWriting Time Summary Statistics:\ncount     696.000000\nmean      572.174059\nstd       630.892576\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       644.047000\nmax      6980.284000\nName: WritingTime, dtype: float64\n\nSkewness: 4.77\n```\n:::\n:::\n\n\n### Outlier Replacement and Log Transformation\n\nTo address skewness and outliers in the `WritingTime` variable, the code:\n- Computes acceptable bounds (mean ± 3 standard deviations).\n- Replaces values outside these bounds with the lesson-specific mean.\n- Applies a logarithmic transformation (using `np.log(x + 1)`) to compress the scale of higher values and stabilize variance.\n\nDescriptive statistics and skewness values are compared across the original, cleaned, and log-transformed data, demonstrating the effectiveness of these preprocessing steps.\n\n::: {#eea46d46 .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calculate mean, standard deviation, and bounds for outlier detection\nmean = processed_df['WritingTime'].mean()\nstd = processed_df['WritingTime'].std()\nlower_bound = mean - 3 * std\nupper_bound = mean + 3 * std\n\n# Preserve the original writing times\nprocessed_df['WritingTime_original'] = processed_df['WritingTime']\n\n# Replace outliers with the mean writing time of the corresponding lesson\nlesson_means = processed_df.groupby('LessonID')['WritingTime'].transform('mean')\nmask = (processed_df['WritingTime'] < lower_bound) | (processed_df['WritingTime'] > upper_bound)\nprocessed_df.loc[mask, 'WritingTime'] = lesson_means[mask]\n\n# Apply a log transformation to the cleaned writing times\nprocessed_df['WritingTime_log'] = np.log(processed_df['WritingTime'] + 1)\n\n# Display statistics for each transformation stage\nprint(\"Original WritingTime Statistics:\")\nprint(processed_df['WritingTime_original'].describe())\nprint(\"\\nSkewness (original):\", processed_df['WritingTime_original'].skew())\n\nprint(\"\\nCleaned WritingTime Statistics:\")\nprint(processed_df['WritingTime'].describe())\nprint(\"\\nSkewness (cleaned):\", processed_df['WritingTime'].skew())\n\nprint(\"\\nLog-transformed WritingTime Statistics:\")\nprint(processed_df['WritingTime_log'].describe())\nprint(\"\\nSkewness (log-transformed):\", processed_df['WritingTime_log'].skew())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal WritingTime Statistics:\ncount     696.000000\nmean      572.174059\nstd       630.892576\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       644.047000\nmax      6980.284000\nName: WritingTime_original, dtype: float64\n\nSkewness (original): 4.774762070142547\n\nCleaned WritingTime Statistics:\ncount     696.000000\nmean      512.355243\nstd       391.086600\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       626.499750\nmax      2375.579000\nName: WritingTime, dtype: float64\n\nSkewness (cleaned): 2.0766269712768888\n\nLog-transformed WritingTime Statistics:\ncount    696.000000\nmean       5.979808\nstd        0.782562\nmin        0.817575\n25%        5.532290\n50%        6.007293\n75%        6.441735\nmax        7.773417\nName: WritingTime_log, dtype: float64\n\nSkewness (log-transformed): -1.0692825002709074\n```\n:::\n:::\n\n\n### Visual Comparison of Distribution Transformations\n\nA three-panel plot compares the original, cleaned, and log-transformed distributions side by side. This visual comparison highlights:\n- The strong positive skew in the original data.\n- The reduction of extreme values in the cleaned data.\n- The near-symmetric distribution achieved after log transformation.\n\n::: {#3a552a78 .cell execution_count=8}\n``` {.python .cell-code}\nplt.figure(figsize=(15, 5))\n\n# Original data distribution\nplt.subplot(131)\nplt.hist(processed_df['WritingTime_original'], bins=30, alpha=0.7)\nplt.title('Original WritingTime')\nplt.xlabel('Seconds')\nplt.ylabel('Frequency')\n\n# Cleaned data distribution (with outliers replaced)\nplt.subplot(132)\nplt.hist(processed_df['WritingTime'], bins=30, alpha=0.7)\nplt.title('Cleaned WritingTime\\n(Outliers Replaced)')\nplt.xlabel('Seconds')\n\n# Log-transformed distribution\nplt.subplot(133)\nplt.hist(processed_df['WritingTime_log'], bins=30, alpha=0.7)\nplt.title('Log-transformed WritingTime')\nplt.xlabel('Log(Seconds)')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(array([155., 245., 142.,  65.,  34.,  16.,   7.,   5.,   6.,   8.,   3.,\n          2.,   1.,   0.,   0.,   0.,   1.,   0.,   2.,   0.,   1.,   0.,\n          1.,   0.,   1.,   0.,   0.,   0.,   0.,   1.]),\n array([1.26500000e+00, 2.33898967e+02, 4.66532933e+02, 6.99166900e+02,\n        9.31800867e+02, 1.16443483e+03, 1.39706880e+03, 1.62970277e+03,\n        1.86233673e+03, 2.09497070e+03, 2.32760467e+03, 2.56023863e+03,\n        2.79287260e+03, 3.02550657e+03, 3.25814053e+03, 3.49077450e+03,\n        3.72340847e+03, 3.95604243e+03, 4.18867640e+03, 4.42131037e+03,\n        4.65394433e+03, 4.88657830e+03, 5.11921227e+03, 5.35184623e+03,\n        5.58448020e+03, 5.81711417e+03, 6.04974813e+03, 6.28238210e+03,\n        6.51501607e+03, 6.74765003e+03, 6.98028400e+03]),\n <BarContainer object of 30 artists>)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0.5, 1.0, 'Original WritingTime')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0.5, 0, 'Seconds')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0, 0.5, 'Frequency')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(array([21., 40., 99., 74., 99., 71., 68., 58., 31., 24., 24., 14., 13.,\n        10., 11.,  3.,  6.,  3.,  5.,  1.,  3.,  0.,  2.,  3.,  3.,  1.,\n         1.,  1.,  5.,  2.]),\n array([1.2650000e+00, 8.0408800e+01, 1.5955260e+02, 2.3869640e+02,\n        3.1784020e+02, 3.9698400e+02, 4.7612780e+02, 5.5527160e+02,\n        6.3441540e+02, 7.1355920e+02, 7.9270300e+02, 8.7184680e+02,\n        9.5099060e+02, 1.0301344e+03, 1.1092782e+03, 1.1884220e+03,\n        1.2675658e+03, 1.3467096e+03, 1.4258534e+03, 1.5049972e+03,\n        1.5841410e+03, 1.6632848e+03, 1.7424286e+03, 1.8215724e+03,\n        1.9007162e+03, 1.9798600e+03, 2.0590038e+03, 2.1381476e+03,\n        2.2172914e+03, 2.2964352e+03, 2.3755790e+03]),\n <BarContainer object of 30 artists>)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0.5, 1.0, 'Cleaned WritingTime\\n(Outliers Replaced)')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0.5, 0, 'Seconds')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(array([ 1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  2.,  3.,  3.,\n         5.,  3.,  1.,  7., 19., 43., 64., 58., 93., 97., 93., 79., 48.,\n        33., 19.,  9., 13.]),\n array([0.81757476, 1.04943618, 1.2812976 , 1.51315902, 1.74502044,\n        1.97688186, 2.20874327, 2.44060469, 2.67246611, 2.90432753,\n        3.13618895, 3.36805037, 3.59991179, 3.83177321, 4.06363463,\n        4.29549605, 4.52735747, 4.75921889, 4.99108031, 5.22294173,\n        5.45480314, 5.68666456, 5.91852598, 6.1503874 , 6.38224882,\n        6.61411024, 6.84597166, 7.07783308, 7.3096945 , 7.54155592,\n        7.77341734]),\n <BarContainer object of 30 artists>)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0.5, 1.0, 'Log-transformed WritingTime')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0.5, 0, 'Log(Seconds)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-11.png){width=1430 height=471}\n:::\n:::\n\n\n### Feature Scaling: Min-Max Normalization\n\nThe cleaned `WritingTime` variable is scaled to a [0, 1] range using min-max normalization. Although this step does not alter the distribution’s skewness, it standardizes the data for algorithms that require features on a similar scale.\n\n::: {#f4ac288d .cell execution_count=9}\n``` {.python .cell-code}\nprocessed_df['WritingTime_scale'] = (\n    (processed_df['WritingTime'] - processed_df['WritingTime'].min()) /\n    (processed_df['WritingTime'].max() - processed_df['WritingTime'].min())\n)\n\nprint(\"Original WritingTime Statistics:\")\nprint(processed_df['WritingTime'].describe())\nprint(\"\\nSkewness (original):\", processed_df['WritingTime'].skew())\n\nprint(\"\\nMin-Max Scaled WritingTime Statistics:\")\nprint(processed_df['WritingTime_scale'].describe())\nprint(\"\\nSkewness (scaled):\", processed_df['WritingTime_scale'].skew())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal WritingTime Statistics:\ncount     696.000000\nmean      512.355243\nstd       391.086600\nmin         1.265000\n25%       251.722250\n50%       405.382500\n75%       626.499750\nmax      2375.579000\nName: WritingTime, dtype: float64\n\nSkewness (original): 2.0766269712768888\n\nMin-Max Scaled WritingTime Statistics:\ncount    696.000000\nmean       0.215258\nstd        0.164716\nmin        0.000000\n25%        0.105486\n50%        0.170204\n75%        0.263333\nmax        1.000000\nName: WritingTime_scale, dtype: float64\n\nSkewness (scaled): 2.076626971276889\n```\n:::\n:::\n\n\nA side-by-side histogram confirms that the underlying distribution shape remains unchanged after scaling.\n\n::: {#b1d87dff .cell execution_count=10}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 5))\n\n# Original distribution\nplt.subplot(121)\nplt.hist(processed_df['WritingTime'], bins=30, alpha=0.7)\nplt.title('Original WritingTime')\nplt.xlabel('Seconds')\nplt.ylabel('Frequency')\n\n# Scaled distribution\nplt.subplot(122)\nplt.hist(processed_df['WritingTime_scale'], bins=30, alpha=0.7)\nplt.title('Min-Max Scaled WritingTime')\nplt.xlabel('Scaled Value (0-1)')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n(array([21., 40., 99., 74., 99., 71., 68., 58., 31., 24., 24., 14., 13.,\n        10., 11.,  3.,  6.,  3.,  5.,  1.,  3.,  0.,  2.,  3.,  3.,  1.,\n         1.,  1.,  5.,  2.]),\n array([1.2650000e+00, 8.0408800e+01, 1.5955260e+02, 2.3869640e+02,\n        3.1784020e+02, 3.9698400e+02, 4.7612780e+02, 5.5527160e+02,\n        6.3441540e+02, 7.1355920e+02, 7.9270300e+02, 8.7184680e+02,\n        9.5099060e+02, 1.0301344e+03, 1.1092782e+03, 1.1884220e+03,\n        1.2675658e+03, 1.3467096e+03, 1.4258534e+03, 1.5049972e+03,\n        1.5841410e+03, 1.6632848e+03, 1.7424286e+03, 1.8215724e+03,\n        1.9007162e+03, 1.9798600e+03, 2.0590038e+03, 2.1381476e+03,\n        2.2172914e+03, 2.2964352e+03, 2.3755790e+03]),\n <BarContainer object of 30 artists>)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0.5, 1.0, 'Original WritingTime')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0.5, 0, 'Seconds')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0, 0.5, 'Frequency')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n(array([21., 40., 99., 74., 99., 71., 68., 58., 31., 24., 24., 14., 13.,\n        10., 11.,  3.,  6.,  3.,  5.,  1.,  3.,  0.,  2.,  3.,  3.,  1.,\n         1.,  1.,  5.,  2.]),\n array([0.        , 0.03333333, 0.06666667, 0.1       , 0.13333333,\n        0.16666667, 0.2       , 0.23333333, 0.26666667, 0.3       ,\n        0.33333333, 0.36666667, 0.4       , 0.43333333, 0.46666667,\n        0.5       , 0.53333333, 0.56666667, 0.6       , 0.63333333,\n        0.66666667, 0.7       , 0.73333333, 0.76666667, 0.8       ,\n        0.83333333, 0.86666667, 0.9       , 0.93333333, 0.96666667,\n        1.        ]),\n <BarContainer object of 30 artists>)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0.5, 1.0, 'Min-Max Scaled WritingTime')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0.5, 0, 'Scaled Value (0-1)')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0, 0.5, 'Frequency')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-9.png){width=1142 height=470}\n:::\n:::\n\n\n### Data Quality Assurance and Aggregation\n\nAdditional steps include:\n- Checking for and confirming the absence of duplicate records.\n\n::: {#17e0e043 .cell execution_count=11}\n``` {.python .cell-code}\n# Check duplicates\nprint(\"Number of duplicates:\", processed_df.duplicated().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of duplicates: 0\n```\n:::\n:::\n\n\n- Identifying missing values and removing rows with incomplete key columns.\n\n::: {#10fc3faa .cell execution_count=12}\n``` {.python .cell-code}\n# Check missing values\nprint(\"\\nMissing values in relevant columns:\")\nprint(processed_df[['WritingTime', 'WritingTime_log', 'WritingTime_scale', 'test']].isnull().sum())\n# Remove rows with missing values if any exist\nprocessed_df = processed_df.dropna(subset=['WritingTime', 'WritingTime_log', 'WritingTime_scale', 'test'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMissing values in relevant columns:\nWritingTime          71\nWritingTime_log      71\nWritingTime_scale    71\ntest                  0\ndtype: int64\n```\n:::\n:::\n\n\n- Aggregating summary statistics by test condition (pretest vs. posttest).\n\n::: {#a7bff8b8 .cell execution_count=13}\n``` {.python .cell-code}\ngrouped_stats = processed_df.groupby('test').agg({\n    'WritingTime_log': ['mean', 'std'],\n    'WritingTime_scale': ['mean', 'std']\n}).round(3)\n\nprint(\"\\nDescriptive Statistics by Test Group:\")\nprint(grouped_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDescriptive Statistics by Test Group:\n         WritingTime_log        WritingTime_scale       \n                    mean    std              mean    std\ntest                                                    \nposttest           6.007  0.786             0.224  0.176\npretest            5.953  0.779             0.207  0.153\n```\n:::\n:::\n\n\n- Saving the processed dataset as a CSV file for future analysis.\n\n::: {#55089e5d .cell execution_count=14}\n``` {.python .cell-code}\n# Save the processed DataFrame for future analysis\nprocessed_df.to_csv('assign1_summary_log.csv', index=False)\n```\n:::\n\n\n## Conclusion\n\nThis project implements a comprehensive data processing pipeline designed to prepare raw log data for statistical analysis and machine learning applications. The key steps include:\n\n1. Importing raw data from an Excel file.\n2. Processing multiple attempts per student by grouping and selecting the first non-null response.\n3. Creating a test label to distinguish between pretest and posttest conditions.\n4. Renaming and visualizing a key performance metric (WritingTime).\n5. Identifying and replacing outliers, followed by a log transformation to reduce skewness.\n6. Normalizing the data using min-max scaling.\n7. Ensuring data quality by checking for duplicates and handling missing values.\n8. Aggregating summary statistics by test type and saving the cleaned dataset.\n\nThis systematic approach not only cleans and prepares the data but also enhances its suitability for subsequent statistical tests and modeling. Ultimately, the refined data supports more accurate and meaningful conclusions about the effects of conversational agent language style on learning outcomes within ITS environments.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}