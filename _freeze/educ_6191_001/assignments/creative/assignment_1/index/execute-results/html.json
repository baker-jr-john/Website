{
  "hash": "1d607a270b09449e08cecf9b6c9d827d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Behavior Detection\"\ndescription: \"A machine learning model to detect off-task behavior\"\nauthor:\n  - name: John Baker\n    affiliations:\n      - name: \"Penn GSE: University of Pennsylvania Graduate School of Education\"\nlang: en\ndate: 09-18-2024\n# date-modified: 09-07-2024\nformat:\n  html:\n    code-link: false\ndraft: true\njupyter: python3\n---\n\n\nThis is a Quarto website!\n\nTo learn more about Quarto websites visit <https://quarto.org/docs/websites>!\n\n::: {#17a7d691 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, cohen_kappa_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\n\n# Load the dataset\ndata = pd.read_csv('data/ca1-dataset.csv')\n\n# Prepare the data\ndata['OffTask'] = data['OffTask'].map({'N': 0, 'Y': 1})  # Encode target variable\nX = data.drop(columns=['Unique-id', 'namea', 'OffTask'])  # Features\ny = data['OffTask']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Calculate the ratio of classes\nclass_0_count = sum(y_train_resampled == 0)\nclass_1_count = sum(y_train_resampled == 1)\nratio_of_classes = class_0_count / class_1_count\n\n# Define the model\nmodel = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n                           scoring='f1', cv=5, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV\ngrid_search.fit(X_train_resampled, y_train_resampled)\n\n# Best parameters\nprint(\"Best parameters found: \", grid_search.best_params_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 108 candidates, totalling 540 fits\nBest parameters found:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n```\n:::\n:::\n\n\n::: {#88ca0d57 .cell execution_count=2}\n``` {.python .cell-code}\n# Train the model on the resampled data\nmodel.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nkappa = cohen_kappa_score(y_test, y_pred)\nprint(\"Kappa Score:\", kappa)\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKappa Score: 0.40175953079178883\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97       147\n           1       0.38      0.50      0.43         6\n\n    accuracy                           0.95       153\n   macro avg       0.68      0.73      0.70       153\nweighted avg       0.96      0.95      0.95       153\n\n```\n:::\n:::\n\n\n::: {#20e80919 .cell execution_count=3}\n``` {.python .cell-code}\n# Train the XGBoost model without the use_label_encoder parameter\nxgb_model = XGBClassifier(eval_metric='logloss', scale_pos_weight=ratio_of_classes)\nxgb_model.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluate the model\nkappa_xgb = cohen_kappa_score(y_test, y_pred_xgb)\nprint(\"Kappa Score (XGBoost):\", kappa_xgb)\nprint(classification_report(y_test, y_pred_xgb))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKappa Score (XGBoost): 0.29655172413793107\n              precision    recall  f1-score   support\n\n           0       0.98      0.94      0.96       147\n           1       0.25      0.50      0.33         6\n\n    accuracy                           0.92       153\n   macro avg       0.61      0.72      0.65       153\nweighted avg       0.95      0.92      0.93       153\n\n```\n:::\n:::\n\n\n::: {#a76a5b4f .cell execution_count=4}\n``` {.python .cell-code}\n# Define the Gradient Boosting model\ngb_model = GradientBoostingClassifier(\n    learning_rate=0.2,\n    max_depth=5,\n    min_samples_split=10,\n    n_estimators=200,\n    random_state=42\n)\n\n# Fit the model on the resampled training data\ngb_model.fit(X_train_resampled, y_train_resampled)\n\n# Make predictions on the test set\ny_pred_gb = gb_model.predict(X_test)\n\n# Evaluate the model\nkappa_gb = cohen_kappa_score(y_test, y_pred_gb)\nprint(\"Kappa Score (Gradient Boosting):\", kappa_gb)\nprint(classification_report(y_test, y_pred_gb))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKappa Score (Gradient Boosting): 0.4137931034482758\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       147\n           1       0.33      0.67      0.44         6\n\n    accuracy                           0.93       153\n   macro avg       0.66      0.81      0.70       153\nweighted avg       0.96      0.93      0.94       153\n\n```\n:::\n:::\n\n\n::: {#559347f3 .cell execution_count=5}\n``` {.python .cell-code}\n# Get predicted probabilities\ny_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n\n# Set a new threshold\nthreshold = 0.3  # Example threshold\ny_pred_adjusted_gb = (y_pred_proba_gb >= threshold).astype(int)\n\n# Evaluate the model with the adjusted predictions\nkappa_adjusted_gb = cohen_kappa_score(y_test, y_pred_adjusted_gb)\nprint(\"Adjusted Kappa Score (Gradient Boosting):\", kappa_adjusted_gb)\nprint(classification_report(y_test, y_pred_adjusted_gb))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAdjusted Kappa Score (Gradient Boosting): 0.36514522821576767\n              precision    recall  f1-score   support\n\n           0       0.99      0.93      0.96       147\n           1       0.29      0.67      0.40         6\n\n    accuracy                           0.92       153\n   macro avg       0.64      0.80      0.68       153\nweighted avg       0.96      0.92      0.94       153\n\n```\n:::\n:::\n\n\n::: {#8f2c281a .cell execution_count=6}\n``` {.python .cell-code}\n# Experiment with different thresholds\nthresholds = np.arange(0.0, 1.0, 0.05)\nprecisions = []\nrecalls = []\nkappa_scores = []\n\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb >= threshold).astype(int)\n    \n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) > 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) > 0 else 0\n    \n    kappa = cohen_kappa_score(y_test, y_pred_adjusted)\n    \n    precisions.append(precision)\n    recalls.append(recall)\n    kappa_scores.append(kappa)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, kappa_scores, label='Kappa Score', marker='o')\nplt.title('Precision, Recall, and Kappa Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\nbest_threshold_index = np.argmax(recalls)\nbest_threshold = thresholds[best_threshold_index]\nprint(f\"Best Threshold for Maximum Recall: {best_threshold:.2f}\")\nprint(f\"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}\")\nprint(f\"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}\")\nprint(f\"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=823 height=523}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Threshold for Maximum Recall: 0.00\nPrecision at Best Threshold: 0.04\nRecall at Best Threshold: 1.00\nKappa Score at Best Threshold: 0.00\n```\n:::\n:::\n\n\n::: {#78d3d1e4 .cell execution_count=7}\n``` {.python .cell-code}\n# Initialize lists to store precision, recall, and F1-score values\nf1_scores = []\n\n# Calculate precision, recall, and F1-score for each threshold\nfor threshold in thresholds:\n    y_pred_adjusted = (y_pred_proba_gb >= threshold).astype(int)\n    \n    # Calculate precision and recall\n    precision = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_pred_adjusted) if np.sum(y_pred_adjusted) > 0 else 0\n    recall = np.sum(y_pred_adjusted[y_test == 1]) / np.sum(y_test) if np.sum(y_test) > 0 else 0\n    \n    # Calculate F1-score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Append F1-score to the list\n    f1_scores.append(f1_score)\n\n# Plot Precision, Recall, and F1-Score curve\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precisions, label='Precision', marker='o')\nplt.plot(thresholds, recalls, label='Recall', marker='o')\nplt.plot(thresholds, f1_scores, label='F1 Score', marker='o')\nplt.title('Precision, Recall, and F1 Score vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.xticks(np.arange(0.0, 1.1, 0.1))\nplt.legend()\nplt.grid()\nplt.show()\n\n# Print the best threshold based on maximum F1-score\nbest_threshold_index = np.argmax(f1_scores)\nbest_threshold = thresholds[best_threshold_index]\nprint(f\"Best Threshold for Maximum F1-Score: {best_threshold:.2f}\")\nprint(f\"Precision at Best Threshold: {precisions[best_threshold_index]:.2f}\")\nprint(f\"Recall at Best Threshold: {recalls[best_threshold_index]:.2f}\")\nprint(f\"Kappa Score at Best Threshold: {kappa_scores[best_threshold_index]:.2f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=823 height=523}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Threshold for Maximum F1-Score: 0.90\nPrecision at Best Threshold: 0.50\nRecall at Best Threshold: 0.67\nKappa Score at Best Threshold: 0.55\n```\n:::\n:::\n\n\n::: {#45f06e80 .cell execution_count=8}\n``` {.python .cell-code}\n# Make predictions using the new threshold\ny_pred_final = (gb_model.predict_proba(X_test)[:, 1] >= 0.90).astype(int)\n\n# Evaluate the model with the new predictions\nkappa_final = cohen_kappa_score(y_test, y_pred_final)\nprint(\"Final Kappa Score with Threshold 0.90:\", kappa_final)\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_final))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal Kappa Score with Threshold 0.90: 0.5513196480938416\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98       147\n           1       0.50      0.67      0.57         6\n\n    accuracy                           0.96       153\n   macro avg       0.74      0.82      0.78       153\nweighted avg       0.97      0.96      0.96       153\n\n```\n:::\n:::\n\n\n::: {#76e65a6c .cell execution_count=9}\n``` {.python .cell-code}\n# Optionally, you can also calculate and print confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_test, y_pred_final)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Visualize the confusion matrix (optional)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Not OffTask (0)', 'OffTask (1)'], \n            yticklabels=['Not OffTask (0)', 'OffTask (1)'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Perform k-fold cross-validation\ncv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')  # You can change scoring to 'accuracy', 'precision', etc.\n\n# Print the cross-validation scores\nprint(\"Cross-Validation F1 Scores:\", cv_scores)\nprint(\"Mean F1 Score:\", np.mean(cv_scores))\nprint(\"Standard Deviation of F1 Scores:\", np.std(cv_scores))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix:\n [[143   4]\n [  2   4]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=623 height=523}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCross-Validation F1 Scores: [0.25       0.54545455 0.5        0.2        0.        ]\nMean F1 Score: 0.2990909090909091\nStandard Deviation of F1 Scores: 0.20136722754852265\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}