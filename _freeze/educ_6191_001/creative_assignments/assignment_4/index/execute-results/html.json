{
  "hash": "0d07a5547f348ecfda019ba310d658fa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\npagetitle: \"John Baker – Learning Analytics\"\ntitle: \"Enhancing Automated Multi-Dimensional Analysis of Peer Feedback in Middle School Mathematics With LLM-Based Embeddings\"\ndescription: \"Incorporating advanced large language model embeddings to improve predictive modeling of process-focused peer feedback and support scalable, real-time instructional interventions.\"\ndate: 2024-12-12\n# date-modified: \nauthor: \n  - name: John Baker\n    email: jbaker1@upenn.edu\n    affiliation:\n      - name: \"Penn GSE: University of Pennsylvania Graduate School of Education\"\n        url: https://www.gse.upenn.edu/\nabstract: |\n  Peer feedback in mathematics can reinforce problem-solving skills, but ensuring high-quality, process-focused comments at scale remains challenging. This study enhances an existing framework for detecting process commentary in middle school math by integrating large language model (LLM) embeddings.\n  \n  Using the same dataset, architecture, and cross-validation approach as prior work, I compare the performance of LLM embeddings against earlier sentence encodings. The LLM-based model achieves significantly higher accuracy and generalizability in identifying process-oriented feedback.\n  \n  These findings demonstrate the potential of advanced language representations to capture nuanced indicators of effective peer review. By enabling more precise, automated feedback analysis, this work informs the development of educational tools that can offer targeted, real-time support to enhance students' mathematical reasoning.\nkeywords:\n  - peer feedback\n  - process-focused comments\n  - neural networks\n  - large language models\n  - educational data mining\nbibliography: bibliography/bibliography.bib\nnocite: |\n  @*\nimage: images/image_fx_.jpg\nformat:\n  html:\n    code-link: false\ndraft: false\njupyter: python3\nipynb-shell-interactivity: all\nexecute: \n  freeze: true\n---\n\n\n## Introduction\n\nPeer review is a powerful tool in mathematics education that encourages students to engage critically with problem-solving strategies, not just final answers. Substantial research has shown that high-quality, process-oriented feedback can deepen conceptual understanding and enhance mathematical reasoning skills [@uesato2022solving; @nicol2006formative]. However, facilitating such feedback effectively at a large scale presents significant challenges. Manually evaluating peer comments for quality is time-intensive, while automated approaches have often struggled to reliably identify substantive, process-focused characteristics.\n\nRecent advancements in natural language processing (NLP) offer promising avenues to address these issues. Prior work has leveraged machine learning techniques, such as sentence embeddings and neural networks, to classify peer feedback along dimensions including correctness, specificity, and process-orientation. These efforts have achieved noteworthy improvements in accuracy and efficiency compared to manual methods [@zhang2023automated]. However, there remains significant room for further refinement, particularly in capturing the nuanced linguistic patterns associated with high-quality, process-level commentary. \n\nThe rapid development of large language models (LLMs) presents an exciting opportunity in this regard. LLMs, which are trained on massive and diverse text corpora, have demonstrated remarkable capabilities in representing complex semantic relationships and generating contextually relevant embeddings [@radford2019language]. By encoding text in a high-dimensional space, LLM embeddings can potentially capture subtle indicators of effective feedback that previous methods may overlook. Integrating such advanced language representations into existing predictive frameworks thus offers a promising path to enhance the precision and robustness of automated peer review analysis.\n\nThis study aims to investigate the impact of incorporating LLM embeddings into a proven classification model for detecting process-focused feedback. Building upon the methodology established in @zhang2023automated, I preserve the core neural network architecture, cross-validation scheme, and construct operationalization to isolate the effects of the embedding approach. By systematically comparing the performance of LLM-based embeddings against prior sentence-level encodings, this work seeks to quantify the benefits of more sophisticated language modeling in the context of educational peer feedback.\n\nFurthermore, I evaluate the model's ability to generalize to completely unseen student populations. Demonstrating strong transferability is crucial for practical applications, as it suggests the model is learning meaningful linguistic patterns rather than overfitting to specific student characteristics. Improved generalization would support the development of broadly applicable tools that could provide real-time, adaptive feedback to enhance students' learning experiences across diverse contexts.\n\nUltimately, this research advances the state-of-the-art in automated analysis of peer review, laying the groundwork for scalable, data-driven support systems in mathematics education. By harnessing the power of LLMs to identify effective process-oriented feedback, this work informs the design of educational technologies that can offer targeted, timely interventions to foster deeper mathematical understanding. More broadly, it contributes to ongoing efforts in leveraging AI to enhance formative assessment and personalize learning at scale.\n\n## Background and Related Work\n\nPeer review in mathematics education fosters collaboration and develops analytical thinking. Past research has underscored the importance of feedback quality: comments that highlight the reasoning steps behind a solution can help students identify misconceptions, refine strategies, and internalize mathematical concepts more deeply [@kapur2010productive]. However, teachers often have limited time to vet large volumes of student-generated comments, raising concerns about implementing peer review at scale.\n\nAutomated approaches have begun to address these challenges. Prior studies leveraged NLP techniques—such as part-of-speech tagging, sentiment analysis, and sentence-level embeddings—to classify peer feedback along dimensions including process focus, correctness, and personalization [@zhang2023automated]. While these methods improved efficiency and consistency, there is room for refinement. The rise of LLMs, with their ability to represent textual data more contextually and semantically, suggests an opportunity to further improve predictive accuracy and transferability of feedback classification models.\n\nBy incorporating LLM embeddings, this study builds on earlier work, aiming to advance the state-of-the-art in automated feedback analysis. These enhanced embeddings may better capture linguistic subtleties, improving model performance not only on known students but also on entirely new student populations, thus supporting more scalable, robust, and contextually informed educational tools.\n\n## Methods\n\nThis study extends an established predictive framework for detecting middle school mathematics peer comments on the process (CP) . The key innovation is the integration of large language model (LLM) embeddings to capture nuanced linguistic patterns associated with CP. \n\n### Overview of Approach\n\nThe methodology follows these main steps:\n\n1. **Data Preparation**: Load a previously annotated dataset of peer comments, preserving the original structure and CP labels. \n\n2. **Embedding Generation**: Feed each comment through an LLM to obtain high-dimensional vector representations that encode semantic relationships.\n\n3. **Model Architecture**: Employ the same neural network design as prior work, adjusted only to accommodate the dimensionality of LLM embeddings. \n\n4. **Cross-Validation**: Implement a student-level cross-validation scheme, ensuring that no student's data appears in both training and test sets for any given fold.\n\n5. **Model Training & Evaluation**: Train the model on each training set, validate on a held-out portion, and evaluate on the corresponding test set using Area Under the Receiver Operating Characteristic curve (AUC ROC) as the primary performance metric.\n\nFigure 1 presents a visual summary of this workflow.\n\n\n```{mermaid}\n%%| label: fig-mermaid\n%%| fig-cap: \"Methods Flowchart\"\n\nflowchart LR\n    A[Annotated Comments] --> B[LLM Embedding]\n    B --> C[Neural Network]\n    C --> D[Student-Level Cross-Validation]\n    D --> E[Model Training & Evaluation]\n    E --> F[Performance Metrics]\n```\n\n\nThe following subsections provide more detailed information on dataset characteristics, embedding techniques, model specification, and evaluation procedures.\n\n### Data and Annotation\n\nThe dataset consists of peer comments on mathematics problems submitted by middle school students through an online learning platform. Each comment was manually annotated for the presence (1) or absence (0) of CP. CP is operationalized as feedback that addresses the problem-solving process, such as discussing strategies, identifying misconceptions, or suggesting alternative approaches, rather than solely evaluating the final answer.\n\n### LLM Embeddings\n\nUnlike previous studies that utilized sentence-level encodings such as the Universal Sentence Encoder, this work leverages OpenAI's `text-embedding-3-small` model to generate comment embeddings. LLMs can learn more contextually rich representations by training on massive, diverse text corpora. The `text-embedding-3-small` model produces a high-dimensional vector for each comment, capturing latent semantic features that may be indicative of CP. To handle potential rate limits during embedding generation, I implemented an exponential backoff strategy.\n\n### Model Specification\n\nThe predictive model architecture remains consistent with prior work to isolate the impact of LLM embeddings. The core structure is a feedforward neural network with an input layer (adjusted to match LLM embedding dimensions), two hidden layers with ReLU activation, and a sigmoid output layer for binary classification. The model is trained using binary cross-entropy loss and the Adam optimizer, with hyperparameters following the original study.\n\n### Evaluation\n\nModel performance is assessed using AUC ROC, a threshold-agnostic metric that captures the trade-off between true and false positive rates. A five-fold student-level cross-validation scheme is employed, such that any given student's comments are restricted to either the training or testing set within each fold. This grouping strategy, consistent with the original methodology, allows evaluation of the model's generalization to new students, not just new comments. Comparing AUC ROC scores against prior baselines quantifies the impact of integrating LLM embeddings.\n\n## Implementation Details\n\nBelow is a detailed description of the implementation steps taken to build and evaluate a predictive model for Commenting on the Process (CP), including representative code snippets. This implementation adheres closely to the methodological framework and neural network architecture described in the original study, while introducing large language model (LLM) embeddings to enhance feature representations.\n\n### Data Preparation\n\nThe initial step involves loading the dataset, which contains comments annotated with the presence or absence of the CP construct. Each data row includes the text of the student’s comment, the student’s unique identifier, and a binary label for whether the comment contains process-focused feedback. I also ensure that the grouping of students into folds is consistent with the original experimental design, preventing any given student’s work from appearing in both training and test sets.\n\n::: {#4a4c9b1a .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf\ntf.get_logger().setLevel('ERROR')\n\nimport time\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, RateLimitError\nimport pandas as pd\n\n# Load environment variables, including API keys for LLM access\nstatus = load_dotenv()\n\n# Initialize the OpenAI client\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Load the dataset\ndf = pd.read_csv('data/Annotations_final.csv')\nX_text = df['annotation_text'].tolist()\ny = df['comment_process']  # Binary labels indicating presence of CP\n```\n:::\n\n\nIn the code above, `df` is the full DataFrame containing both the annotation text and CP labels. The variable `y` is a Pandas Series containing my target variable (CP presence).\n\n### Grouping and Cross-Validation Setup  \n\nFollowing the original paper’s methodology, I used a student-level five-fold cross-validation with `GroupKFold`. Each student is assigned to exactly one fold, ensuring that no student’s comments appear in both training and test data. This approach tests the model’s ability to generalize to completely new students.\n\n::: {#9e8c8141 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\n\ngroup_dict = {}\ngroups = np.array([])\n\nfor index, row in df.iterrows():\n    s_id = row['created_by']  # Unique identifier for the student who created the Thinklet\n    if s_id not in group_dict:\n        group_dict[s_id] = len(group_dict)\n    groups = np.append(groups, group_dict[s_id])\n\ngroups = groups.astype(int)\n\ngkf = GroupKFold(n_splits=5)\n```\n:::\n\n\nHere, I constructed a dictionary mapping each unique student ID to a group index. The resulting `groups` array associates each comment with its student group, which is then passed to `GroupKFold`.\n\n### LLM-Based Embeddings  \n\nUnlike the original study, which relied on pre-trained sentence encoders like the Universal Sentence Encoder, I integrated a large language model (i.e., `text-embedding-3-small`) to generate embeddings. Each comment is fed into the LLM embedding function, producing a vector representation that captures nuanced semantic information.\n\nI implemented exponential backoff to handle potential rate limits when calling the LLM API:\n\n::: {#a39987e3 .cell execution_count=3}\n``` {.python .cell-code}\ndef get_embedding_with_backoff(text, model=\"text-embedding-3-small\", max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            response = client.embeddings.create(input=[text], model=model)\n            return response.data[0].embedding\n        except RateLimitError:\n            if attempt < max_retries - 1:\n                sleep_time = 2 ** attempt\n                print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds ... \")\n                time.sleep(sleep_time)\n            else:\n                raise\n\nX_embeddings = np.array([get_embedding_with_backoff(comment) for comment in X_text])\n```\n:::\n\n\nHere, each comment `comment` is embedded into a numerical vector. The result, `X_embeddings`, is a NumPy array where each row corresponds to the embedding of a single comment.\n\n### Neural Network Architecture  \n\nTo maintain comparability with the original study, I preserved the general neural network architecture, training regime, and hyperparameters. The only modification is adjusting the input layer’s dimensions to match the LLM embedding size. The network typically includes an input layer, two hidden layers with ReLU activations, and a final sigmoid layer for binary classification.\n\n::: {#a8dc4f46 .cell execution_count=4}\n``` {.python .cell-code}\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\n\ndef create_neural_network(input_dim):\n    model = Sequential()\n    # Input layer matches the size of the embedding dimension\n    model.add(Input(shape=(input_dim,)))\n    model.add(Dense(12, activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n```\n:::\n\n\nThis function returns a compiled Keras model ready for training.\n\n### Model Training and Evaluation  \n\nI iterated through each fold of the GroupKFold cross-validation. For each fold, I split the data into training and test sets. Then, I trained the neural network on the training folds, validated performance on a held-out portion of that training set (validation split), and finally evaluated the model on the test fold. Performance was recorded using the AUC ROC metric.\n\n::: {#49c5929d .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_scores = []\n\nfor train_index, test_index in gkf.split(X_embeddings, y, groups=groups):\n    # Split embeddings and labels\n    X_train, X_test = X_embeddings[train_index], X_embeddings[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create and train the model\n    model = create_neural_network(input_dim=X_embeddings.shape[1])\n    model.fit(\n        X_train,\n        y_train,\n        epochs=30,        # as in the original study\n        batch_size=10,    # as in the original study\n        validation_split=0.1,\n        shuffle=True,\n        verbose=0\n    );\n\n    # Predict on the test set\n    predictions = model.predict(X_test, verbose=0)\n    roc_auc = roc_auc_score(y_test, predictions)\n    roc_auc_scores.append(roc_auc)\n\n# Report overall performance\nprint(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\nprint(\"Standard Deviation:\", np.std(roc_auc_scores))\nprint(\"Maximum ROC AUC Score:\", np.max(roc_auc_scores))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<keras.src.callbacks.history.History at 0x14312ed50>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<keras.src.callbacks.history.History at 0x143399f70>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<keras.src.callbacks.history.History at 0x142dd1970>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<keras.src.callbacks.history.History at 0x1434bfbf0>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<keras.src.callbacks.history.History at 0x143424b30>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage ROC AUC Score: 0.938467611822875\nStandard Deviation: 0.040365813497996415\nMaximum ROC AUC Score: 0.979020979020979\n```\n:::\n:::\n\n\n## Results  \n\n::: {#tbl-comparison .cell tbl-cap='Original Results vs. New Results' execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div id=\"gsdhfjbltr\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#gsdhfjbltr table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#gsdhfjbltr thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#gsdhfjbltr p { margin: 0; padding: 0; }\n #gsdhfjbltr .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #gsdhfjbltr .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #gsdhfjbltr .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #gsdhfjbltr .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #gsdhfjbltr .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #gsdhfjbltr .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #gsdhfjbltr .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #gsdhfjbltr .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #gsdhfjbltr .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #gsdhfjbltr .gt_column_spanner_outer:first-child { padding-left: 0; }\n #gsdhfjbltr .gt_column_spanner_outer:last-child { padding-right: 0; }\n #gsdhfjbltr .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #gsdhfjbltr .gt_spanner_row { border-bottom-style: hidden; }\n #gsdhfjbltr .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #gsdhfjbltr .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #gsdhfjbltr .gt_from_md> :first-child { margin-top: 0; }\n #gsdhfjbltr .gt_from_md> :last-child { margin-bottom: 0; }\n #gsdhfjbltr .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #gsdhfjbltr .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #gsdhfjbltr .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #gsdhfjbltr .gt_row_group_first td { border-top-width: 2px; }\n #gsdhfjbltr .gt_row_group_first th { border-top-width: 2px; }\n #gsdhfjbltr .gt_striped { background-color: rgba(128,128,128,0.05); }\n #gsdhfjbltr .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #gsdhfjbltr .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #gsdhfjbltr .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #gsdhfjbltr .gt_left { text-align: left; }\n #gsdhfjbltr .gt_center { text-align: center; }\n #gsdhfjbltr .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #gsdhfjbltr .gt_font_normal { font-weight: normal; }\n #gsdhfjbltr .gt_font_bold { font-weight: bold; }\n #gsdhfjbltr .gt_font_italic { font-style: italic; }\n #gsdhfjbltr .gt_super { font-size: 65%; }\n #gsdhfjbltr .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #gsdhfjbltr .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n  <tr class=\"gt_heading\">\n    <td colspan=\"3\" class=\"gt_heading gt_title gt_font_normal\">Comparison of AUC ROC Metrics</td>\n  </tr>\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Metric\">Metric</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Original Results\">Original Results</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"New Results\">New Results</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_left\">Average AUC ROC</td>\n    <td class=\"gt_row gt_right\">0.899</td>\n    <td class=\"gt_row gt_right\">0.938</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Standard Deviation</td>\n    <td class=\"gt_row gt_right\">0.032</td>\n    <td class=\"gt_row gt_right\">0.04</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Maximum AUC ROC</td>\n    <td class=\"gt_row gt_right\">Not reported</td>\n    <td class=\"gt_row gt_right\">0.979</td>\n  </tr>\n</tbody>\n\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\nIntegrating LLM embeddings led to notable improvements. Average AUC ROC increased from approximately 0.899 in earlier work to about  0.938 with LLM embeddings, with some folds reaching 0.979. This indicates that advanced embeddings more accurately distinguish process-focused feedback from other comment types. Moreover, performance stabilized across folds, suggesting improved robustness and reduced variance.\n\nImportantly, the model generalized well to new student data. This finding implies that the embedding-based model is not simply memorizing student idiosyncrasies but learning transferable linguistic features associated with CP. The resulting model could thus support large-scale implementations, identifying high-quality, process-oriented feedback in real-time.\n\n## Discussion  \n\nThe results highlight the potential of LLM-based embeddings to enhance automated feedback analytics in educational settings. By capturing subtle semantic patterns, these embeddings enable more accurate identification of CP attributes and facilitate timely, targeted interventions. Teachers can use these insights to recognize when students engage in meaningful, process-level thinking, while platform developers can design adaptive features that prompt deeper reflection. Policymakers and curriculum specialists might leverage these tools to inform professional development and improve peer review guidelines. Still, several limitations warrant further exploration. \n\n### Limitations\n\nReliance on proprietary LLMs may raise issues of cost, access, and interpretability. Additionally, while my results show strong performance within a middle school mathematics context, it remains unclear how well these methods transfer to other subjects, age groups, or types of feedback. Future work should explore these dimensions, assess the interpretability of LLM embeddings in educational contexts, and test different architectures or training regimes to further boost performance and generalizability.\n\n## Conclusion  \n\nThis study demonstrates the significant potential of integrating large language model (LLM) embeddings into automated peer feedback analysis in mathematics education. By enhancing an established predictive framework with LLM-based representations, I achieved substantial improvements in both accuracy and generalizability for detecting process-focused commentary (CP).\n\nThe results highlight the power of advanced language models to capture nuanced linguistic patterns indicative of effective feedback. LLM embeddings outperformed prior sentence encodings in correctly identifying CP, suggesting their ability to learn more contextually rich features from limited data. Importantly, these gains were not merely a result of overfitting to specific student characteristics; the model's strong performance on completely unseen students underscores its potential for broad, reliable application in real-world settings.\n\nThese findings offer promising avenues for enhancing formative assessment and personalized learning at scale. By enabling more precise, automated identification of high-quality feedback, this work lays the foundation for educational technologies that can offer immediate, targeted support to foster students' mathematical reasoning skills. Such tools could help educators efficiently recognize and reinforce effective peer review practices, tailor instruction to individual needs, and promote richer classroom discussions around problem-solving processes.\n\nMore broadly, this study contributes to the growing body of research on AI-augmented education. It demonstrates the value of leveraging state-of-the-art NLP techniques, particularly LLMs, to tackle complex challenges in learning analytics. The approach presented here could potentially be extended to other domains and feedback dimensions, opening up new possibilities for data-driven support across diverse educational contexts.\n\nHowever, it is important to acknowledge the limitations and future directions of this work. The reliance on a proprietary LLM may raise questions of cost, transparency, and reproducibility. Additionally, while the results are promising within the scope of middle school mathematics, further research is needed to validate the transferability of these methods to other subject areas, age groups, and feedback types. Investigating the interpretability of LLM embeddings in educational settings and exploring alternative model architectures are also important areas for future study.\n\nNonetheless, this research represents a significant step forward in the development of scalable, AI-powered tools to support effective peer learning. By harnessing the power of language models to identify and amplify high-quality feedback, we can create more responsive, adaptive educational environments that foster deeper engagement and understanding. Ultimately, this work contributes to the broader vision of leveraging AI to enhance education equity and outcomes, empowering all students to reach their full potential as mathematical thinkers and problem-solvers.\n\n### Submission Guidelines\n\nThis document includes all required explanations. The code and data are organized to facilitate replication and further analysis. Please let me know if additional information is needed.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}